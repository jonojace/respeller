{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0453510-71c2-4a70-960a-970940bfc5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstring = \"\"\"\n",
    "Helper script that takes a folder of speech reps (wav2vec2, mel-spec, etc.)\n",
    "and aligns them at word-level using MFA alignments.\n",
    "\n",
    "Speech reps corresponding to word tokens in the corpus are then saved individually to an output folder\n",
    "with the following structure:\n",
    "- data_path\n",
    "    - word1\n",
    "        - word1_LJ010-0292_001.pt\n",
    "        - word1_LJ010-0292_002.pt\n",
    "        - ...\n",
    "    - word2\n",
    "        - word2_LJ001-0012_001.pt\n",
    "        - word2_LJ002-0024_001.pt\n",
    "        - ...\n",
    "    - ...\n",
    "\n",
    "- word1, word2, ... subfolders refer to a particular wordtype in the corpus.\n",
    "- .pt files contain speech representations that map to a particular example of a wordtype.\n",
    "  It is named as:\n",
    "    <wordtype>_<utt id>_<numbered occurrence in the utterance>.pt\n",
    "\n",
    "Example usage:\n",
    "    #hubert w/ padding offset\n",
    "    cd ~/fairseq\n",
    "    python examples/lexicon_learner/wordalign_speechreps.py \\\n",
    "        -t hubert \\\n",
    "        --padding_idx_offset 1 \\\n",
    "        -s /home/s1785140/fairseq/examples/lexicon_learner/lj_speech_quantized.txt \\\n",
    "        -a /home/s1785140/data/ljspeech_MFA_alignments \\\n",
    "        -o /home/s1785140/data/ljspeech_hubert_reps/hubert-base/layer-6/word_level_with_padding_idx_offset\n",
    "\n",
    "    #hubert w/o padding offset\n",
    "    cd ~/fairseq\n",
    "    python examples/lexicon_learner/wordalign_speechreps.py \\\n",
    "        -t hubert \\\n",
    "        --padding_idx_offset 0 \\\n",
    "        -s /home/s1785140/fairseq/examples/lexicon_learner/lj_speech_quantized.txt \\\n",
    "        -a /home/s1785140/data/ljspeech_MFA_alignments \\\n",
    "        -o /home/s1785140/data/ljspeech_hubert_reps/hubert-base/layer-6/word_level_without_padding_idx_offset\n",
    "\n",
    "    #wav2vec2\n",
    "    cd ~/fairseq\n",
    "    python examples/lexicon_learner/wordalign_speechreps.py \\\n",
    "        -t wav2vec2 \\\n",
    "        -s /home/s1785140/data/ljspeech_wav2vec2_reps/wav2vec2-large-960h/layer-15/utt_level \\\n",
    "        -a /home/s1785140/data/ljspeech_MFA_alignments \\\n",
    "        -o /home/s1785140/data/ljspeech_wav2vec2_reps/wav2vec2-large-960h/layer-15/word_level\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ed31824-8d7f-4e8c-a96b-937915cfc08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imitate CLAs\n",
    "import sys\n",
    "sys.argv = [\n",
    "    'train.py',\n",
    "    '--type', 'mel',\n",
    "    '--utt_id_list', '/home/s1785140/data/ljspeech_fastpitch/respeller_uttids.txt', \n",
    "    '--input_directory', '/home/s1785140/data/ljspeech_fastpitch/mels',\n",
    "    '--alignments', '/home/s1785140/data/ljspeech_fastpitch/aligns', \n",
    "    '--output_directory', '/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels',\n",
    "    \n",
    "    # FOR TESTING\n",
    "    # '--input_directory', '/home/s1785140/data/ljspeech_fastpitch/mels_test',\n",
    "    # '--alignments', '/home/s1785140/data/ljspeech_fastpitch/aligns_test', \n",
    "    # '--output_directory', '/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels_test',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3f0393f-64f2-4056-b219-a3e4f557516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import tgt\n",
    "import string\n",
    "\n",
    "SAMPLING_RATE = 22050\n",
    "HOP_LENGTH = 256\n",
    "SKIP_NON_ASCII = False\n",
    "WORDS_TO_SKIP = [\"wdsu-tv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "050c2ce5-b47b-4bf5-96a1-fd8bedc6330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_textgrid(tier, sampling_rate, hop_length, ignore_all_pauses=True):\n",
    "    # latest MFA replaces silence phones with \"\" in output TextGrids\n",
    "    sil_phones = [\"sil\", \"sp\", \"spn\", \"\"]\n",
    "    utt_start_time = tier[0].start_time\n",
    "    utt_end_time = tier[-1].end_time\n",
    "    phones = []\n",
    "    durations = [] # NOTE includes durations of silences\n",
    "    start_frames = []\n",
    "    end_frames = []\n",
    "    for i, t in enumerate(tier._objects):\n",
    "        s, e, p = t.start_time, t.end_time, t.text\n",
    "        if p not in sil_phones:\n",
    "            phones.append(p)\n",
    "            start_frames.append(int(np.ceil(s * sampling_rate / hop_length)))\n",
    "            end_frames.append(int(np.ceil(e * sampling_rate / hop_length)))\n",
    "            durations.append(int(np.ceil(e * sampling_rate / hop_length)\n",
    "                                 - np.ceil(s * sampling_rate / hop_length)))\n",
    "        else:\n",
    "            if not ignore_all_pauses:\n",
    "                if (i == 0) or (i == len(tier) - 1):\n",
    "                    # leading or trailing silence\n",
    "                    phones.append(\"sil\")\n",
    "                else:\n",
    "                    # short pause between words\n",
    "                    phones.append(\"sp\")\n",
    "\n",
    "    n_samples = utt_end_time * sampling_rate\n",
    "    n_frames = n_samples / hop_length\n",
    "    # fix occasional length mismatches at the end of utterances when\n",
    "    # duration in samples is an integer multiple of hop_length\n",
    "    if n_frames.is_integer():\n",
    "        durations[-1] += 1\n",
    "    return phones, durations, start_frames, end_frames, utt_start_time, utt_end_time\n",
    "\n",
    "def save_to_disk(tensor, word, utt_id, count, output_directory):\n",
    "    output_directory = os.path.join(output_directory, word)\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    save_path = os.path.join(output_directory, f'{word}__{utt_id}__occ{count}__seqlen{tensor.size(0)}.pt')\n",
    "    torch.save(tensor, save_path)\n",
    "    \n",
    "def allowed_word(word):\n",
    "    if len(word) <= 1:\n",
    "        return False\n",
    "    if word == '--':\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9ff2a0ad-4c05-476f-a430-ddd4e8e92e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-t', '--type', type=str, default='hubert',\n",
    "                    help='type of input speech reps that we are using, i.e. hubert wav2vec2 etc.')\n",
    "parser.add_argument('--padding_idx_offset', type=int, default=0,\n",
    "                    help='add 1 to token id of discrete reps in order to allow for padding_idx==0')\n",
    "parser.add_argument('--utt_id_list', type=str, required=False, default=\"\",\n",
    "                    help='path to text file that contains list of utterance ids that we extract from')\n",
    "parser.add_argument('-s', '--input_directory', type=str, required=True,\n",
    "                    help='path to single non-nested folder containing speech representations (.pt files) or txt file (hubert)')\n",
    "parser.add_argument('-a', '--alignments', type=str, required=True,\n",
    "                    help='path to single non-nested folder containing MFA alignments (.TextGrid files)')\n",
    "parser.add_argument('-o', '--output_directory', type=str, required=True,\n",
    "                    help='where to write word-level data')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8affea0-41bb-4739-b9ee-496642017f14",
   "metadata": {},
   "source": [
    "# load speech reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "225bb850-5a9e-4d90-bc1e-3c44c9d8048d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading mels from disk for 6551 utts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6551/6551 [00:21<00:00, 310.18it/s]\n"
     ]
    }
   ],
   "source": [
    "if args.type == \"hubert\":\n",
    "    with open(args.input_directory, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    num_of_utts = len(lines)\n",
    "    utt_id2speechreps = {l.split('|')[0]:l.split('|')[1] for l in lines}\n",
    "    utt_ids = sorted(utt_id2speechreps.keys()) # ensure we always process utts in same alphabetical order\n",
    "elif args.type == \"wav2vec2\":\n",
    "    num_of_utts = len(os.listdir(args.input_directory))\n",
    "    utt_ids = sorted(file.split('.')[0] for file in os.listdir(args.input_directory))\n",
    "elif args.type == \"mel\":\n",
    "    if args.utt_id_list:\n",
    "        # we specified a subset of utt ids\n",
    "        with open(args.utt_id_list, 'r') as f:\n",
    "            utt_ids = f.read().splitlines()\n",
    "    else:\n",
    "        # all files in directory\n",
    "        utt_ids = list(sorted(file.split('.')[0] for file in os.listdir(args.input_directory)))\n",
    "    num_of_utts = len(utt_ids)\n",
    "    utt_id2speechreps = {}\n",
    "    print(f\"loading mels from disk for {len(utt_ids)} utts\")\n",
    "    for utt_id in tqdm(utt_ids):\n",
    "        # load mel data\n",
    "        p = os.path.join(args.input_directory, f'{utt_id}.pt')\n",
    "        mel = torch.load(p).transpose(0,1) #[seqlen, feats]\n",
    "        utt_id2speechreps[utt_id] = mel\n",
    "else:\n",
    "    raise ValueError(f\"invalid input type {args.type}\")\n",
    "\n",
    "# sanity check - assert that each utt has a corresponding alignment\n",
    "alignment_files = set(os.listdir(args.alignments))\n",
    "for utt_id in utt_ids:\n",
    "    assert f\"{utt_id}.TextGrid\" in alignment_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a8a42a-79ba-468d-9cb8-cae8ef38cb95",
   "metadata": {},
   "source": [
    "# perform splitting of mel specs using MFA alignments and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d30b8ce-c345-40e2-8ae5-bb1fc4f468bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split speech reps using word alignments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████████████████████████████████▌                                                                                                     | 1870/6551 [06:35<07:15, 10.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: char â in word grâce\n",
      "\tnormalised 'grâce' to 'grace'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████████████████████████████▎                                                                                               | 2134/6551 [07:35<10:10,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: char é in word habitué\n",
      "\tnormalised 'habitué' to 'habitue'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████████████████████████████████████▋                                                                               | 2894/6551 [10:01<08:14,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: char ê in word dêtre\n",
      "\tnormalised 'dêtre' to 'detre'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████████████████████████████████████▉                                                                               | 2905/6551 [10:03<07:25,  8.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: char ü in word müllers\n",
      "\tnormalised 'müllers' to 'mullers'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|███████████████████████████████████████████████████████████████▏                                                                              | 2913/6551 [10:04<07:07,  8.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: char ü in word müller\n",
      "\tnormalised 'müller' to 'muller'\n",
      "WARNING: char ü in word müller\n",
      "\tnormalised 'müller' to 'muller'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|███████████████████████████████████████████████████████████████▎                                                                              | 2919/6551 [10:04<07:22,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: char ü in word müller\n",
      "\tnormalised 'müller' to 'muller'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|███████████████████████████████████████████████████████████████▍                                                                              | 2925/6551 [10:05<07:18,  8.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: char ü in word müller\n",
      "\tnormalised 'müller' to 'muller'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|█████████████████████████████████████████████████████████████████████▎                                                                        | 3198/6551 [10:50<09:42,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: char ü in word müller\n",
      "\tnormalised 'müller' to 'muller'\n",
      "WARNING: char ü in word müller\n",
      "\tnormalised 'müller' to 'muller'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|█████████████████████████████████████████████████████████████████████▎                                                                        | 3200/6551 [10:50<13:00,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: char ü in word müller\n",
      "\tnormalised 'müller' to 'muller'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|██████████████████████████████████████████████████████████████████████████▏                                                                   | 3421/6551 [11:37<11:40,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: char é in word célèbre\n",
      "WARNING: char è in word célèbre\n",
      "\tnormalised 'célèbre' to 'celebre'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████████████████████████████████████████████████████████████████▎                                                            | 3752/6551 [12:33<04:30, 10.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: char - in word forward-\n",
      "\tnormalised 'forward-' to 'forward'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|██████████████████████████████████████████████████████████████████████████████████▌                                                           | 3811/6551 [12:44<08:05,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: char - in word self-\n",
      "\tnormalised 'self-' to 'self'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|██████████████████████████████████████████████████████████████████████████████████▊                                                           | 3823/6551 [12:46<05:41,  7.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: char - in word vice-\n",
      "\tnormalised 'vice-' to 'vice'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                      | 4762/6551 [15:30<03:03,  9.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: char - in word full-\n",
      "\tnormalised 'full-' to 'full'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████                                 | 5034/6551 [16:20<03:46,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: char - in word gray-\n",
      "\tnormalised 'gray-' to 'gray'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 5942/6551 [18:53<00:48, 12.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: char - in word wdsu-tv. skipping!...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6551/6551 [20:11<00:00,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordtype with longest num of timesteps is anesthesiologists from LJ031-0023 with len 152\n",
      "you can set transformer max_source_positions to this\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "longest_word = ''\n",
    "longest_word_utt_id = ''\n",
    "longest_word_num_frames = 0\n",
    "\n",
    "# split each speech reps file using the word-level alignments\n",
    "print(\"split speech reps using word alignments\")\n",
    "for utt_id in tqdm(utt_ids):\n",
    "    # load speech reps\n",
    "    if args.type == \"hubert\":\n",
    "        reps = utt_id2speechreps[utt_id]\n",
    "        reps = [int(s)+args.padding_idx_offset for s in reps.split(' ')] # NOTE add 1 to each index so that 0 is available as a padding_idx\n",
    "        reps = torch.tensor(reps)\n",
    "        reps.requires_grad = False\n",
    "\n",
    "        # check dimensions\n",
    "        if reps.dim() == 1:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"speech representations have an incorrect number of dimensions\")\n",
    "    elif args.type == \"mel\":\n",
    "        reps = utt_id2speechreps[utt_id]\n",
    "        reps.requires_grad = False\n",
    "\n",
    "        # check dimensions\n",
    "        if reps.dim() == 2 and reps.size(1) == 80:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"speech representations have an incorrect number of dimensions\")\n",
    "    else:\n",
    "        raise ValueError(f\"invalid input type {args.type}\")\n",
    "\n",
    "    tg_path = f\"{args.alignments}/{utt_id}.TextGrid\"\n",
    "    tg = tgt.io.read_textgrid(tg_path, include_empty_intervals=True)\n",
    "    words, all_durs, start_frames, end_frames, utt_start, utt_end = parse_textgrid(tg.get_tier_by_name('words'), SAMPLING_RATE, HOP_LENGTH)\n",
    "    \n",
    "    word_occ_in_utt_counter = Counter()\n",
    "    mel = utt_id2speechreps[utt_id]\n",
    "    # assert mel.size(0) == sum(all_durs), f\"{mel.size(0)=} != {sum(all_durs)=}\" # verify that MFA frame durations match up with the extracted mels\n",
    "    for word, dur, start_frame, end_frame in zip(words, all_durs, start_frames, end_frames):\n",
    "        if allowed_word(word):\n",
    "            skip_word = False\n",
    "            normalise_non_ascii = False\n",
    "            for c in word:\n",
    "                if c not in string.ascii_lowercase:\n",
    "                    s = f'WARNING: char {c} in word {word}'\n",
    "                    if SKIP_NON_ASCII or word in WORDS_TO_SKIP:\n",
    "                        s += '. skipping!...'\n",
    "                        skip_word = True\n",
    "                    else:\n",
    "                        normalise_non_ascii = True\n",
    "\n",
    "                    print(s)\n",
    "                    \n",
    "            if not skip_word:\n",
    "                if normalise_non_ascii: # normalise word\n",
    "                    prenorm_word = word\n",
    "                    # remove trailing '-'\n",
    "                    word = word.rstrip('-')\n",
    "                    # convert diacritics to ascii\n",
    "                    word = unidecode.unidecode(word)\n",
    "                    print(f\"\\tnormalised '{prenorm_word}' to '{word}'\")\n",
    "                \n",
    "                # check if word is the longest word we have seen so far\n",
    "                word_dur = end_frame - start_frame \n",
    "                if word_dur > longest_word_num_frames:\n",
    "                    longest_word_num_frames = word_dur\n",
    "                    longest_word = word\n",
    "                    longest_word_utt_id = utt_id\n",
    "\n",
    "                # extract mel\n",
    "                wordaligned_mel = mel[start_frame:end_frame]\n",
    "\n",
    "                # save extracted mel to disk\n",
    "                word_occ_in_utt_counter[word] += 1\n",
    "                extracted_timesteps = wordaligned_mel.size(0)\n",
    "                assert dur == extracted_timesteps == word_dur, f\"{dur=}, {extracted_timesteps=}, {word_dur=}\"\n",
    "                save_to_disk(wordaligned_mel, word, utt_id, word_occ_in_utt_counter[word], args.output_directory)\n",
    "\n",
    "print(\"wordtype with longest num of timesteps is\", longest_word, \"from\", longest_word_utt_id, \"with len\", longest_word_num_frames)\n",
    "print(\"you can set transformer max_source_positions to this\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd5c7db-860e-4daf-aa2e-52f6d6a12cff",
   "metadata": {},
   "source": [
    "# create train,dev,test datasplits for training respeller\n",
    "\n",
    "We hold out WORDTYPES from training for the dev and test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1d3344-f2ed-44f3-bf87-0f0b038c0421",
   "metadata": {},
   "source": [
    "## Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "835dc300-35a9-4f21-a5d3-584919022662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "random.seed(1337)\n",
    "\n",
    "train_ratio, dev_ratio, test_ratio = [0.9, 0.05, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "d7fc263e-51ae-4d31-b7c8-b5b75454bf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original before cleaning/sampling len(all_wordtypes)=8343\n"
     ]
    }
   ],
   "source": [
    "# get oov wordtypes list (words that are not seen in tts training)\n",
    "oov_wordlist_path = '/home/s1785140/data/ljspeech_fastpitch/oov_list.json'\n",
    "with open(oov_wordlist_path, 'r') as f:\n",
    "    oovs_and_freqs = json.load(f)\n",
    "    \n",
    "all_wordtypes = set(oovs_and_freqs.keys())\n",
    "print(f'original before cleaning/sampling {len(all_wordtypes)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "17c6d733-557d-49bf-8ef2-63983caba003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34=\n"
     ]
    }
   ],
   "source": [
    "# clean/remove words that do not have speech reps\n",
    "words_with_aligned_mels = set(os.listdir(args.output_directory))\n",
    "words_no_mels = all_wordtypes - words_with_aligned_mels\n",
    "print(f'{len(words_no_mels)}=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "a134aa68-17fb-425e-b5ee-0a4bd19407bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of words to be excluded from respeller training as they do not have mels (likely due to how normalisation is different between mfa and our own data processing):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'aaa',\n",
       " 'cos',\n",
       " 'dc',\n",
       " 'eg',\n",
       " 'eightthirty',\n",
       " 'elevenfifty',\n",
       " 'eleventhirty',\n",
       " 'fivefifty',\n",
       " 'fourfifty',\n",
       " 'fourforty',\n",
       " 'fourthirty',\n",
       " 'iq',\n",
       " 'k',\n",
       " 'lj',\n",
       " 'lld',\n",
       " 'mps',\n",
       " 'ninethirty',\n",
       " 'onefifteen',\n",
       " 'onefifty',\n",
       " 'oneforty',\n",
       " 'ps',\n",
       " 'sevenfifteen',\n",
       " 'seventhirty',\n",
       " 'sixthirty',\n",
       " 'tenforty',\n",
       " 'tenthirty',\n",
       " 'threetwenty',\n",
       " 'tv',\n",
       " 'twelvefifteen',\n",
       " 'twofifteen',\n",
       " 'twoforty',\n",
       " 'twothirty',\n",
       " 'u',\n",
       " 'uss'}"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"list of words to be excluded from respeller training as they do not have mels (likely due to how normalisation is different between mfa and our own data processing):\")\n",
    "words_no_mels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "0cae348a-0333-484a-8245-383537c4877f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original after cleaning len(all_wordtypes)=8309\n"
     ]
    }
   ],
   "source": [
    "# remove these problematic words from respeller training dev test\n",
    "for w in words_no_mels:\n",
    "    del oovs_and_freqs[w]\n",
    "    \n",
    "all_wordtypes = set(oovs_and_freqs.keys())\n",
    "print(f'original after cleaning {len(all_wordtypes)=}')\n",
    "\n",
    "dev_N = int(dev_ratio * len(all_wordtypes))\n",
    "test_N = int(test_ratio * len(all_wordtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "07ce65fc-a973-4fbc-8b82-e63dd0131ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_remove(s: set, N: int):\n",
    "    \"\"\"sample N words from set s\n",
    "    then remove these words from the set\"\"\"\n",
    "    sampled = random.sample(s, N)\n",
    "    for item in sampled:\n",
    "        s.remove(item)\n",
    "    return set(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "5f3e4eed-a336-43c1-8634-8b54707034fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before sampling dev and test len(oov_singletons)=5440\n",
      "after sampling dev len(oov_singletons)=5025, len(dev)=415\n",
      "after sampling test len(oov_singletons)=4610, len(test)=415\n"
     ]
    }
   ],
   "source": [
    "#get dev and test splits\n",
    "oov_singletons = set(wordtype for wordtype, freq in oovs_and_freqs.items() if freq == 1)\n",
    "assert len(oov_singletons) > dev_N + test_N, \"not enough OOV singletons to create dev and test sets\" \n",
    "print(f'before sampling dev and test {len(oov_singletons)=}')\n",
    "\n",
    "dev = sample_and_remove(oov_singletons, dev_N)\n",
    "print(f'after sampling dev {len(oov_singletons)=}, {len(dev)=}')\n",
    "\n",
    "test = sample_and_remove(oov_singletons, test_N)\n",
    "print(f'after sampling test {len(oov_singletons)=}, {len(test)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "a69f58b0-b7c7-4c54-9333-adee647b291b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sumpter',\n",
       " 'rumor',\n",
       " 'esther',\n",
       " 'depressed',\n",
       " 'violins',\n",
       " 'apprise',\n",
       " 'summarize',\n",
       " 'adelphi',\n",
       " 'sighing',\n",
       " 'entreating']"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dev)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "acb7c6d1-0c33-479d-8cbf-859fba31d7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yell',\n",
       " 'invaded',\n",
       " 'oblivion',\n",
       " 'punches',\n",
       " 'divide',\n",
       " 'permits',\n",
       " 'facilitating',\n",
       " 'resurrection',\n",
       " 'cashier',\n",
       " 'delicacy']"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "01092fe6-fd9f-42db-a206-01097ef558e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before removing dev and test wordtypes len(all_wordtypes)=8309\n",
      "after removing dev and test wordtypes len(all_wordtypes)=7479\n"
     ]
    }
   ],
   "source": [
    "#get train split\n",
    "print(f'before removing dev and test wordtypes {len(all_wordtypes)=}')\n",
    "for word in dev | test:\n",
    "    all_wordtypes.remove(word)\n",
    "print(f'after removing dev and test wordtypes {len(all_wordtypes)=}')\n",
    "\n",
    "train = set(all_wordtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "9aa0bce1-1baa-422d-a6eb-fb53e8d05964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good! No overlapping words between train, dev, and test!!!\n"
     ]
    }
   ],
   "source": [
    "# sanity checks\n",
    "assert len(dev.intersection(test)) == 0\n",
    "assert len(train.intersection(dev)) == 0\n",
    "assert len(train.intersection(test)) == 0\n",
    "print(\"Good! No overlapping words between train, dev, and test!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "455c0255-bf59-480b-9d83-6b43376125f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to disk\n",
    "def save_wordlist(path, words):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(sorted(list(words)), f, indent=4)\n",
    "        \n",
    "train_path = '/home/s1785140/data/ljspeech_fastpitch/respeller_train_words.json'\n",
    "dev_path = '/home/s1785140/data/ljspeech_fastpitch/respeller_dev_words.json'\n",
    "test_path = '/home/s1785140/data/ljspeech_fastpitch/respeller_test_words.json'\n",
    "\n",
    "save_wordlist(train_path, train)\n",
    "save_wordlist(dev_path, dev)\n",
    "save_wordlist(test_path, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61f2ebd-5d54-4e81-ae06-295add409a5e",
   "metadata": {},
   "source": [
    "## G2P selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c63569d-3f12-45c0-ba6a-a4379235d35f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
