{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d482e8f7-78e4-4a05-81fa-74a63de4e706",
   "metadata": {},
   "source": [
    "# autoreload notebook magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b9f5a3c3-31af-4835-88ee-4410bc5af326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2de0d84a-9335-48be-8bfb-ba9cbbf0dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cfb56b-58ed-447f-9308-a25d1e92cbc0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f776e058-d463-414a-b612-c6540f40a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train respeller model\n",
    "\n",
    "We backpropagate loss from pretrained TTS model to a Grapheme-to-Grapheme (G2G) respeller model to help it respell words\n",
    "into a simpler form\n",
    "\n",
    "Intermediated respellings are discrete character sequences\n",
    "We can backpropagate through these using gumbel softmax and the straight through estimator\n",
    "'''\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "from fastpitch import models as fastpitch_model\n",
    "from fastpitch.common.text.text_processing import TextProcessor\n",
    "\n",
    "from modules.model import EncoderRespeller\n",
    "from modules.gumbel_vector_quantizer import GumbelVectorQuantizer\n",
    "from modules.sdtw_cuda_loss import SoftDTW\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "import fastpitch.common.tb_dllogger as logger\n",
    "from torch_optimizer import Lamb\n",
    "import time\n",
    "from fastpitch.common.utils import mask_from_lens\n",
    "from collections import OrderedDict\n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e48e9d99-904c-40a9-b1f0-825c0300efc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72cec08-fc1a-459b-87d4-49f1219cc86d",
   "metadata": {},
   "source": [
    "# simulate running script at command line with arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2490a6-4c44-4a4e-8f77-7525f98fb4d5",
   "metadata": {},
   "source": [
    "## set experiment name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c9eb43f2-963e-457d-ab36-1a8f39997a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.1\n",
    "lr = 0.1 # def for lamb optimizer is 0.001\n",
    "dist_metric = 'l1'\n",
    "\n",
    "wandb_project_name = \"respeller-training\"\n",
    "exp_name = f\"test_development\"\n",
    "fastpitch_chkpt = 'fastpitch/exps/halved_ljspeech_data_nospaces_noeos/FastPitch_checkpoint_1000.pt' # 'fastpitch/exps/halved_ljspeech_data/FastPitch_checkpoint_1000.pt',"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d6d09b-7d75-4f92-846f-70df8eb41033",
   "metadata": {},
   "source": [
    "## CLAs that affect training loop in this notebook!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bc78f43f-f8f6-4ffc-94b2-5aa10bdb06d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread SystemMonitor:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s1785140/miniconda3/envs/fastpitch/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/s1785140/miniconda3/envs/fastpitch/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/s1785140/miniconda3/envs/fastpitch/lib/python3.8/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 118, in _start\n",
      "    asset.start()\n",
      "  File \"/home/s1785140/miniconda3/envs/fastpitch/lib/python3.8/site-packages/wandb/sdk/internal/system/assets/disk.py\", line 76, in start\n",
      "    self.metrics_monitor.start()\n",
      "  File \"/home/s1785140/miniconda3/envs/fastpitch/lib/python3.8/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 168, in start\n",
      "    logger.info(f\"Started {self._process.name}\")\n",
      "AttributeError: 'NoneType' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "# imitate CLAs\n",
    "import sys\n",
    "sys.argv = [\n",
    "    'train.py',\n",
    "    '--wandb-project-name', wandb_project_name,\n",
    "    '--chkpt-save-dir', f'/home/s1785140/respeller/exps/{exp_name}', \n",
    "    '--fastpitch-chkpt', fastpitch_chkpt,\n",
    "    '--input-type', 'char',\n",
    "    '--symbol-set', 'english_basic_lowercase_no_arpabet',\n",
    "    '--use-mas',\n",
    "    '--cuda',\n",
    "    '--n-speakers', '1',\n",
    "    '--use-sepconv',\n",
    "    # '--add-spaces',\n",
    "    # '--eos-symbol', '$',\n",
    "    '--batch-size', '4',\n",
    "    '--val-num-to-gen', '2',\n",
    "    '--softdtw-temp', str(gamma),\n",
    "    '--dist-func', dist_metric,\n",
    "    '--learning-rate', str(lr),\n",
    "    \n",
    "    # NB for real training!\n",
    "    # '--epochs', '10000', \n",
    "    # '--val-log-interval', '10',\n",
    "    # '--resume', # resume from latest checkpoint\n",
    "    \n",
    "    # # NB for development!\n",
    "    '--epochs', '2', # NB for development!\n",
    "    '--val-log-interval', '1', # NB for development!\n",
    "    '--max-iters-per-epoch', '5', # NB for development!\n",
    "    # '--skip-before-train-loop-validation', # NB for development!\n",
    "    # '--freeze-embedding-table',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ae5f1cf9-947c-4b32-a463-7790f5bf305a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train.py --wandb-project-name respeller-training --chkpt-save-dir /home/s1785140/respeller/exps/test_development --fastpitch-chkpt fastpitch/exps/halved_ljspeech_data_nospaces_noeos/FastPitch_checkpoint_1000.pt --input-type char --symbol-set english_basic_lowercase_no_arpabet --use-mas --cuda --n-speakers 1 --use-sepconv --batch-size 4 --val-num-to-gen 2 --softdtw-temp 0.1 --dist-func l1 --learning-rate 0.1 --epochs 2 --val-log-interval 1 --max-iters-per-epoch 5\n"
     ]
    }
   ],
   "source": [
    "# print the command to be run so we can copy and paste it into terminal if needs be\n",
    "print(f\"python\", \" \".join(sys.argv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "10164276-6357-4458-8924-3bb94eb0bdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd /home/s1785140/respeller\n",
      "python train.py \\\n",
      "    --wandb-project-name respeller-training \\\n",
      "    --chkpt-save-dir /home/s1785140/respeller/exps/test_development \\\n",
      "    --fastpitch-chkpt fastpitch/exps/halved_ljspeech_data_nospaces_noeos/FastPitch_checkpoint_1000.pt \\\n",
      "    --input-type char \\\n",
      "    --symbol-set english_basic_lowercase_no_arpabet \\\n",
      "    --use-mas \\\n",
      "    --cuda \\\n",
      "    --n-speakers 1 \\\n",
      "    --use-sepconv \\\n",
      "    --batch-size 4 \\\n",
      "    --val-num-to-gen 2 \\\n",
      "    --softdtw-temp 0.1 \\\n",
      "    --dist-func l1 \\\n",
      "    --learning-rate 0.1 \\\n",
      "    --epochs 2 \\\n",
      "    --val-log-interval 1 \\\n",
      "    --max-iters-per-epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread SystemMonitor:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s1785140/miniconda3/envs/fastpitch/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/s1785140/miniconda3/envs/fastpitch/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/s1785140/miniconda3/envs/fastpitch/lib/python3.8/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 118, in _start\n",
      "    asset.start()\n",
      "  File \"/home/s1785140/miniconda3/envs/fastpitch/lib/python3.8/site-packages/wandb/sdk/internal/system/assets/disk.py\", line 76, in start\n",
      "    self.metrics_monitor.start()\n",
      "  File \"/home/s1785140/miniconda3/envs/fastpitch/lib/python3.8/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 168, in start\n",
      "    logger.info(f\"Started {self._process.name}\")\n",
      "AttributeError: 'NoneType' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "def prettyprint_cla_for_training(sys_argv):\n",
    "    \"\"\"for every flag and its arguments print on a new line\"\"\"\n",
    "    def print_line(to_print):\n",
    "        s = \"    \" + \" \".join(to_print)\n",
    "        print(s)\n",
    "\n",
    "    if sys_argv[0].endswith('.py'):\n",
    "        args = sys_argv[1:]\n",
    "    else: \n",
    "        args = sys_argv\n",
    "        \n",
    "    to_print = []        \n",
    "    for i, arg in enumerate(args):            \n",
    "        if arg.startswith('--') and i != 0:\n",
    "            print_line(to_print + ['\\\\'])\n",
    "            to_print = []\n",
    "        to_print.append(arg)\n",
    "        \n",
    "    print_line(to_print)\n",
    "            \n",
    "print(\"cd /home/s1785140/respeller\")\n",
    "print(\"python train.py \\\\\")\n",
    "prettyprint_cla_for_training(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b9c46-a867-474f-aa94-d032c9769fdb",
   "metadata": {},
   "source": [
    "# print sbatch.sh hyperparam sweep training commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5039b8af-0129-4b3c-ae58-0929cc66907c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd /home/s1785140/respeller\n",
      "\n",
      "# Experiment 1/5: Sweep_DropoutLayer_dropout_layer=0.0\n",
      "./sbatch.sh python train.py \\\n",
      "    --wandb-project-name respeller \\\n",
      "    --chkpt-save-dir /home/s1785140/respeller/exps/Sweep_DropoutLayer_dropout_layer=0.0 \\\n",
      "    --fastpitch-chkpt fastpitch/exps/halved_ljspeech_data_nospaces_noeos/FastPitch_checkpoint_1000.pt \\\n",
      "    --input-type char \\\n",
      "    --symbol-set english_basic_lowercase_no_arpabet \\\n",
      "    --use-mas \\\n",
      "    --cuda \\\n",
      "    --n-speakers 1 \\\n",
      "    --use-sepconv  \\\n",
      "    --respelling-len-modifier 0 \\\n",
      "    --nheads 2 \\\n",
      "    --num-layers 1 \\\n",
      "    --d-model 128 \\\n",
      "    --d-feedforward 512 \\\n",
      "    --dropout-inputs 0.0 \\\n",
      "    --dropout-layers 0.0 \\\n",
      "    --embedding-dim 384 \\\n",
      "    --freeze-embedding-table \\\n",
      "    --pretrained-embedding-table \\\n",
      "    --gumbel-temp 2 0.5 0.999995 \\\n",
      "    --batch-size 128 \\\n",
      "    --seed 1337 \\\n",
      "    --val-num-to-gen 32 \\\n",
      "    --softdtw-temp 1.0 \\\n",
      "    --dist-func l1 \\\n",
      "    --learning-rate 0.1 \\\n",
      "    --epochs 1000 \\\n",
      "    --val-log-interval 10 \n",
      "\n",
      "# Experiment 2/5: Sweep_DropoutLayer_dropout_layer=0.05\n",
      "./sbatch.sh python train.py \\\n",
      "    --wandb-project-name respeller \\\n",
      "    --chkpt-save-dir /home/s1785140/respeller/exps/Sweep_DropoutLayer_dropout_layer=0.05 \\\n",
      "    --fastpitch-chkpt fastpitch/exps/halved_ljspeech_data_nospaces_noeos/FastPitch_checkpoint_1000.pt \\\n",
      "    --input-type char \\\n",
      "    --symbol-set english_basic_lowercase_no_arpabet \\\n",
      "    --use-mas \\\n",
      "    --cuda \\\n",
      "    --n-speakers 1 \\\n",
      "    --use-sepconv  \\\n",
      "    --respelling-len-modifier 0 \\\n",
      "    --nheads 2 \\\n",
      "    --num-layers 1 \\\n",
      "    --d-model 128 \\\n",
      "    --d-feedforward 512 \\\n",
      "    --dropout-inputs 0.0 \\\n",
      "    --dropout-layers 0.05 \\\n",
      "    --embedding-dim 384 \\\n",
      "    --freeze-embedding-table \\\n",
      "    --pretrained-embedding-table \\\n",
      "    --gumbel-temp 2 0.5 0.999995 \\\n",
      "    --batch-size 128 \\\n",
      "    --seed 1337 \\\n",
      "    --val-num-to-gen 32 \\\n",
      "    --softdtw-temp 1.0 \\\n",
      "    --dist-func l1 \\\n",
      "    --learning-rate 0.1 \\\n",
      "    --epochs 1000 \\\n",
      "    --val-log-interval 10 \n",
      "\n",
      "# Experiment 3/5: Sweep_DropoutLayer_dropout_layer=0.1\n",
      "./sbatch.sh python train.py \\\n",
      "    --wandb-project-name respeller \\\n",
      "    --chkpt-save-dir /home/s1785140/respeller/exps/Sweep_DropoutLayer_dropout_layer=0.1 \\\n",
      "    --fastpitch-chkpt fastpitch/exps/halved_ljspeech_data_nospaces_noeos/FastPitch_checkpoint_1000.pt \\\n",
      "    --input-type char \\\n",
      "    --symbol-set english_basic_lowercase_no_arpabet \\\n",
      "    --use-mas \\\n",
      "    --cuda \\\n",
      "    --n-speakers 1 \\\n",
      "    --use-sepconv  \\\n",
      "    --respelling-len-modifier 0 \\\n",
      "    --nheads 2 \\\n",
      "    --num-layers 1 \\\n",
      "    --d-model 128 \\\n",
      "    --d-feedforward 512 \\\n",
      "    --dropout-inputs 0.0 \\\n",
      "    --dropout-layers 0.1 \\\n",
      "    --embedding-dim 384 \\\n",
      "    --freeze-embedding-table \\\n",
      "    --pretrained-embedding-table \\\n",
      "    --gumbel-temp 2 0.5 0.999995 \\\n",
      "    --batch-size 128 \\\n",
      "    --seed 1337 \\\n",
      "    --val-num-to-gen 32 \\\n",
      "    --softdtw-temp 1.0 \\\n",
      "    --dist-func l1 \\\n",
      "    --learning-rate 0.1 \\\n",
      "    --epochs 1000 \\\n",
      "    --val-log-interval 10 \n",
      "\n",
      "# Experiment 4/5: Sweep_DropoutLayer_dropout_layer=0.2\n",
      "./sbatch.sh python train.py \\\n",
      "    --wandb-project-name respeller \\\n",
      "    --chkpt-save-dir /home/s1785140/respeller/exps/Sweep_DropoutLayer_dropout_layer=0.2 \\\n",
      "    --fastpitch-chkpt fastpitch/exps/halved_ljspeech_data_nospaces_noeos/FastPitch_checkpoint_1000.pt \\\n",
      "    --input-type char \\\n",
      "    --symbol-set english_basic_lowercase_no_arpabet \\\n",
      "    --use-mas \\\n",
      "    --cuda \\\n",
      "    --n-speakers 1 \\\n",
      "    --use-sepconv  \\\n",
      "    --respelling-len-modifier 0 \\\n",
      "    --nheads 2 \\\n",
      "    --num-layers 1 \\\n",
      "    --d-model 128 \\\n",
      "    --d-feedforward 512 \\\n",
      "    --dropout-inputs 0.0 \\\n",
      "    --dropout-layers 0.2 \\\n",
      "    --embedding-dim 384 \\\n",
      "    --freeze-embedding-table \\\n",
      "    --pretrained-embedding-table \\\n",
      "    --gumbel-temp 2 0.5 0.999995 \\\n",
      "    --batch-size 128 \\\n",
      "    --seed 1337 \\\n",
      "    --val-num-to-gen 32 \\\n",
      "    --softdtw-temp 1.0 \\\n",
      "    --dist-func l1 \\\n",
      "    --learning-rate 0.1 \\\n",
      "    --epochs 1000 \\\n",
      "    --val-log-interval 10 \n",
      "\n",
      "# Experiment 5/5: Sweep_DropoutLayer_dropout_layer=0.3\n",
      "./sbatch.sh python train.py \\\n",
      "    --wandb-project-name respeller \\\n",
      "    --chkpt-save-dir /home/s1785140/respeller/exps/Sweep_DropoutLayer_dropout_layer=0.3 \\\n",
      "    --fastpitch-chkpt fastpitch/exps/halved_ljspeech_data_nospaces_noeos/FastPitch_checkpoint_1000.pt \\\n",
      "    --input-type char \\\n",
      "    --symbol-set english_basic_lowercase_no_arpabet \\\n",
      "    --use-mas \\\n",
      "    --cuda \\\n",
      "    --n-speakers 1 \\\n",
      "    --use-sepconv  \\\n",
      "    --respelling-len-modifier 0 \\\n",
      "    --nheads 2 \\\n",
      "    --num-layers 1 \\\n",
      "    --d-model 128 \\\n",
      "    --d-feedforward 512 \\\n",
      "    --dropout-inputs 0.0 \\\n",
      "    --dropout-layers 0.3 \\\n",
      "    --embedding-dim 384 \\\n",
      "    --freeze-embedding-table \\\n",
      "    --pretrained-embedding-table \\\n",
      "    --gumbel-temp 2 0.5 0.999995 \\\n",
      "    --batch-size 128 \\\n",
      "    --seed 1337 \\\n",
      "    --val-num-to-gen 32 \\\n",
      "    --softdtw-temp 1.0 \\\n",
      "    --dist-func l1 \\\n",
      "    --learning-rate 0.1 \\\n",
      "    --epochs 1000 \\\n",
      "    --val-log-interval 10 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def print_train_commands_for_hparam_sweep(exp_name, hparams, prettyprint):\n",
    "    # TODO add prettyprint option if false  command is all on one line, \n",
    "    # might make pasting a lot of jobs more robust through terminal\n",
    "    \n",
    "    gammas = hparams['gammas']\n",
    "    lrs = hparams['lrs']\n",
    "    arch_cfgs = hparams['arch_cfgs']\n",
    "    freezes = hparams['freeze_embeddings']\n",
    "\n",
    "    product = list(itertools.product(*[\n",
    "        hparams['gammas'],\n",
    "        hparams['lrs'],\n",
    "        hparams['arch_cfgs'],\n",
    "        hparams['freeze_embeddings'],\n",
    "        hparams['embedding_dim'],\n",
    "        hparams['seeds'],\n",
    "        hparams['src_key_padding_mask'],\n",
    "        hparams['gumbel_temps'],\n",
    "        hparams['desired_text_lens'],\n",
    "        hparams['dropout_layers'],\n",
    "    ]))\n",
    "    \n",
    "    for i, (\n",
    "        gamma, \n",
    "        lr, \n",
    "        arch, \n",
    "        freeze, \n",
    "        embed_dim, \n",
    "        seed, \n",
    "        padmask,\n",
    "        gumbel_temp,\n",
    "        desired_text_len,\n",
    "        dropout_layer,\n",
    "    ) in enumerate(product):\n",
    "        # model_dir = f\"{exp_name}-bsz{hparams['bsz']}-gamma={gamma}-lr={lr}-nheads={arch['nheads']}-nlayers={arch['num_layers']}-dim={arch['d_model']}-ffdim{arch['ffdim']}\"\n",
    "        # model_dir = f\"{exp_name}-PadMask{padmask}-DropInput{arch['dropout_inputs']}-ffdim{arch['ffdim']}\"\n",
    "        model_dir = f\"{exp_name}_{dropout_layer=}\"\n",
    "        sys_argv = [\n",
    "            'train.py',\n",
    "            '--wandb-project-name', hparams['wandb_project_name'],\n",
    "            '--chkpt-save-dir', f'/home/s1785140/respeller/exps/{model_dir}', \n",
    "            '--fastpitch-chkpt', hparams['tts_model_path'],\n",
    "            '--input-type', 'char',\n",
    "            '--symbol-set', 'english_basic_lowercase_no_arpabet',\n",
    "            '--use-mas',\n",
    "            '--cuda',\n",
    "            '--n-speakers', '1',\n",
    "            '--use-sepconv',\n",
    "            '' if padmask else '--no-src-key-padding-mask',\n",
    "            '--respelling-len-modifier', str(desired_text_len),\n",
    "            '--nheads', str(arch['nheads']),\n",
    "            '--num-layers', str(arch['num_layers']),\n",
    "            '--d-model', str(arch['d_model']),\n",
    "            '--d-feedforward', str(arch['ffdim']),\n",
    "            '--dropout-inputs', str(arch['dropout_inputs']),\n",
    "            '--dropout-layers', str(dropout_layer),\n",
    "            '--embedding-dim', str(embed_dim),\n",
    "            '--freeze-embedding-table' if freeze else '',\n",
    "            '--pretrained-embedding-table' if hparams['pretrained_embedding_table'] else '',\n",
    "            '--gumbel-temp', str(gumbel_temp[0]), str(gumbel_temp[1]), str(gumbel_temp[2]),\n",
    "            '--batch-size', str(hparams['bsz']),\n",
    "            '--seed', str(seed),\n",
    "            '--val-num-to-gen', '32',\n",
    "            '--softdtw-temp', str(gamma),\n",
    "            '--dist-func', \"l1\",\n",
    "            '--learning-rate', str(lr),\n",
    "            '--epochs', str(hparams['epochs']), \n",
    "            '--val-log-interval', '10',\n",
    "            '--resume' if hparams['resume'] else '',\n",
    "        ]\n",
    "        print()\n",
    "        print(f\"# Experiment {i+1}/{len(product)}: {model_dir}\")\n",
    "        print(\"./sbatch.sh python train.py \\\\\")\n",
    "        prettyprint_cla_for_training(sys_argv)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"cd /home/s1785140/respeller\")\n",
    "\n",
    "exps = {\n",
    "    # 18th jan 2023 sweep gammas and freeze embeddings\n",
    "    'SweepGammaFreeze': {\n",
    "        'bsz': 32,\n",
    "        'wandb_project_name': 'respeller',\n",
    "        'tts_model_path': fastpitch_chkpt,\n",
    "        'gammas': [10.0,1.0,0.1,0.01], \n",
    "        'lrs': [0.1],\n",
    "        'arch_cfgs': [\n",
    "            # {'nheads': 4, 'num_layers': 4, 'd_model':512},\n",
    "            # {'nheads': 2, 'num_layers': 2, 'd_model':256},\n",
    "            {'nheads': 1, 'num_layers': 1, 'd_model':128},\n",
    "        ],\n",
    "        'embedding_dim': [384],\n",
    "        'freeze_embeddings': [True, False],\n",
    "        'pretrained_embedding_table': True,\n",
    "        'seeds': [1337],\n",
    "        'resume': False,\n",
    "    },\n",
    "    \n",
    "    # 18th jan 2023 init embedding table from scratch and change embedding table size\n",
    "    'RandInitEmbedding': {\n",
    "        'bsz': 32,\n",
    "        'wandb_project_name': 'respeller',\n",
    "        'tts_model_path': fastpitch_chkpt,\n",
    "        'gammas': [1.0], \n",
    "        'lrs': [0.1],\n",
    "        'arch_cfgs': [\n",
    "            {'nheads': 1, 'num_layers': 1, 'd_model':128},\n",
    "        ],\n",
    "        'embedding_dim': [384, 128, 64],\n",
    "        'freeze_embeddings': [False],\n",
    "        'pretrained_embedding_table': False,\n",
    "        'seeds': [1337],\n",
    "        'resume': False,\n",
    "    },\n",
    "    \n",
    "    # random seed exp to investigate collapse of b to p in respellings\n",
    "    '10RandSeeds': {\n",
    "        'bsz': 32,\n",
    "        'wandb_project_name': 'respeller',\n",
    "        'tts_model_path': fastpitch_chkpt,\n",
    "        'gammas': [1.0], \n",
    "        'lrs': [0.1],\n",
    "        'arch_cfgs': [\n",
    "            {'nheads': 1, 'num_layers': 1, 'd_model':128},\n",
    "        ],\n",
    "        'embedding_dim': [384],\n",
    "        'freeze_embeddings': [True],\n",
    "        'pretrained_embedding_table': True,\n",
    "        'seeds': [1337, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "        'resume': False,\n",
    "    },\n",
    "    \n",
    "    # random seed exp to investigate collapse of b to p in respellings\n",
    "    'Sweep_PaddingMask_DropoutInputs_FFdim': {\n",
    "        'bsz': 128,\n",
    "        'wandb_project_name': 'respeller',\n",
    "        'tts_model_path': fastpitch_chkpt,\n",
    "        'gammas': [1.0], \n",
    "        'lrs': [0.1],\n",
    "        'arch_cfgs': [\n",
    "            {'nheads': 2, 'num_layers': 1, 'd_model':128, 'dropout_inputs': 0.0, 'ffdim': 2048},\n",
    "            {'nheads': 2, 'num_layers': 1, 'd_model':128, 'dropout_inputs': 0.1, 'ffdim': 2048},\n",
    "            {'nheads': 2, 'num_layers': 1, 'd_model':128, 'dropout_inputs': 0.0, 'ffdim': 512},\n",
    "            {'nheads': 2, 'num_layers': 1, 'd_model':128, 'dropout_inputs': 0.1, 'ffdim': 512},\n",
    "        ],\n",
    "        'embedding_dim': [384],\n",
    "        'freeze_embeddings': [True],\n",
    "        'pretrained_embedding_table': True,\n",
    "        'seeds': [1337],\n",
    "        'resume': False,\n",
    "        'src_key_padding_mask': [True, False],\n",
    "        'gumbel_temp': [(2, 0.5, 0.999995)],\n",
    "    },\n",
    "    \n",
    "    # gumbel temp\n",
    "    'Sweep_GumbelTemp': {\n",
    "        'bsz': 128,\n",
    "        'wandb_project_name': 'respeller',\n",
    "        'tts_model_path': fastpitch_chkpt,\n",
    "        'gammas': [1.0], \n",
    "        'lrs': [0.1],\n",
    "        'arch_cfgs': [\n",
    "            {'nheads': 2, 'num_layers': 1, 'd_model':128, 'dropout_inputs': 0.0, 'ffdim': 512},\n",
    "        ],\n",
    "        'embedding_dim': [384],\n",
    "        'freeze_embeddings': [True],\n",
    "        'pretrained_embedding_table': True,\n",
    "        'seeds': [1337],\n",
    "        'resume': False,\n",
    "        'src_key_padding_mask': [True],\n",
    "        'gumbel_temps': [\n",
    "            (2, 0.5, 0.999995),\n",
    "            (2, 0.5, 0.99999),\n",
    "            (2, 0.5, 0.99995),\n",
    "            (2, 0.5, 1.0), # temp never decays\n",
    "        ],\n",
    "    },\n",
    "    \n",
    "    'Sweep_DesiredTextLen': {\n",
    "        'bsz': 128,\n",
    "        'epochs': 1000,\n",
    "        'wandb_project_name': 'respeller',\n",
    "        'tts_model_path': fastpitch_chkpt,\n",
    "        'gammas': [1.0], \n",
    "        'lrs': [0.1],\n",
    "        'arch_cfgs': [\n",
    "            {'nheads': 2, 'num_layers': 1, 'd_model':128, 'dropout_inputs': 0.0, 'ffdim': 512},\n",
    "        ],\n",
    "        'embedding_dim': [384],\n",
    "        'freeze_embeddings': [True],\n",
    "        'pretrained_embedding_table': True,\n",
    "        'seeds': [1337],\n",
    "        'resume': False,\n",
    "        'src_key_padding_mask': [True],\n",
    "        'gumbel_temps': [\n",
    "            (2, 0.5, 0.999995),\n",
    "        ],\n",
    "        'desired_text_lens': [-2,-1,0,1,2],\n",
    "        'dropout_layers': [0.0],\n",
    "    },\n",
    "    \n",
    "    'Sweep_DropoutLayer': {\n",
    "        'bsz': 128,\n",
    "        'epochs': 1000,\n",
    "        'wandb_project_name': 'respeller',\n",
    "        'tts_model_path': fastpitch_chkpt,\n",
    "        'gammas': [1.0], \n",
    "        'lrs': [0.1],\n",
    "        'arch_cfgs': [\n",
    "            {'nheads': 2, 'num_layers': 1, 'd_model':128, 'dropout_inputs': 0.0, 'ffdim': 512},\n",
    "        ],\n",
    "        'embedding_dim': [384],\n",
    "        'freeze_embeddings': [True],\n",
    "        'pretrained_embedding_table': True,\n",
    "        'seeds': [1337],\n",
    "        'resume': False,\n",
    "        'src_key_padding_mask': [True],\n",
    "        'gumbel_temps': [\n",
    "            (2, 0.5, 0.999995),\n",
    "        ],\n",
    "        'desired_text_lens': [0],\n",
    "        'dropout_layers': [0.0,0.05,0.1,0.2,0.3],\n",
    "    },\n",
    "}\n",
    "\n",
    "exp_names = [\n",
    "    # 'SweepGammaFreeze',\n",
    "    # 'RandInitEmbedding',\n",
    "    # '10RandSeeds',\n",
    "    # 'Sweep_PaddingMask_DropoutInputs_FFdim',\n",
    "    # 'Sweep_GumbelTemp',\n",
    "    # 'Sweep_DesiredTextLen',\n",
    "    'Sweep_DropoutLayer',\n",
    "]\n",
    "exp_name = exp_names[0]\n",
    "hparams = exps[exp_name]\n",
    "print_train_commands_for_hparam_sweep(exp_name, hparams, prettyprint=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53034b89-b847-4927-a604-cc6cb78516c2",
   "metadata": {},
   "source": [
    "# dev train loop + model for respeller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752f248-d859-4a78-8c79-4c65080948b6",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cbcac7-2394-41a3-8fe2-c8c486309b08",
   "metadata": {},
   "source": [
    "## arguments to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a95a42be-2ea3-4b7e-a5e3-f506fdf2aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(parser):\n",
    "    \"\"\"Parse commandline arguments\"\"\"\n",
    "    parser.add_argument('--wandb-project-name', type=str, required=True,\n",
    "                        help='Which wandb project to add this job to')\n",
    "    parser.add_argument('-o', '--chkpt-save-dir', type=str, required=True,\n",
    "                        help='Directory to save checkpoints')\n",
    "    parser.add_argument('-d', '--dataset-path', type=str, default='./',\n",
    "                        help='Path to dataset')\n",
    "    parser.add_argument('--log-file', type=str, default=None,\n",
    "                        help='Path to a DLLogger log file')\n",
    "\n",
    "    train = parser.add_argument_group('training setup')\n",
    "    train.add_argument('--cuda', action='store_true',\n",
    "                      help='Enable GPU training')\n",
    "    train.add_argument('--batch-size', type=int, default=16,\n",
    "                      help='Batchsize (this is divided by number of GPUs if running Data Distributed Parallel Training)')\n",
    "    train.add_argument('--val-num-to-gen', type=int, default=32,\n",
    "                      help='Number of samples to generate in validation (determines how many samples show up in wandb')\n",
    "    train.add_argument('--seed', type=int, default=1337,\n",
    "                       help='Seed for PyTorch random number generators')\n",
    "    train.add_argument('--grad-accumulation', type=int, default=1,\n",
    "                       help='Training steps to accumulate gradients for')\n",
    "    train.add_argument('--epochs', type=int, default=100, #required=True,\n",
    "                       help='Number of total epochs to run')\n",
    "    train.add_argument('--max-iters-per-epoch', type=int, default=None, \n",
    "                       help='Number of total batches to iterate through each epoch (reduce this to small number to quickly test whole training loop)')\n",
    "    train.add_argument('--epochs-per-checkpoint', type=int, default=10,\n",
    "                       help='Number of epochs per checkpoint')\n",
    "    train.add_argument('--checkpoint-path', type=str, default=None,\n",
    "                       help='Checkpoint path to resume train')\n",
    "    train.add_argument('--resume', action='store_true',\n",
    "                       help='Resume train from the last available checkpoint')\n",
    "    train.add_argument('--val-log-interval', type=int, default=5,\n",
    "                       help='How often to generate melspecs/audio for respellings and log to wandb')\n",
    "    train.add_argument('--speech-length-penalty-training', action='store_true',\n",
    "                       help='Whether or not to encourage model to output similar length outputs\\\n",
    "                       as the ground truth. Idea from V2C: Visual Voice Cloning (Chen et al. 2021)')\n",
    "    train.add_argument('--skip-before-train-loop-validation', action='store_true',\n",
    "                       help='Skip running validation before model training begins (mostly for speeding up testing of actual training loop)')\n",
    "    train.add_argument('--avg-loss-by-speech_lens', action='store_true',\n",
    "                       help='Average the softdtw loss according to number of timesteps in predicted sequence')\n",
    "    train.add_argument('--softdtw-temp', type=float, default=0.01,\n",
    "                      help='How hard/soft to make min operation. Minimum is recovered by setting this to 0.')\n",
    "    train.add_argument('--softdtw-bandwidth', type=int, default=120,\n",
    "                      help='Bandwidth for pruning paths in alignment matrix when calculating SoftDTW')\n",
    "    train.add_argument('--dist-func', type=str, default=\"l1\",\n",
    "                       help='What distance function to use in softdtw loss calculation')\n",
    "    \n",
    "    opt = parser.add_argument_group('optimization setup')\n",
    "    opt.add_argument('--optimizer', type=str, default='lamb', choices=['adam', 'lamb'],\n",
    "                     help='Optimization algorithm')\n",
    "    opt.add_argument('-lr', '--learning-rate', default=0.1, type=float,\n",
    "                     help='Learning rate')\n",
    "    opt.add_argument('--weight-decay', default=1e-6, type=float,\n",
    "                     help='Weight decay')\n",
    "    opt.add_argument('--grad-clip-thresh', default=1000.0, type=float,\n",
    "                     help='Clip threshold for gradients')\n",
    "    opt.add_argument('--warmup-steps', type=int, default=1000,\n",
    "                     help='Number of steps for lr warmup')\n",
    "\n",
    "    arch = parser.add_argument_group('architecture')\n",
    "    arch.add_argument('--d-model', type=int, default=512,\n",
    "                       help='Hidden dimension of tranformer')\n",
    "    arch.add_argument('--latent-temp', type=tuple, default=(2, 0.5, 0.999995),\n",
    "                       help='Temperature annealling parameters for Gumbel-Softmax (start, end, decay)')\n",
    "    arch.add_argument('--freeze-embedding-table', action='store_true',\n",
    "                      help='Whether or not to allow grapheme embedding input table for EncoderRespeller to be updated.')\n",
    "    \n",
    "    parser.add_argument('--feature', action='store_true')\n",
    "\n",
    "    pretrained_tts = parser.add_argument_group('pretrained tts model')\n",
    "    # pretrained_tts.add_argument('--fastpitch-with-mas', type=bool, default=True,\n",
    "    #                   help='Whether or not fastpitch was trained with Monotonic Alignment Search (MAS)')\n",
    "    pretrained_tts.add_argument('--fastpitch-chkpt', type=str, required=True,\n",
    "                      help='Path to pretrained fastpitch checkpoint')\n",
    "    pretrained_tts.add_argument('--input-type', type=str, default='char',\n",
    "                      choices=['char', 'phone', 'pf', 'unit'],\n",
    "                      help='Input symbols used, either char (text), phone, pf '\n",
    "                      '(phonological feature vectors) or unit (quantized acoustic '\n",
    "                      'representation IDs)')\n",
    "    pretrained_tts.add_argument('--symbol-set', type=str, default='english_basic_lowercase',\n",
    "                      help='Define symbol set for input sequences. For quantized '\n",
    "                      'unit inputs, pass the size of the vocabulary.')\n",
    "    pretrained_tts.add_argument('--n-speakers', type=int, default=1,\n",
    "                      help='Condition on speaker, value > 1 enables trainable '\n",
    "                      'speaker embeddings.')\n",
    "    # pretrained_tts.add_argument('--use-sepconv', type=bool, default=True,\n",
    "    #                   help='Use depthwise separable convolutions')\n",
    "    \n",
    "    audio = parser.add_argument_group('log generated audio')\n",
    "    audio.add_argument('--hifigan', type=str, default='/home/s1785140/pretrained_models/hifigan/ljspeech/LJ_V1/generator_v1',\n",
    "                       help='Path to HiFi-GAN audio checkpoint')\n",
    "    audio.add_argument('--hifigan-config', type=str, default='/home/s1785140/pretrained_models/hifigan/ljspeech/LJ_V1/config.json',\n",
    "                       help='Path to HiFi-GAN audio config file')\n",
    "    audio.add_argument('--sampling-rate', type=int, default=22050,\n",
    "                       help='Sampling rate for output audio')\n",
    "    audio.add_argument('--hop-length', type=int, default=256,\n",
    "                       help='STFT hop length for estimating audio length from mel size')\n",
    "    \n",
    "    data = parser.add_argument_group('dataset parameters')\n",
    "    data.add_argument('--wordaligned-speechreps', type=str, default='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels',\n",
    "                       help='Path to directory of wordaligned speechreps/mels. Inside are folders\\\n",
    "                       each named as a wordtype and containing tensors of word aligned speechreps for each example')\n",
    "    data.add_argument('--train-wordlist', type=str, default='/home/s1785140/data/ljspeech_fastpitch/respeller_train_words.json',\n",
    "                       help='Path to words that are used to train respeller')\n",
    "    data.add_argument('--val-wordlist', type=str, default='/home/s1785140/data/ljspeech_fastpitch/respeller_dev_words.json',\n",
    "                       help='Path to words that are used to report validation metrics for respeller')\n",
    "    data.add_argument('--max-examples-per-wordtype', type=int, default=1,\n",
    "                       help='Path to words that are used to report validation metrics for respeller')\n",
    "    # data.add_argument('--add-spaces', action='store_true',\n",
    "    #                   help='Whether to add leading space and trailing EOS symbol to word during training and test.')\n",
    "    \n",
    "    \n",
    "    cond = parser.add_argument_group('conditioning on additional attributes')\n",
    "    dist = parser.add_argument_group('distributed training setup')\n",
    "\n",
    "    return parser\n",
    "\n",
    "def load_checkpoint(args, model, filepath):\n",
    "    if args.local_rank == 0:\n",
    "        print(f'Loading model and optimizer state from {filepath}')\n",
    "    checkpoint = torch.load(filepath, map_location='cpu')\n",
    "    sd = {k.replace('module.', ''): v\n",
    "          for k, v in checkpoint['state_dict'].items()}\n",
    "    getattr(model, 'module', model).load_state_dict(sd)\n",
    "    return model\n",
    "\n",
    "def load_respeller_checkpoint(args, model, filepath, optimizer, epoch, total_iter):\n",
    "    if args.local_rank == 0:\n",
    "        print(f'Loading model and optimizer state from {filepath}')\n",
    "    checkpoint = torch.load(filepath, map_location='cpu')\n",
    "    epoch[0] = checkpoint['epoch'] + 1\n",
    "    total_iter[0] = checkpoint['iteration']\n",
    "    sd = {k.replace('module.', ''): v\n",
    "          for k, v in checkpoint['state_dict'].items()}\n",
    "    getattr(model, 'module', model).load_state_dict(sd)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model\n",
    "\n",
    "def last_checkpoint(output):\n",
    "    saved = sorted(\n",
    "        glob.glob(f'{output}/respeller_checkpoint_*.pt'),\n",
    "        key=lambda f: int(re.search('_(\\d+).pt', f).group(1)))\n",
    "\n",
    "    def corrupted(fpath):\n",
    "        try:\n",
    "            torch.load(fpath, map_location='cpu')\n",
    "            return False\n",
    "        except:\n",
    "            warnings.warn(f'Cannot load {fpath}')\n",
    "            return True\n",
    "\n",
    "    if len(saved) >= 1 and not corrupted(saved[-1]):\n",
    "        return saved[-1]\n",
    "    elif len(saved) >= 2:\n",
    "        return saved[-2]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def init_embedding_weights(source_tensor, target_tensor):\n",
    "    \"\"\"copy weights inplace from source tensor to target tensor\"\"\"\n",
    "    target_tensor.requires_grad = False\n",
    "    target_tensor.copy_(source_tensor.clone().detach())\n",
    "    target_tensor.requires_grad = True\n",
    "\n",
    "def load_pretrained_fastpitch(args):\n",
    "    # load chkpt\n",
    "    device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "    model_config = fastpitch_model.get_model_config('FastPitch', args)\n",
    "    fastpitch = fastpitch_model.get_model('FastPitch', model_config, device, forward_is_infer=True)\n",
    "    load_checkpoint(args, fastpitch, args.fastpitch_chkpt)\n",
    "    # get information about grapheme embedding table\n",
    "    n_symbols = fastpitch.encoder.word_emb.weight.size(0)\n",
    "    grapheme_embedding_dim = fastpitch.encoder.word_emb.weight.size(1)\n",
    "    return fastpitch, n_symbols, grapheme_embedding_dim, model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b41f829-86ec-4d6f-a829-cd5fa032c819",
   "metadata": {},
   "source": [
    "# beginning of main(), parse Command Line Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4ff82f08-8500-40f7-ab53-0d2d76b3c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Respeller Training', allow_abbrev=False)\n",
    "parser = parse_args(parser)\n",
    "args, _unk_args = parser.parse_known_args()\n",
    "\n",
    "parser = fastpitch_model.parse_model_args('FastPitch', parser)\n",
    "args, unk_args = parser.parse_known_args()\n",
    "if len(unk_args) > 0:\n",
    "    raise ValueError(f'Invalid options {unk_args}')\n",
    "\n",
    "if args.cuda:\n",
    "    args.num_gpus = torch.cuda.device_count()\n",
    "    args.distributed_run = args.num_gpus > 1\n",
    "    args.batch_size = int(args.batch_size / args.num_gpus)\n",
    "else:\n",
    "    args.distributed_run = False\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "if args.distributed_run:\n",
    "    mp.spawn(train, nprocs=args.num_gpus, args=(args,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e299494f-abbe-4db6-b3d7-566ba8f9f093",
   "metadata": {},
   "source": [
    "# WANDB - weights and biases init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "544eb7bc-6a8c-4de5-bbcc-50ed422f8bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login() # needed for wandb integration with jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7838aa19-5f86-4248-b905-6fc6e56958db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: \"WANDB_NOTEBOOK_NAME\"=\"respeller-dev-train-ipynb\"\n"
     ]
    }
   ],
   "source": [
    "%env \"WANDB_NOTEBOOK_NAME\" \"respeller-dev-train-ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3c5ca-3e92-4a05-817d-4a42f01150e6",
   "metadata": {},
   "source": [
    "## set project name and run name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d82c2847-c9a9-4ba4-908c-b0fa99defc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2023-01-23 16:28:52.721457\n",
      "date and time = 23/01/2023-16:28:52\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d/%m/%Y-%H:%M:%S\")\n",
    "print(\"date and time =\", dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9f57d277-14e0-4e11-a95b-2e7a52a6312d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:n2h814te) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>memory/allocated</td><td>██████▁▁▁▁▁▁</td></tr><tr><td>memory/free</td><td>▁▁▁▁▁▁██████</td></tr><tr><td>memory/reserved</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>memory/total</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/epoch_num</td><td>▁█</td></tr><tr><td>train/epoch_time</td><td>█▁</td></tr><tr><td>train/iter_loss</td><td>▆▆▄▂▃▁▃█▁▄▂▂</td></tr><tr><td>train/iter_sl_penalty_coef</td><td>▂▂▅▃▄▄▇▄▁▂█▅</td></tr><tr><td>train/iter_time</td><td>█▁▁▁▂▁▂▁▁▁▁▁</td></tr><tr><td>val/epoch_loss</td><td>▁▁▁</td></tr><tr><td>val/epoch_loss_with_sl_penalty</td><td>▁▁▁</td></tr><tr><td>val/epoch_sl_penalty_coef</td><td>▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>memory/allocated</td><td>0.27593</td></tr><tr><td>memory/free</td><td>0.31127</td></tr><tr><td>memory/reserved</td><td>0.5872</td></tr><tr><td>memory/total</td><td>11.55472</td></tr><tr><td>train/epoch_loss</td><td>8898.72957</td></tr><tr><td>train/epoch_num</td><td>2</td></tr><tr><td>train/epoch_time</td><td>0.60068</td></tr><tr><td>train/iter_loss</td><td>7791.86182</td></tr><tr><td>train/iter_sl_penalty_coef</td><td>1.42648</td></tr><tr><td>train/iter_time</td><td>0.03791</td></tr><tr><td>val/epoch_loss</td><td>10501.90293</td></tr><tr><td>val/epoch_loss_with_sl_penalty</td><td>16282.22337</td></tr><tr><td>val/epoch_sl_penalty_coef</td><td>1.49868</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Sweep_DropoutLayer_run-1_23/01/2023-15:56:56</strong> at: <a href=\"https://wandb.ai/jonojace/respeller-training/runs/n2h814te\" target=\"_blank\">https://wandb.ai/jonojace/respeller-training/runs/n2h814te</a><br/>Synced 6 W&B file(s), 6 media file(s), 26 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230123_155656-n2h814te/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:n2h814te). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e67fb62b014d509fcdd639949f9f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669368278235196, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/disk/nfs/ostrom/s1785140/respeller/wandb/run-20230123_162853-f6fy0kp2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jonojace/respeller-training/runs/f6fy0kp2\" target=\"_blank\">test_dev_run-1_23/01/2023-16:28:52</a></strong> to <a href=\"https://wandb.ai/jonojace/respeller-training\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/jonojace/respeller-training\" target=\"_blank\">https://wandb.ai/jonojace/respeller-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/jonojace/respeller-training/runs/f6fy0kp2\" target=\"_blank\">https://wandb.ai/jonojace/respeller-training/runs/f6fy0kp2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/jonojace/respeller-training/runs/f6fy0kp2?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f681c823af0>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = 1\n",
    "\n",
    "project_name = args.wandb_project_name #\"respeller-dev-train-ipynb\"\n",
    "exp_name = \"test_dev\"\n",
    "experiment_name = f\"{exp_name}_run-{run}_{dt_string}\"\n",
    "\n",
    "# store important information into WANDB config for easier tracking of experiments\n",
    "# add all key values from parser\n",
    "wandb_config = vars(args)\n",
    "wandb.init(\n",
    "    project=project_name,\n",
    "    name=experiment_name,\n",
    "    config=wandb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924bddc-abcd-42d1-b3db-30797ab59b1e",
   "metadata": {},
   "source": [
    "# 'train()' - forward pass through model to get loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e209554-640e-40e3-953b-32512f7d94ff",
   "metadata": {},
   "source": [
    "## create / load models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8187a843-19ec-4aaf-842d-0e442a93fdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and optimizer state from fastpitch/exps/halved_ljspeech_data_nospaces_noeos/FastPitch_checkpoint_1000.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SoftDTW()"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank = 0\n",
    "device = 'cuda'\n",
    "\n",
    "args.local_rank = rank\n",
    "tts, n_symbols, grapheme_embedding_dim, model_config = load_pretrained_fastpitch(args)\n",
    "tts.to(device)\n",
    "\n",
    "respeller = EncoderRespeller(n_symbols=n_symbols, pretrained_tts=tts, d_model=args.d_model)\n",
    "respeller.to(device)\n",
    "\n",
    "# quantiser = GumbelVectorQuantizer(\n",
    "#     in_dim=args.d_model,\n",
    "#     codebook_size=n_symbols,  # number of codebook entries\n",
    "#     embedding_dim=grapheme_embedding_dim,\n",
    "#     temp=args.latent_temp,\n",
    "# )\n",
    "# quantiser.to(device)\n",
    "\n",
    "# init_embedding_weights(tts.encoder.word_emb.weight.unsqueeze(0), quantiser.vars)\n",
    "\n",
    "\n",
    "# batch_size, len_x, len_y, dims = 8, 15, 12, 5\n",
    "# x = torch.rand((batch_size, len_x, dims), requires_grad=True)\n",
    "# y = torch.rand((batch_size, len_y, dims))\n",
    "\n",
    "# criterion = SoftDTW(use_cuda=True, gamma=0.1, dist_func=F.mse_loss)\n",
    "criterion = SoftDTW(use_cuda=True, gamma=args.softdtw_temp)\n",
    "# input should be size [bsz, seqlen, dim]\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ca344-d795-42db-ad89-4302e826df2e",
   "metadata": {},
   "source": [
    "### load HiFiGAN vocoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "950abcf8-088b-423f-88e3-e8d92d29d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocoder(args, device):\n",
    "    \"\"\"Load HiFi-GAN vocoder from checkpoint\"\"\"\n",
    "    checkpoint_data = torch.load(args.hifigan)\n",
    "    vocoder_config = fastpitch_model.get_model_config('HiFi-GAN', args)\n",
    "    vocoder = fastpitch_model.get_model('HiFi-GAN', vocoder_config, device)\n",
    "    vocoder.load_state_dict(checkpoint_data['generator'])\n",
    "    vocoder.remove_weight_norm()\n",
    "    vocoder.eval()\n",
    "    return vocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "20403dd1-6daa-4680-8b06-4a86c89e2e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocoder = load_vocoder(args, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cf3977-0de7-452d-a07d-190ba3555fd2",
   "metadata": {},
   "source": [
    "## forward pass through model with dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6115a992-4221-4698-944b-d405782da458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batches = []\n",
    "# symbol_set = 'english_basic_lowercase'\n",
    "# text_cleaners = []\n",
    "# gt_log_mel = torch.load('/home/s1785140/data/ljspeech_fastpitch/mels/LJ001-0001.pt').cuda().unsqueeze(0).transpose(1,2) # intro batch dimension + [bsz, seqlen, dim]\n",
    "# raw_text = 'printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the exhibition'\n",
    "\n",
    "# # process text using same processor as fastpitch\n",
    "# tp = TextProcessor(symbol_set, text_cleaners)\n",
    "# text = torch.LongTensor(tp.encode_text(raw_text)).unsqueeze(0).cuda()\n",
    "\n",
    "# batches.append((text, gt_log_mel))\n",
    "\n",
    "# # for batch in batches:\n",
    "# batch = batches[0]\n",
    "    \n",
    "# ###############################################################################################################\n",
    "# # text, ssl_reps, e2e_asr_predictions, gt_log_mel = batch\n",
    "# text, gt_log_mel = batch\n",
    "\n",
    "# ###############################################################################################################\n",
    "# # create inputs\n",
    "# # if args.use_acoustic_input:\n",
    "# #     inputs = inputs.concat(ssl_reps)\n",
    "\n",
    "# ###############################################################################################################\n",
    "# # forward pass\n",
    "# g_embeddings, g_embedding_indices = respeller(text[:13])\n",
    "\n",
    "# print('examine the padding embedding')\n",
    "# pad_idx = 0\n",
    "# respeller.quantiser.vars[:,pad_idx,:] \n",
    "\n",
    "# masked_g_embedding_indices = torch.zeros(g_embedding_indices.size())\n",
    "\n",
    "# masked_g_embedding_indices.size()\n",
    "\n",
    "# g_embeddings.size()\n",
    "\n",
    "# masked_g_embeddings = torch.zeros(g_embeddings.size())\n",
    "\n",
    "# text_lens = [10]\n",
    "# bsz = len(text_lens)\n",
    "# for i, text_len in enumerate(text_lens):\n",
    "#     #indices\n",
    "#     indices = g_embedding_indices[i, :text_len]\n",
    "#     masked_g_embedding_indices[i, :text_len] = indices\n",
    "#     #embeddings\n",
    "#     embeddings = g_embeddings[i, :text_len, :]\n",
    "#     masked_g_embeddings[i, :text_len, :] = embeddings\n",
    "    \n",
    "    \n",
    "\n",
    "# n_symbols\n",
    "\n",
    "# g_embedding_indices.size()\n",
    "\n",
    "# g_embeddings.size()\n",
    "\n",
    "# padding_idx = 0\n",
    "# mask = (g_embedding_indices != padding_idx).unsqueeze(2)\n",
    "# mask.size()\n",
    "\n",
    "# log_mel, dec_lens, _dur_pred, _pitch_pred = tts(g_embeddings, skip_embeddings=True, ids=g_embedding_indices)\n",
    "# # log_mel [bsz, dim, seqlen]\n",
    "# log_mel = log_mel.transpose(1,2)\n",
    "# # log_mel [bsz, seqlen, dim]\n",
    "\n",
    "# print(f'{log_mel.size()=}')\n",
    "# print(f'{gt_log_mel.size()=}')\n",
    "\n",
    "# ###############################################################################################################\n",
    "# # calculate val_losses\n",
    "# # respelling_loss = respelling_loss_fn(respelling, e2e_asr_predictions)\n",
    "# acoustic_loss = criterion(log_mel, gt_log_mel)\n",
    "\n",
    "# # average loss over frames \n",
    "# acoustic_loss = acoustic_loss / dec_lens\n",
    "# # mel_loss = (mel_loss * mel_mask).sum() / mel_mask.sum()\n",
    "\n",
    "# ###############################################################################################################\n",
    "# # backward pass\n",
    "# loss = acoustic_loss \n",
    "\n",
    "# print(f'{loss=}')\n",
    "\n",
    "# # loss.backward()\n",
    "\n",
    "# ###############################################################################################################\n",
    "# # log tensorboard metrics\n",
    "\n",
    "# ###############################################################################################################\n",
    "# # validation set evaluation\n",
    "\n",
    "# def plot_spectrogram(log_mel, figsize=(15,5), wandb_log=False, image_name=\"\"):\n",
    "#     plt.figure(figsize=figsize)\n",
    "#     librosa.display.specshow(log_mel, x_axis='frames', y_axis='linear')\n",
    "#     plt.colorbar()\n",
    "#     if wandb_log:\n",
    "#         wandb.log({image_name: wandb.Image(plt, caption=image_name)})\n",
    "\n",
    "# batch_index = 0\n",
    "# print(f'{log_mel[batch_index].transpose(0,1).size()=}')\n",
    "# plot_spectrogram(log_mel[batch_index].transpose(0,1).detach().cpu().numpy())\n",
    "\n",
    "# # # play audio\n",
    "# # import IPython.display as ipd\n",
    "# # audio = vocoder(log_mel[batch_index].transpose(0,1).detach().unsqueeze(0))\n",
    "# # ipd.Audio(audio, rate=22050)\n",
    "\n",
    "# batch_index = 0\n",
    "# plot_spectrogram(gt_log_mel[batch_index].transpose(0,1).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e647fb-37af-41ee-940d-83207764ebc4",
   "metadata": {},
   "source": [
    "# develop respeller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d58a6e33-cd99-4eff-ac6c-945a6b15f268",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordaligned_speechreps_dir = '/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels' # path to directory that contains folders of word aligned speech reps\n",
    "wordlist = ['identifies','mash','player','russias','techniques'] # txt file for the words to include speech reps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "31370ea3-2979-45de-bf96-1f76edc83436",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_and_melfilepaths = []\n",
    "for word in wordlist:\n",
    "    # find all word aligned mels for the word\n",
    "    word_dir = os.path.join(wordaligned_speechreps_dir, word)\n",
    "    mel_files = os.listdir(word_dir)\n",
    "    for mel_file in mel_files:\n",
    "        mel_file_path = os.path.join(word_dir, mel_file)\n",
    "        token_and_melfilepaths.append((word, mel_file_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d772b957-53e6-48ca-b848-8fcc4c5818e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('identifies',\n",
       "  '/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels/identifies/identifies__LJ040-0003__occ1__seqlen68.pt'),\n",
       " ('mash',\n",
       "  '/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels/mash/mash__LJ020-0011__occ1__seqlen28.pt'),\n",
       " ('player',\n",
       "  '/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels/player/player__LJ041-0014__occ1__seqlen49.pt'),\n",
       " ('russias',\n",
       "  '/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels/russias/russias__LJ042-0204__occ1__seqlen38.pt'),\n",
       " ('techniques',\n",
       "  '/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels/techniques/techniques__LJ046-0157__occ1__seqlen73.pt'),\n",
       " ('techniques',\n",
       "  '/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels/techniques/techniques__LJ050-0196__occ1__seqlen59.pt')]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_and_melfilepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a88c7-9d65-40fe-89cd-75eb84c989e2",
   "metadata": {},
   "source": [
    "## process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c71e534d-639f-4854-a88f-7ebc2b992413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11,  2, 20, 16, 15, 20,  1,  9, 16, 24,  1,  5, 16,  1, 26, 26, 26, 16,\n",
       "        22,  1, 10,  5,  6, 15, 21, 10,  7, 10,  6, 20], dtype=torch.int32)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastpitch.common.text.text_processing import TextProcessor\n",
    "text_cleaners = ['lowercase_no_punc']\n",
    "symbol_set = \"english_pad_lowercase_nopunc\"\n",
    "tp = TextProcessor(symbol_set, text_cleaners, add_spaces=False, eos_symbol=\"$\")\n",
    "\n",
    "encoded = torch.IntTensor(tp.encode_text(\"JASON's!!! How?! do yyyou-identifies??.   \"))\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "449002ec-f41d-40c7-95d0-f6ff7ed43be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jasons how do yyyou identifies'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = [tp.id_to_symbol[id] for id in encoded.tolist()]\n",
    "\"\".join(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfee31c-654b-4277-9db2-4d78c7815668",
   "metadata": {},
   "source": [
    "## process mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "db5903a2-5306-40de-95b2-cee43d2405f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([68, 80])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word, fp = token_and_melfilepaths[0]\n",
    "wordaligned_mel = torch.load(fp)\n",
    "wordaligned_mel.size() # [seqlen, feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d7b51d-491c-4358-8550-56963edfa5d1",
   "metadata": {},
   "source": [
    "## 'class'-ified dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f2318a1b-ce64-4b23-bb28-9aab8a50b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastpitch.common.text.text_processing import TextProcessor\n",
    "\n",
    "class RespellerDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "        1) loads word + word-aligned mel spec for all words in a wordlist\n",
    "        2) converts text to sequences of one-hot vectors (corresponding to grapheme indices in fastpitch)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        wordaligned_speechreps_dir, # path to directory that contains folders of word aligned speech reps\n",
    "        wordlist, # txt file for the words to include speech reps from\n",
    "        max_examples_per_wordtype=None,\n",
    "        text_cleaners=[],\n",
    "        symbol_set=\"english_basic_lowercase_no_arpabet\",\n",
    "        add_spaces=True,\n",
    "        eos_symbol=\"$\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # load wordlist as a python list\n",
    "        if type(wordlist) == str:\n",
    "            if wordlist.endswith('.json'):\n",
    "                with open(wordlist) as f:\n",
    "                    wordlist = json.load(f)\n",
    "            else:\n",
    "                with open(wordlist) as f:\n",
    "                    wordlist = f.read().splitlines()\n",
    "        elif type(wordlist) == list:\n",
    "            pass # dont need to do anything, already in expected form\n",
    "        elif type(wordlist) == set:\n",
    "            wordlist = list(wordlist)\n",
    "        \n",
    "        wordlist = sorted(wordlist)\n",
    "        \n",
    "        # create list of all word tokens and their word aligned speech reps\n",
    "        self.word_freq = Counter()\n",
    "        self.token_and_melfilepaths = []\n",
    "        print(\"Initialising respeller dataset\")\n",
    "        for word in tqdm(wordlist):\n",
    "            # find all word aligned mels for the word\n",
    "            word_dir = os.path.join(wordaligned_speechreps_dir, word)\n",
    "            mel_files = os.listdir(word_dir)\n",
    "            if max_examples_per_wordtype:\n",
    "                mel_files = mel_files[:max_examples_per_wordtype]\n",
    "            for mel_file in mel_files:\n",
    "                mel_file_path = os.path.join(word_dir, mel_file)\n",
    "                self.token_and_melfilepaths.append((word, mel_file_path))\n",
    "                self.word_freq[word] += 1\n",
    "                \n",
    "        self.tp = TextProcessor(symbol_set, text_cleaners, add_spaces=add_spaces, eos_symbol=eos_symbol)\n",
    "\n",
    "    def get_mel(self, filename):\n",
    "        return torch.load(filename)\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        \"\"\"encode raw text into indices defined by grapheme embedding table of the TTS model\"\"\"\n",
    "        return torch.IntTensor(self.tp.encode_text(text))\n",
    "    \n",
    "    def decode_text(self, encoded):\n",
    "        # if encoded.dim() == 1:\n",
    "        #     decodings = [self.tp.id_to_symbol[id] for id in encoded.tolist()]\n",
    "        # else:\n",
    "        decodings = []\n",
    "        for batch_idx in range(encoded.size(0)):\n",
    "            decodings.append(''.join(self.tp.id_to_symbol[idx] for idx in encoded[batch_idx].tolist()))\n",
    "        return decodings\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_mel_len(melfilepath):\n",
    "        return int(melfilepath.split('seqlen')[1].split('.pt')[0])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        word, mel_filepath = self.token_and_melfilepaths[index]\n",
    "        encoded_word = self.encode_text(word)\n",
    "        mel = self.get_mel(mel_filepath)\n",
    "        \n",
    "        return {\n",
    "            'word': word, \n",
    "            'encoded_word': encoded_word, \n",
    "            'mel_filepath': mel_filepath,\n",
    "            'mel': mel,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_and_melfilepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "3553aaea-bd2f-42e4-aa1f-8e0ff95dc398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising respeller dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.16it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels', # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist=['identifies','mash','player','russias','techniques'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f0446504-217a-4c1e-acfe-553a30f87f01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identifies tensor([12, 21, 16, 17, 26, 32, 21, 18, 21, 17, 31, 39], dtype=torch.int32) torch.Size([68, 80])\n",
      "mash tensor([12, 25, 13, 31, 20, 39], dtype=torch.int32) torch.Size([28, 80])\n",
      "player tensor([12, 28, 24, 13, 37, 17, 30, 39], dtype=torch.int32) torch.Size([49, 80])\n",
      "russias tensor([12, 30, 33, 31, 31, 21, 13, 31, 39], dtype=torch.int32) torch.Size([38, 80])\n",
      "techniques tensor([12, 32, 17, 15, 20, 26, 21, 29, 33, 17, 31, 39], dtype=torch.int32) torch.Size([73, 80])\n",
      "techniques tensor([12, 32, 17, 15, 20, 26, 21, 29, 33, 17, 31, 39], dtype=torch.int32) torch.Size([59, 80])\n"
     ]
    }
   ],
   "source": [
    "batch = []\n",
    "\n",
    "for itemdict in dataset:\n",
    "    # unpack dict\n",
    "    word = itemdict['word'] \n",
    "    encoded_word = itemdict['encoded_word'] \n",
    "    mel = itemdict['mel'] \n",
    "    \n",
    "    # check\n",
    "    print(word, encoded_word, mel.size())\n",
    "    \n",
    "    batch.append(itemdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f288135-9e29-4815-84c1-5928d2200259",
   "metadata": {},
   "source": [
    "## collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d695df44-dba7-4e6e-9ab1-2e8a966d58a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate():\n",
    "    \"\"\" Zero-pads model inputs and targets based on number of frames per setep\n",
    "    \"\"\"\n",
    "    # def __init__(self):\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Collate's training batch from encoded word token and its \n",
    "        corresponding word-aligned mel spectrogram\n",
    "        \n",
    "        batch: [encoded_token, wordaligned_mel]\n",
    "        \"\"\"\n",
    "        # Right zero-pad all one-hot text sequences to max input length\n",
    "        input_lengths, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([len(x['encoded_word']) for x in batch]),\n",
    "            dim=0, descending=True)\n",
    "        max_input_len = input_lengths[0]\n",
    "\n",
    "        words = []\n",
    "        mel_filepaths = []\n",
    "        text_padded = torch.LongTensor(len(batch), max_input_len)\n",
    "        text_padded.zero_()\n",
    "        text_lengths = torch.LongTensor(len(batch))\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            words.append(batch[ids_sorted_decreasing[i]]['word'])\n",
    "            mel_filepaths.append(batch[ids_sorted_decreasing[i]]['mel_filepath'])\n",
    "            text = batch[ids_sorted_decreasing[i]]['encoded_word']\n",
    "            text_padded[i, :text.size(0)] = text\n",
    "            text_lengths[i] = text.size(0)\n",
    "\n",
    "        # Right zero-pad mel-spec\n",
    "        num_mels = batch[0]['mel'].size(1)\n",
    "        max_target_len = max([x['mel'].size(0) for x in batch])\n",
    "\n",
    "        mel_padded = torch.FloatTensor(len(batch), max_target_len, num_mels)\n",
    "        mel_padded.zero_()\n",
    "        mel_lengths = torch.LongTensor(len(batch))\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            mel = batch[ids_sorted_decreasing[i]]['mel']\n",
    "            mel_padded[i, :mel.size(0), :] = mel\n",
    "            mel_lengths[i] = mel.size(0)\n",
    "            \n",
    "\n",
    "        return {\n",
    "            'words': words,\n",
    "            'text_padded': text_padded,\n",
    "            'text_lengths': text_lengths,\n",
    "            'mel_padded': mel_padded, \n",
    "            'mel_lengths': mel_lengths,\n",
    "            'mel_filepaths': mel_filepaths\n",
    "        }\n",
    "                # input_lengths, mel_padded, output_lengths,\n",
    "                # len_x, dur_padded, dur_lens, pitch_padded, speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "cfb37e48-9158-49bc-98d7-c8809ef0cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = Collate()\n",
    "collated = collate_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8b32c93e-b4e8-4c2a-abaf-62ac8ed4a39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 12])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collated['text_padded'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8b14603f-c81a-496a-9182-187596d64811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 21, 16, 17, 26, 32, 21, 18, 21, 17, 31, 39],\n",
       "        [12, 32, 17, 15, 20, 26, 21, 29, 33, 17, 31, 39],\n",
       "        [12, 32, 17, 15, 20, 26, 21, 29, 33, 17, 31, 39],\n",
       "        [12, 30, 33, 31, 31, 21, 13, 31, 39,  0,  0,  0],\n",
       "        [12, 28, 24, 13, 37, 17, 30, 39,  0,  0,  0,  0],\n",
       "        [12, 25, 13, 31, 20, 39,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collated['text_padded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fead2c55-ee31-4298-9a3a-a016990595ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['identifies', 'techniques', 'techniques', 'russias', 'player', 'mash']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collated['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "cd7f597b-ee36-4f4c-b624-8b08e51d7e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 73, 80])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collated['mel_padded'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5c7a03-8768-4313-974c-2e2761587481",
   "metadata": {},
   "source": [
    "## put batch on gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4d6dc51a-dfa4-413b-83e7-61dd23781e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gpu(x):\n",
    "    x = x.contiguous()\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda(non_blocking=True)\n",
    "    return torch.autograd.Variable(x)\n",
    "\n",
    "def batch_to_gpu(collated_batch):\n",
    "    \"\"\"put elements that are used throughout training onto gpu\"\"\"\n",
    "    words = collated_batch['words']\n",
    "    text_padded = collated_batch['text_padded']\n",
    "    text_lengths = collated_batch['text_lengths']\n",
    "    mel_padded = collated_batch['mel_padded']\n",
    "    mel_lengths = collated_batch['mel_lengths']\n",
    "    \n",
    "    # no need to put words on gpu, its only used during eval loop\n",
    "    text_padded = to_gpu(text_padded).long()\n",
    "    text_lengths = to_gpu(text_lengths).long()\n",
    "    mel_padded = to_gpu(mel_padded).float()\n",
    "    mel_lengths = to_gpu(mel_lengths).long()\n",
    "    \n",
    "    # x: inputs\n",
    "    x = {\n",
    "        'words': words,\n",
    "        'text_padded': text_padded,\n",
    "        'text_lengths': text_lengths,\n",
    "    }\n",
    "    # y: targets\n",
    "    y = {\n",
    "        'mel_padded': mel_padded, \n",
    "        'mel_lengths': mel_lengths,\n",
    "    }\n",
    "    \n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b6176f94-30a8-493a-9aa2-449a412699cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batch_to_gpu(collated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ece1b4-0435-49ea-b931-85321887cc7d",
   "metadata": {},
   "source": [
    "# full train + dev datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "5518d342-df2d-4f2b-b6e7-a361e92b29e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising respeller dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7479/7479 [00:03<00:00, 2324.59it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir=args.wordaligned_speechreps, # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist=args.train_wordlist,\n",
    "    max_examples_per_wordtype=args.max_examples_per_wordtype,\n",
    "    add_spaces=args.add_spaces,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "751e1951-2de3-4062-8346-37410ca7bb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7479"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5e012e6e-78b5-4059-9de5-effb26d1aa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7479"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_dataset.word_freq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8b96a4dc-b55e-4bdd-bdc3-8dbbeba8b613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising respeller dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 415/415 [00:00<00:00, 2447.53it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir=args.wordaligned_speechreps, # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist=args.val_wordlist,\n",
    "    add_spaces=args.add_spaces,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9cb6f124-e16c-4ee2-bfa5-e185dcf50238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6d57753d-c2a6-4e1f-8805-bcb99e30cb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(val_dataset.word_freq.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99926204-22dc-4eea-81a7-b9746e22bf70",
   "metadata": {},
   "source": [
    "# create torch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "7942e3ba-c782-43c9-b3f4-91f2558015e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, implement distributed training?\n",
    "train_sampler = None\n",
    "shuffle = True\n",
    "num_cpus = 1 \n",
    "train_loader = DataLoader(train_dataset, num_workers=2*num_cpus, shuffle=shuffle,\n",
    "                          sampler=train_sampler, batch_size=args.batch_size,\n",
    "                          pin_memory=False, drop_last=True,\n",
    "                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "09437605-2bfb-4115-8cd0-60c0fd939156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b528ac5e-3a62-4559-b726-440b508716ed",
   "metadata": {},
   "source": [
    "# FULL train() loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3d6ad1-80e2-4e21-a5f9-9628faed98bf",
   "metadata": {},
   "source": [
    "## init dl logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8ca31455-d649-44f0-b685-5ee30ee60b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING DLLLoggerAlreadyInitialized error raised\n"
     ]
    }
   ],
   "source": [
    "def touch_file(path):\n",
    "    if not os.path.exists(path):\n",
    "        basedir = os.path.dirname(path)\n",
    "        os.makedirs(basedir, exist_ok=True)\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(\"\")\n",
    "\n",
    "# initialise logger\n",
    "tb_subsets = ['train', 'val']\n",
    "log_fpath = args.log_file or os.path.join(args.chkpt_save_dir, 'nvlog.json')\n",
    "touch_file(log_fpath)\n",
    "\n",
    "try: \n",
    "    logger.init(log_fpath, args.chkpt_save_dir, enabled=(args.local_rank == 0),\n",
    "                tb_subsets=tb_subsets)\n",
    "    logger.parameters(vars(args), tb_subset='train')\n",
    "except:\n",
    "    print(\"WARNING DLLLoggerAlreadyInitialized error raised\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b6543-f520-4bd7-aa1a-5233b2df9703",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "5660ff4c-c406-4e59-b445-7370c6a6482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(total_iter, opt, learning_rate, warmup_iters=None):\n",
    "    if warmup_iters == 0:\n",
    "        scale = 1.0\n",
    "    elif total_iter > warmup_iters:\n",
    "        scale = 1. / (total_iter ** 0.5)\n",
    "    else:\n",
    "        scale = total_iter / (warmup_iters ** 1.5)\n",
    "\n",
    "    for param_group in opt.param_groups:\n",
    "        param_group['lr'] = learning_rate * scale\n",
    "\n",
    "def log_stdout(logger, subset, epoch_iters, total_steps, loss, took):\n",
    "    logger_data = [\n",
    "        ('Loss/Total', loss),\n",
    "    ]\n",
    "    logger_data.append(('Time/Iter time', took))\n",
    "    logger.log(epoch_iters,\n",
    "               tb_total_steps=total_steps,\n",
    "               subset=subset,\n",
    "               data=OrderedDict(logger_data)\n",
    "    )\n",
    "\n",
    "def maybe_save_checkpoint(args, model, optimizer, epoch,\n",
    "                          total_iter, config):\n",
    "    if args.local_rank != 0:\n",
    "        return\n",
    "\n",
    "    intermediate = (args.epochs_per_checkpoint > 0\n",
    "                    and epoch % args.epochs_per_checkpoint == 0)\n",
    "\n",
    "    if not intermediate and epoch < args.epochs:\n",
    "        return\n",
    "\n",
    "    fpath = os.path.join(args.chkpt_save_dir, f\"respeller_checkpoint_{epoch}.pt\")\n",
    "    print(f\"Saving model and optimizer state at epoch {epoch} to {fpath}\")\n",
    "    checkpoint = {'epoch': epoch,\n",
    "                  'iteration': total_iter,\n",
    "                  'config': config,\n",
    "                  'state_dict': model.state_dict(),\n",
    "                  'optimizer': optimizer.state_dict()}\n",
    "    torch.save(checkpoint, fpath)\n",
    "\n",
    "def calc_sl_penalty(pred_lens, gt_lens):\n",
    "    '''speech length mismatch penalty similar to MCD-DTW-SL\n",
    "    encourages two sequences to be of same length\n",
    "    M and N are length of each sequence\n",
    "    coef = Max(M,N) / Min(M,N)'''\n",
    "    # stack so we can calculate max along batch dimension\n",
    "    stacked = torch.stack([pred_lens, gt_lens])\n",
    "    maxs, _ = torch.max(stacked, dim=0)\n",
    "    mins, _ = torch.min(stacked, dim=0)\n",
    "    coefs = maxs/mins\n",
    "    return coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d879d-b55f-405b-a9e1-441146ca7441",
   "metadata": {},
   "source": [
    "## pre-training loop stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "66a6ae69-609e-479b-90b0-29ee8388a97b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and optimizer state from fastpitch/exps/halved_ljspeech_data_nospaces_noeos/FastPitch_checkpoint_1000.pt\n",
      "Initialising respeller dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7479/7479 [00:01<00:00, 4121.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising respeller dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 415/415 [00:00<00:00, 4342.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished setting up models + dataloaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# def train(rank, args):\n",
    "\n",
    "\n",
    "# handle GPU\n",
    "rank = 0\n",
    "args.local_rank = rank\n",
    "device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "\n",
    "# load models\n",
    "tts, n_symbols, grapheme_embedding_dim, model_config = load_pretrained_fastpitch(args)\n",
    "respeller = EncoderRespeller(n_symbols=n_symbols, pretrained_tts=tts, d_model=args.d_model)\n",
    "# quantiser = GumbelVectorQuantizer(\n",
    "#     in_dim=args.d_model,\n",
    "#     codebook_size=n_symbols,  # number of codebook entries\n",
    "#     embedding_dim=grapheme_embedding_dim,\n",
    "#     temp=args.latent_temp,\n",
    "# )\n",
    "# init_embedding_weights(tts.encoder.word_emb.weight.unsqueeze(0), quantiser.vars)\n",
    "def mean_absolute_error(x, y):\n",
    "    \"\"\"for calculating softdtw using L1 loss\n",
    "    Calculates the Euclidean distance between each element in x and y per timestep\n",
    "    \"\"\"\n",
    "    n = x.size(1)\n",
    "    m = y.size(1)\n",
    "    d = x.size(2)\n",
    "    x = x.unsqueeze(2).expand(-1, n, m, d)\n",
    "    y = y.unsqueeze(1).expand(-1, n, m, d)\n",
    "    return torch.abs(x - y).sum(3)\n",
    "\n",
    "if args.dist_func == 'l1':\n",
    "    dist_func = mean_absolute_error\n",
    "elif args.dist_func == 'l2':\n",
    "    dist_func = None # softdtw package uses L2 as default\n",
    "    \n",
    "criterion = SoftDTW(use_cuda=True, gamma=args.softdtw_temp, bandwidth=args.softdtw_bandwidth, dist_func=dist_func) # input should be size [bsz, seqlen, dim]\n",
    "\n",
    "tts.to(device)\n",
    "respeller.to(device)\n",
    "# quantiser.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "# load optimiser and assign to it the weights to be trained\n",
    "kw = dict(lr=args.learning_rate, betas=(0.9, 0.98), eps=1e-9,\n",
    "          weight_decay=args.weight_decay)\n",
    "optimizer = Lamb(respeller.trainable_parameters(), **kw)\n",
    "\n",
    "# (optional) load checkpoint for respeller\n",
    "start_epoch = [1]\n",
    "start_iter = [0]\n",
    "assert args.checkpoint_path is None or args.resume is False, (\n",
    "    \"Specify a single checkpoint source\")\n",
    "if args.checkpoint_path is not None:\n",
    "    ch_fpath = args.checkpoint_path\n",
    "elif args.resume:\n",
    "    ch_fpath = last_checkpoint(args.chkpt_save_dir)\n",
    "else:\n",
    "    ch_fpath = None\n",
    "if ch_fpath is not None:\n",
    "    load_respeller_checkpoint(args, respeller, ch_fpath, optimizer, start_epoch, start_iter)\n",
    "    \n",
    "start_epoch = start_epoch[0]\n",
    "total_iter = start_iter[0]\n",
    "    \n",
    "# create datasets, collate func, dataloader\n",
    "train_dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels', # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist='/home/s1785140/data/ljspeech_fastpitch/respeller_train_words.json',\n",
    "    max_examples_per_wordtype=2,\n",
    "    symbol_set=\"english_basic_lowercase_no_arpabet\",\n",
    "    add_spaces=args.add_spaces,\n",
    ")\n",
    "val_dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels', # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist='/home/s1785140/data/ljspeech_fastpitch/respeller_dev_words.json',\n",
    "    symbol_set=\"english_basic_lowercase_no_arpabet\",\n",
    "    add_spaces=args.add_spaces,\n",
    ")\n",
    "num_cpus = 1 # TODO change to CLA?\n",
    "train_loader = DataLoader(train_dataset, num_workers=2*num_cpus, shuffle=True,\n",
    "                          sampler=None, batch_size=args.batch_size,\n",
    "                          pin_memory=False, drop_last=True,\n",
    "                          collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, num_workers=2*num_cpus, shuffle=False,\n",
    "                          sampler=None, batch_size=args.batch_size,\n",
    "                          pin_memory=False, collate_fn=collate_fn)\n",
    "\n",
    "# load pretrained hifigan\n",
    "\n",
    "# log spectrograms and generated audio for first few validation wordtypes\n",
    "\n",
    "# train loop\n",
    "respeller.train()\n",
    "# quantiser.train()\n",
    "tts.eval()\n",
    "\n",
    "print('Finished setting up models + dataloaders')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f34556-f71c-4907-a455-f8673375690e",
   "metadata": {},
   "source": [
    "## validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc34776-dc1b-49fc-92b5-b297bd4ad1ba",
   "metadata": {},
   "source": [
    "### plot spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c9535bd6-1dc7-4f51-b66f-745aaaa0d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_spectrogram(log_mel, figsize=(15,5), image_name=\"\"):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    img = librosa.display.specshow(log_mel, ax=ax, x_axis='frames', y_axis='linear')\n",
    "    ax.set_title(image_name)\n",
    "    fig.colorbar(img, ax=ax)\n",
    "    return fig\n",
    "\n",
    "def get_spectrograms_plots(y, fnames, step, n=4, label='Predicted spectrogram', mas=False, return_figs=False):\n",
    "    \"\"\"Plot spectrograms for n utterances in batch\"\"\"\n",
    "    bs = len(fnames)\n",
    "    n = min(n, bs)\n",
    "    s = bs // n\n",
    "    fnames = fnames[::s]\n",
    "    # print(f\"inside get_spectrograms_plots(), {fnames=}\")\n",
    "    if label == 'Predicted spectrogram':\n",
    "        # y: mel_out, dec_mask, dur_pred, log_dur_pred, pitch_pred\n",
    "        mel_specs = y[0][::s].transpose(1, 2).cpu().numpy()\n",
    "        mel_lens = y[1][::s].cpu().numpy() - 1\n",
    "    elif label == 'Reference spectrogram':\n",
    "        # y: mel_padded, mel_lens\n",
    "        mel_specs = y[0][::s].cpu().numpy()\n",
    "        # if mas:\n",
    "        mel_lens = y[1][::s].cpu().numpy()  # output_lengths\n",
    "        # else:\n",
    "            # mel_lens = y[1][::s].cpu().numpy().sum(axis=1) - 1\n",
    "            \n",
    "    image_names = []\n",
    "    spectrograms = []\n",
    "    for mel_spec, mel_len, fname in zip(mel_specs, mel_lens, fnames):\n",
    "        mel_spec = mel_spec[:, :mel_len]\n",
    "        utt_id = os.path.splitext(os.path.basename(fname))[0]\n",
    "        image_name = f'val/{label}/{utt_id}'\n",
    "        fig = log_spectrogram(mel_spec, image_name=image_name)\n",
    "        image_names.append(image_name)\n",
    "        \n",
    "        if return_figs:\n",
    "            spectrograms.append(fig)\n",
    "        else:\n",
    "            buf = BytesIO()\n",
    "            fig.savefig(buf, format='png')\n",
    "            img = Image.open(buf)\n",
    "            plt.close(fig)\n",
    "            spectrograms.append(img)\n",
    "            \n",
    "    return image_names, spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a888d3-9b58-4e3a-9364-951f5a46cdcc",
   "metadata": {},
   "source": [
    "### gen audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "4d421eb5-b66f-4d3e-9598-9473bb6a483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio(y, fnames, step, vocoder=None, sampling_rate=22050, hop_length=256,\n",
    "                   n=4, label='Predicted audio', mas=False):\n",
    "    \"\"\"Generate audio from spectrograms for n utterances in batch\"\"\"\n",
    "    bs = len(fnames)\n",
    "    n = min(n, bs)\n",
    "    s = bs // n\n",
    "    fnames = fnames[::s]\n",
    "    # print(f\"inside generate_audio(), {fnames=}\")\n",
    "    with torch.no_grad():\n",
    "        if label == 'Predicted audio':\n",
    "            # y: mel_out, dec_mask, dur_pred, log_dur_pred, pitch_pred\n",
    "            audios = vocoder(y[0][::s].transpose(1, 2)).cpu().squeeze(1).numpy() # [bsz, dim, samples ]only squeeze away dim (equals 1 for waveform)\n",
    "            # print(f\"{vocoder(y[0][::s].transpose(1, 2)).cpu().size()=}\")\n",
    "            # print(f\"{vocoder(y[0][::s].transpose(1, 2)).cpu().squeeze().size()=}\")\n",
    "            # print(f\"{y[1][::s]=}, {y[1][::s].size()=}\")\n",
    "            # print(f\"{y[1][::s].squeeze()=}, {y[1][::s].squeeze().size()=}\")\n",
    "            mel_lens = y[1][::s].cpu().numpy() - 1\n",
    "        elif label == 'Copy synthesis':\n",
    "            # y: mel_padded, dur_padded, dur_lens, pitch_padded\n",
    "            audios = vocoder(y[0][::s]).cpu().squeeze().numpy()\n",
    "            if mas:\n",
    "                mel_lens = y[2][::s].cpu().numpy()  # output_lengths\n",
    "            else:\n",
    "                mel_lens = y[1][::s].cpu().numpy().sum(axis=1) - 1\n",
    "        elif label == 'Reference audio':\n",
    "            audios = []\n",
    "            for fname in fnames:\n",
    "                wav = re.sub(r'mels/(.+)\\.pt', r'wavs/\\1.wav', fname)\n",
    "                audio, _ = librosa.load(wav, sr=sampling_rate)\n",
    "                audios.append(audio)\n",
    "            if mas:\n",
    "                mel_lens = y[2][::s].cpu().numpy()  # output_lengths\n",
    "            else:\n",
    "                mel_lens = y[1][::s].cpu().numpy().sum(axis=1) - 1\n",
    "    audios_to_return = []\n",
    "    # print(f\"DEBUG generate_audio(), {type(audios)=} {type(mel_lens)=} {type(fnames)=}\")\n",
    "    # print(f\"DEBUG generate_audio(), {audios.shape=} {mel_lens.shape=} {len(fnames)=}\")\n",
    "    for audio, mel_len, fname in zip(audios, mel_lens, fnames):\n",
    "        audio = audio[:mel_len * hop_length]\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "        utt_id = os.path.splitext(os.path.basename(fname))[0]\n",
    "        audios_to_return.append(audio)\n",
    "        \n",
    "    return audios_to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892a0404-d29e-4f82-8a56-3193aee3d8de",
   "metadata": {},
   "source": [
    "### create and log a wandb table (class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ec527837-9437-4603-8338-f15ab2149f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbTable:\n",
    "    def __init__(self):  \n",
    "        self.table = wandb.Table(columns=[\n",
    "            \"names\", \n",
    "            \"orig spelling\", \n",
    "            \"orig spelling spec\", \n",
    "            \"orig spelling audio\",\n",
    "            \"vocoded gt spec\",\n",
    "            \"vocoded gt audio\",\n",
    "            \"respelling\", \n",
    "            \"respelling spec\", \n",
    "            \"respelling audio\",\n",
    "            \"sl penalty coef\",\n",
    "            \"softdtw loss\",\n",
    "        ])\n",
    "        \n",
    "    def add_rows(\n",
    "        self,\n",
    "        names, \n",
    "        vocoded_gt_specs,\n",
    "        vocoded_gt_audios,\n",
    "        orig_words,\n",
    "        respellings,\n",
    "        orig_pred_specs, # either PIL images or matplotlib figures (but might have mem issues!)\n",
    "        orig_pred_audios,\n",
    "        pred_specs, # either PIL images or matplotlib figures (but might have mem issues!)\n",
    "        pred_audios,\n",
    "        sl_penalty_coefs,\n",
    "        losses,\n",
    "        sampling_rate=22050,\n",
    "    ):\n",
    "        for (\n",
    "            name, \n",
    "            orig_word, \n",
    "            orig_pred_spec_fig, \n",
    "            orig_pred_audio, \n",
    "            vocoded_gt_spec_fig,\n",
    "            vocoded_gt_audio, \n",
    "            respelling, \n",
    "            pred_spec_fig, \n",
    "            pred_audio,\n",
    "            sl_penalty_coef,\n",
    "            loss,\n",
    "        ) in zip(\n",
    "            names, \n",
    "            orig_words, \n",
    "            orig_pred_specs,\n",
    "            orig_pred_audios,\n",
    "            vocoded_gt_specs,\n",
    "            vocoded_gt_audios,\n",
    "            respellings, \n",
    "            pred_specs, \n",
    "            pred_audios,\n",
    "            sl_penalty_coefs,\n",
    "            losses,\n",
    "        ):\n",
    "            self.table.add_data(\n",
    "                name, \n",
    "                orig_word,\n",
    "                wandb.Image(orig_pred_spec_fig, caption=name),\n",
    "                wandb.Audio(orig_pred_audio, caption=name, sample_rate=sampling_rate),\n",
    "                wandb.Image(vocoded_gt_spec_fig, caption=name),\n",
    "                wandb.Audio(vocoded_gt_audio, caption=name, sample_rate=sampling_rate),\n",
    "                respelling,\n",
    "                wandb.Image(pred_spec_fig, caption=name),\n",
    "                wandb.Audio(pred_audio, caption=name, sample_rate=sampling_rate),\n",
    "                sl_penalty_coef,\n",
    "                loss,\n",
    "            )\n",
    "            \n",
    "            # close figures to save memory\n",
    "            if type(orig_pred_spec_fig) == matplotlib.figure.Figure:\n",
    "                plt.close(orig_pred_spec_fig)\n",
    "            if type(vocoded_gt_spec_fig) == matplotlib.figure.Figure:\n",
    "                plt.close(vocoded_gt_spec_fig)\n",
    "            if type(pred_spec_fig) == matplotlib.figure.Figure:\n",
    "                plt.close(pred_spec_fig)\n",
    "            \n",
    "    def log(self, train):\n",
    "        if train:\n",
    "            wandb.log({\"train_table\": self.table})\n",
    "        else:\n",
    "            wandb.log({\"val_table\": self.table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a906912a-6ac3-4983-b881-c02ef91d8bbc",
   "metadata": {},
   "source": [
    "### validate() fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "95626bd7-17ae-46dd-ab85-5d4adb6f9c5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_indices(indices):\n",
    "    \"\"\"decode batch of indices to text\n",
    "    [bsz, seqlen]\"\"\"\n",
    "    decodings = []\n",
    "    for batch_idx in range(indices.size(0)):\n",
    "        decodings.append(''.join(tp.id_to_symbol[id] for id in indices[batch_idx].tolist()))\n",
    "    return decodings\n",
    "\n",
    "def select(x, bsz, n):\n",
    "    \"\"\"select items in batch that will be visualised/converted to audio\"\"\"\n",
    "    n = min(n, bsz)\n",
    "    s = bsz // n\n",
    "    return x[::s]\n",
    "\n",
    "def validate(\n",
    "    respeller_model, \n",
    "    tts_model, \n",
    "    vocoder,\n",
    "    criterion,\n",
    "    valset, \n",
    "    epoch, \n",
    "    batch_size, \n",
    "    num_to_gen,\n",
    "    collate_fn, \n",
    "    sampling_rate,\n",
    "    hop_length,\n",
    "    audio_interval=5,\n",
    "    only_log_table=False,\n",
    "    train=False,\n",
    "):\n",
    "    \"\"\"Handles all the validation scoring and printing\n",
    "    GT (beginning of training):\n",
    "    - log GT mel spec and vocoded audio for several validation set words\n",
    "    \n",
    "    Model outputs:\n",
    "    - log predicted mel spec and vocoded audio from fastpitch\n",
    "    - log respelled word from respeller\n",
    "    \"\"\"\n",
    "    was_training = respeller_model.training\n",
    "    respeller_model.eval()\n",
    "    wandb_table = WandbTable()\n",
    "    \n",
    "    tik = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        val_loader = DataLoader(valset, num_workers=4, shuffle=False,\n",
    "                                sampler=None,\n",
    "                                batch_size=batch_size, pin_memory=False,\n",
    "                                collate_fn=collate_fn)\n",
    "        val_meta = defaultdict(float)\n",
    "        val_losses = 0.0\n",
    "        val_losses_with_sl_penalty = 0.0\n",
    "        epoch_iter = 0\n",
    "        sl_penalty_coefs = []\n",
    "        \n",
    "        num_generated = 0\n",
    "        \n",
    "        for i, batch in enumerate(val_loader):\n",
    "            epoch_iter += 1\n",
    "            \n",
    "            # get loss over batch\n",
    "            x, y = batch_to_gpu(batch)\n",
    "            pred_mel, dec_lens, g_embedding_indices = forward_pass(respeller_model, tts_model, x)\n",
    "            iter_loss = (criterion(pred_mel, y[\"mel_padded\"]))\n",
    "            val_losses += iter_loss.mean().item()\n",
    "            \n",
    "            coef = calc_sl_penalty(dec_lens, y['mel_lengths'])\n",
    "            sl_penalty_coefs.append(coef.mean().item())\n",
    "            val_losses_with_sl_penalty += (coef * iter_loss).mean().item()\n",
    "    \n",
    "            # log spectrograms and generated audio for first few utterances\n",
    "            log_table = (epoch % audio_interval == 0 if epoch is not None else True)\n",
    "            \n",
    "            should_generate = num_generated < args.val_num_to_gen\n",
    "            if log_table and should_generate:\n",
    "                fnames = batch['mel_filepaths']\n",
    "                bsz = len(fnames)\n",
    "                \n",
    "                num_to_generate_this_batch = min(bsz, args.val_num_to_gen - num_generated)\n",
    "                \n",
    "                # print(f\"DEBUG validate(), {bsz=}, {num_to_generate_this_batch=}\")\n",
    "                \n",
    "                # get original word and respellings for logging\n",
    "                original_words = valset.decode_text(x['text_padded'])\n",
    "                respellings = valset.decode_text(g_embedding_indices)\n",
    "                \n",
    "                \n",
    "                # vocode original recorded speech\n",
    "                gt_mel = y['mel_padded']\n",
    "                gt_mel_lens = y['mel_lengths']\n",
    "                vocoded_gt = generate_audio((gt_mel, gt_mel_lens), fnames, total_iter, vocoder, \n",
    "                                            sampling_rate, hop_length, n=num_to_generate_this_batch, \n",
    "                                            label='Predicted audio', mas=True)\n",
    "                _orig_token_names, gt_specs = get_spectrograms_plots(\n",
    "                    (gt_mel.transpose(1,2), gt_mel_lens), fnames, total_iter, \n",
    "                    n=num_to_generate_this_batch, label='Reference spectrogram', mas=False)\n",
    "                \n",
    "                # get melspec + generated audio for original spellings\n",
    "                orig_pred_mel, orig_dec_lens, _dur_pred, _pitch_pred = tts(\n",
    "                    inputs=x['text_padded'],\n",
    "                    skip_embeddings=False,\n",
    "                )\n",
    "                orig_pred_mel = orig_pred_mel.transpose(1,2)\n",
    "                _orig_token_names, orig_pred_specs = get_spectrograms_plots(\n",
    "                    (orig_pred_mel, orig_dec_lens), fnames, total_iter, \n",
    "                    n=num_to_generate_this_batch, label='Predicted spectrogram', mas=True)\n",
    "                orig_pred_audios = generate_audio((orig_pred_mel, orig_dec_lens), fnames, \n",
    "                                                  total_iter, vocoder, sampling_rate, hop_length, \n",
    "                                                  n=num_to_generate_this_batch, label='Predicted audio', mas=True)\n",
    "            \n",
    "                # get melspec + generated audio for respellings\n",
    "                token_names, pred_specs = get_spectrograms_plots(\n",
    "                    (pred_mel, dec_lens), fnames, total_iter, \n",
    "                    n=num_to_generate_this_batch, label='Predicted spectrogram', mas=True)\n",
    "                pred_audios = generate_audio(\n",
    "                    (pred_mel, dec_lens), fnames, total_iter, vocoder, sampling_rate, hop_length, \n",
    "                    n=num_to_generate_this_batch, label='Predicted audio', mas=True)\n",
    "                \n",
    "                # log everything to wandb table\n",
    "                token_names = [tok_name.split('/')[-1] for tok_name in token_names]\n",
    "                \n",
    "                wandb_table.add_rows(\n",
    "                    names=token_names,\n",
    "                    vocoded_gt_specs=gt_specs,\n",
    "                    vocoded_gt_audios=vocoded_gt,\n",
    "                    orig_words=select(original_words, bsz, n=num_to_generate_this_batch),\n",
    "                    orig_pred_specs=orig_pred_specs,\n",
    "                    orig_pred_audios=orig_pred_audios,\n",
    "                    respellings=select(respellings, bsz, n=num_to_generate_this_batch),\n",
    "                    pred_specs=pred_specs,\n",
    "                    pred_audios=pred_audios,\n",
    "                    sl_penalty_coefs=coef,\n",
    "                    losses=iter_loss,\n",
    "                    sampling_rate=sampling_rate,\n",
    "                )\n",
    "                \n",
    "                num_generated += num_to_generate_this_batch\n",
    "                \n",
    "            if train or log_table and only_log_table:\n",
    "                break # leave for loop after first iteration\n",
    "        \n",
    "        if not only_log_table:\n",
    "            val_logs = {}\n",
    "            val_logs['val/epoch_loss'] = val_losses/epoch_iter\n",
    "            if val_losses_with_sl_penalty != 0.0:\n",
    "                val_logs['val/epoch_loss_with_sl_penalty'] = val_losses_with_sl_penalty/epoch_iter\n",
    "                val_logs['val/epoch_sl_penalty_coef'] = sum(sl_penalty_coefs) / len(sl_penalty_coefs)\n",
    "            wandb.log(val_logs)\n",
    "    \n",
    "    wandb_table.log(train=train)\n",
    "    \n",
    "    if was_training:\n",
    "        respeller_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84afad70-963f-4ffe-91f2-39101bdd24e2",
   "metadata": {},
   "source": [
    "## forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904457f-89c4-4c6b-b0a9-27321011c49c",
   "metadata": {},
   "source": [
    "### dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e4405c06-19f9-4e89-b025-529c2f605c4c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test forward_pass\n",
    "\n",
    "# get one batch\n",
    "batch = next(iter(train_loader))\n",
    "x, y = batch_to_gpu(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "377c1196-6baf-4355-9cf2-cb3c4509ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get src key padding mask\n",
    "def get_src_key_padding_mask(bsz, max_len, lens, device):\n",
    "    \"\"\"return a Boolean mask for a list or tensor of sequence lengths\n",
    "    True for values in tensor greater than sequence length\n",
    "\n",
    "    bsz (int)\n",
    "    max_len (int): max seq len of item in batch\n",
    "    lens [bsz]: list or tensor of lengths\n",
    "    \"\"\"\n",
    "    if type(lens) == list:\n",
    "        lens = torch.tensor(lens, device=device)\n",
    "    assert lens.dim() == 1\n",
    "\n",
    "    lens = lens.unsqueeze(1)  # [bsz] -> [bsz, seq_len]\n",
    "    m = torch.arange(max_len, device=device)\n",
    "    m = m.expand(bsz, max_len)  # repeat along batch dimension\n",
    "    m = (m < lens)\n",
    "    return ~m  # tilde inverts a bool tensor\n",
    "\n",
    "text_lens = x['text_lengths']\n",
    "max_len = max(text_lens).item()\n",
    "bsz = len(text_lens)\n",
    "\n",
    "mask = get_src_key_padding_mask(bsz, max_len, text_lens, x['text_padded'].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "242976c4-156e-44ff-bb9f-779da5f32dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_embeddings, g_embedding_indices = respeller(x['text_padded'], mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "5fb1fdd3-de3a-431f-9141-d120aa2b43ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally pad outputs according to the text_len/padding in the input original spellings\n",
    "# NB do this in the forward of the respeller model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "9aa1d0fc-cb5e-4481-a1d2-efa45222fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_mel, dec_lens, _dur_pred, _pitch_pred = tts(\n",
    "    inputs=g_embeddings,\n",
    "    ids=g_embedding_indices,\n",
    "    skip_embeddings=True,\n",
    ")\n",
    "\n",
    "# log_mel: [bsz, dim, seqlen]\n",
    "log_mel = log_mel.transpose(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bbf9e1-2ead-4705-afcc-8306248e5add",
   "metadata": {},
   "source": [
    "### func definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "65264fe0-73c2-4bf6-ac8e-d9e5cb162d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(respeller, tts, x):\n",
    "    \"\"\"x: inputs\n",
    "    x = {\n",
    "        'words': words,\n",
    "        'text_padded': text_padded,\n",
    "        'text_lengths': text_lengths,\n",
    "    }\"\"\"\n",
    "    text_lens = x['text_lengths']\n",
    "    max_len = max(text_lens).item()\n",
    "    bsz = len(text_lens)\n",
    "\n",
    "    mask = get_src_key_padding_mask(bsz, max_len, text_lens, x['text_padded'].device)\n",
    "    \n",
    "    g_embeddings, g_embedding_indices = respeller(x['text_padded'], mask)\n",
    "    \n",
    "    # quantiser_outdict = quantiser(logits, produce_targets=True)\n",
    "    # g_embedding_indices = quantiser_outdict[\"targets\"].squeeze(2)\n",
    "    # g_embeddings = quantiser_outdict[\"x\"]\n",
    "    \n",
    "    # use text lens to zero out the output of the respeller so that repelling matches the length of the original spelling\n",
    "    for i, text_len in enumerate(text_lens):\n",
    "        # ###OLD\n",
    "        # #indices\n",
    "        # indices = g_embedding_indices[i, :text_len]\n",
    "        # masked_g_embedding_indices[i, :text_len] = indices\n",
    "        # #embeddings\n",
    "        # embeddings = g_embeddings[i, :text_len, :]\n",
    "        # masked_g_embeddings[i, :text_len, :] = embeddings\n",
    "        ###NEW\n",
    "        g_embedding_indices[i, text_len:] = 0.0\n",
    "        g_embeddings[i, text_len:, :] = 0.0\n",
    "    \n",
    "\n",
    "    log_mel, dec_lens, _dur_pred, _pitch_pred = tts(\n",
    "        inputs=g_embeddings,\n",
    "        ids=g_embedding_indices,\n",
    "        skip_embeddings=True,\n",
    "    )\n",
    "    \n",
    "    # log_mel: [bsz, dim, seqlen]\n",
    "    log_mel = log_mel.transpose(1,2)\n",
    "    # log_mel: [bsz, seqlen, dim]\n",
    "    \n",
    "    # return mask for masking acoustic loss\n",
    "    # padding_idx = 0\n",
    "    # mask = (g_embedding_indices != padding_idx).unsqueeze(2)\n",
    "    # mask.size()\n",
    "    # dec_mask = mask_from_lens(dec_lens).unsqueeze(2)\n",
    "    \n",
    "    return log_mel, dec_lens, g_embedding_indices\n",
    "\n",
    "def byte_to_gigabyte(bytes):\n",
    "    return bytes/1000000000\n",
    "\n",
    "def run_val(epoch):\n",
    "    \"\"\"wrap in fn so that we can call at:\n",
    "    1. before training model\n",
    "    2. at end of every X epochs\"\"\"\n",
    "    # log audio and respellings for training set words\n",
    "    validate(\n",
    "        respeller_model=respeller, \n",
    "        tts_model=tts, \n",
    "        vocoder=vocoder,\n",
    "        criterion=criterion,\n",
    "        valset=train_dataset, \n",
    "        batch_size=args.batch_size,\n",
    "        num_to_gen=args.val_num_to_gen,\n",
    "        collate_fn=collate_fn,\n",
    "        epoch=epoch,\n",
    "        sampling_rate=args.sampling_rate,\n",
    "        hop_length=args.hop_length,\n",
    "        audio_interval=args.val_log_interval,\n",
    "        only_log_table=True,\n",
    "        train=True,\n",
    "    )\n",
    "        \n",
    "    # log audio and respellings for val set words\n",
    "    validate(\n",
    "        respeller_model=respeller, \n",
    "        tts_model=tts, \n",
    "        vocoder=vocoder,\n",
    "        criterion=criterion,\n",
    "        valset=val_dataset, \n",
    "        batch_size=args.batch_size,\n",
    "        num_to_gen=args.val_num_to_gen,\n",
    "        collate_fn=collate_fn,\n",
    "        epoch=epoch,\n",
    "        sampling_rate=args.sampling_rate,\n",
    "        hop_length=args.hop_length,\n",
    "        audio_interval=args.val_log_interval,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4542e44-5ade-412d-b35c-ae23beec5fdd",
   "metadata": {},
   "source": [
    "## before train loop validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d24df-dc7a-487b-88cc-1b5688d48300",
   "metadata": {},
   "source": [
    "DEBUG validate(), bsz=1, num_to_generate_this_batch=1\n",
    "y[1][::s]=tensor([43], device='cuda:0'), y[1][::s].size()=torch.Size([1])\n",
    "y[1][::s].squeeze()=tensor(43, device='cuda:0'), y[1][::s].squeeze().size()=torch.Size([])\n",
    "DEBUG generate_audio(), type(audios)=<class 'numpy.ndarray'> type(mel_lens)=<class 'numpy.int64'> type(fnames)=<class 'list'>\n",
    "DEBUG generate_audio(), audios.shape=(11008,) mel_lens.shape=() len(fnames)=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37fa50f-f74c-45df-aea6-9c03a673a166",
   "metadata": {},
   "source": [
    "DEBUG validate(), bsz=2, num_to_generate_this_batch=2\n",
    "vocoder(y[0][::s].transpose(1, 2)).cpu().size()=torch.Size([2, 1, 11264])\n",
    "vocoder(y[0][::s].transpose(1, 2)).cpu().squeeze().size()=torch.Size([2, 11264])\n",
    "y[1][::s]=tensor([43, 44], device='cuda:0'), y[1][::s].size()=torch.Size([2])\n",
    "y[1][::s].squeeze()=tensor([43, 44], device='cuda:0'), y[1][::s].squeeze().size()=torch.Size([2])\n",
    "DEBUG generate_audio(), type(audios)=<class 'numpy.ndarray'> type(mel_lens)=<class 'numpy.ndarray'> type(fnames)=<class 'list'>\n",
    "DEBUG generate_audio(), audios.shape=(2, 11264) mel_lens.shape=(2,) len(fnames)=2\n",
    "vocoder(y[0][::s].transpose(1, 2)).cpu().size()=torch.Size([2, 1, 17152])\n",
    "vocoder(y[0][::s].transpose(1, 2)).cpu().squeeze().size()=torch.Size([2, 17152])\n",
    "y[1][::s]=tensor([67, 67], device='cuda:0'), y[1][::s].size()=torch.Size([2])\n",
    "y[1][::s].squeeze()=tensor([67, 67], device='cuda:0'), y[1][::s].squeeze().size()=torch.Size([2])\n",
    "DEBUG generate_audio(), type(audios)=<class 'numpy.ndarray'> type(mel_lens)=<class 'numpy.ndarray'> type(fnames)=<class 'list'>\n",
    "DEBUG generate_audio(), audios.shape=(2, 17152) mel_lens.shape=(2,) len(fnames)=2\n",
    "vocoder(y[0][::s].transpose(1, 2)).cpu().size()=torch.Size([2, 1, 14592])\n",
    "vocoder(y[0][::s].transpose(1, 2)).cpu().squeeze().size()=torch.Size([2, 14592])\n",
    "y[1][::s]=tensor([57, 57], device='cuda:0'), y[1][::s].size()=torch.Size([2])\n",
    "y[1][::s].squeeze()=tensor([57, 57], device='cuda:0'), y[1][::s].squeeze().size()=torch.Size([2])\n",
    "DEBUG generate_audio(), type(audios)=<class 'numpy.ndarray'> type(mel_lens)=<class 'numpy.ndarray'> type(fnames)=<class 'list'>\n",
    "DEBUG generate_audio(), audios.shape=(2, 14592) mel_lens.shape=(2,) len(fnames)=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "1f68a8c3-cf6b-468b-ba46-d6a863c56303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s1785140/miniconda3/envs/fastpitch/lib/python3.8/site-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 3 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "if not args.skip_before_train_loop_validation:\n",
    "    run_val(epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e52b96f-6409-4418-b1de-72deec78526e",
   "metadata": {},
   "source": [
    "## train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1b1c4b39-189d-4af5-a5c0-7489fbc95833",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG mode iter 0 of 5\n",
      "DEBUG mode iter 1 of 5\n",
      "DEBUG mode iter 2 of 5\n",
      "DEBUG mode iter 3 of 5\n",
      "DEBUG mode iter 4 of 5\n",
      "DEBUG mode iter 5 of 5\n",
      "quit training loop, FOR DEVELOPMENT!!!\n",
      "DEBUG mode iter 0 of 5\n",
      "DEBUG mode iter 1 of 5\n",
      "DEBUG mode iter 2 of 5\n",
      "DEBUG mode iter 3 of 5\n",
      "DEBUG mode iter 4 of 5\n",
      "DEBUG mode iter 5 of 5\n",
      "quit training loop, FOR DEVELOPMENT!!!\n",
      "Saving model and optimizer state at epoch 2 to /home/s1785140/respeller/exps/test_development/respeller_checkpoint_2.pt\n",
      "\n",
      " *** Finished training! ***\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, args.epochs + 1):\n",
    "    # logging metrics\n",
    "    epoch_start_time = time.perf_counter()\n",
    "    iter_loss = 0\n",
    "    epoch_loss = 0.0\n",
    "    epoch_iter = 0\n",
    "    num_iters = len(train_loader)\n",
    "    mean_sl_penalty_coefs = []\n",
    "    # epoch_mel_loss = 0.0\n",
    "    # epoch_num_frames = 0\n",
    "    # epoch_frames_per_sec = 0.0\n",
    "    # iter_num_frames = 0\n",
    "    # iter_meta = {}\n",
    "\n",
    "    # iterate over all batches in epoch\n",
    "    for batch in train_loader:\n",
    "        if args.max_iters_per_epoch:\n",
    "            if epoch_iter > args.max_iters_per_epoch:\n",
    "                print(\"quit training loop, FOR DEVELOPMENT!!!\")\n",
    "                break \n",
    "            print(f'DEBUG mode iter {epoch_iter} of {args.max_iters_per_epoch}')\n",
    "        \n",
    "        if epoch_iter == num_iters: # useful for gradient accumulation\n",
    "            break\n",
    "                    \n",
    "        total_iter += 1\n",
    "        epoch_iter += 1\n",
    "        iter_start_time = time.perf_counter()\n",
    "\n",
    "        adjust_learning_rate(total_iter, optimizer, args.learning_rate,\n",
    "                             args.warmup_steps)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x, y = batch_to_gpu(batch) # x: inputs, y: targets\n",
    "        gt_mel = y[\"mel_padded\"]\n",
    "        \n",
    "        # # y: targets\n",
    "        # y = {\n",
    "        #     'mel_padded': mel_padded, \n",
    "        #     'mel_lengths': mel_lengths,\n",
    "        # }\n",
    "        \n",
    "        # forward pass through models (respeller -> quantiser -> tts)\n",
    "        pred_mel, dec_lens, _g_embedding_indices = forward_pass(respeller, tts, x)\n",
    "        \n",
    "        # TODO: DO WE NEED MASK IF WE USE SOFTDTW LOSS? \n",
    "        # I THINK IT AUTOMATICALLY WILL ALIGN PADDED FRAMES WITH EACH OTHER???\n",
    "        \n",
    "        # print(f'inputs to loss {pred_mel.size()}, {gt_mel.size()}')\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(pred_mel, gt_mel)\n",
    "        # print('raw loss from softdtw', loss.size())\n",
    "        \n",
    "        if args.avg_loss_by_speech_lens:\n",
    "            loss = loss / dec_lens # needed because softdtw code doesn't return avg loss by default TODO check this!\n",
    "            # TODO also add gt lens? maybe shud normalise according to path len?\n",
    "            # print('loss avg according to dec seqlens', loss.size())\n",
    "        \n",
    "        # penalise length mismatch            \n",
    "        coef = calc_sl_penalty(dec_lens, y['mel_lengths'])\n",
    "        if args.speech_length_penalty_training:\n",
    "            loss_no_sl_penalty = loss.clone().detach()\n",
    "            loss = coef * loss\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        # print('loss avged across batch', loss.size())\n",
    "        \n",
    "        # backpropagation of loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # clip gradients and run optimizer\n",
    "        torch.nn.utils.clip_grad_norm_(respeller.trainable_parameters(), args.grad_clip_thresh)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # log metrics to terminal and to wandb\n",
    "        iter_loss = loss.item()\n",
    "        iter_time = time.perf_counter() - iter_start_time\n",
    "        epoch_loss += iter_loss\n",
    "        \n",
    "        # values to be logged by WANDB\n",
    "        iter_logs = {}\n",
    "        \n",
    "        iter_logs[\"train/iter_loss\"] = iter_loss\n",
    "        iter_logs[\"train/iter_time\"] = iter_time\n",
    "        \n",
    "        mean_sl_penalty_coef = coef.mean().item()\n",
    "        mean_sl_penalty_coefs.append(mean_sl_penalty_coef)\n",
    "        iter_logs[\"train/iter_sl_penalty_coef\"] = mean_sl_penalty_coef\n",
    "        \n",
    "        if args.speech_length_penalty_training:\n",
    "            iter_logs[\"train/iter_loss_no_sl_penalty\"] = loss_no_sl_penalty.mean().item()\n",
    "        \n",
    "        if True:\n",
    "            # log memory usage\n",
    "            t = torch.cuda.get_device_properties(0).total_memory\n",
    "            r = torch.cuda.memory_reserved(0)\n",
    "            a = torch.cuda.memory_allocated(0)\n",
    "            f = r-a  # free inside reserved\n",
    "            iter_logs['memory/total'] = byte_to_gigabyte(t)\n",
    "            iter_logs['memory/reserved'] = byte_to_gigabyte(r)\n",
    "            iter_logs['memory/allocated'] = byte_to_gigabyte(a)\n",
    "            iter_logs['memory/free'] = byte_to_gigabyte(f)\n",
    "        \n",
    "        wandb.log(iter_logs)\n",
    "        \n",
    "\n",
    "        ### Finished Epoch!\n",
    "             \n",
    "    epoch_time = time.perf_counter() - epoch_start_time\n",
    "    \n",
    "    epoch_logs = {\n",
    "        \"train/epoch_num\": epoch,\n",
    "        \"train/epoch_time\": epoch_time,\n",
    "        \"train/epoch_loss\": epoch_loss / epoch_iter,\n",
    "    }\n",
    "    if args.speech_length_penalty_training:\n",
    "        epoch_logs[\"train/epoch_sl_penalty_coef\"] = sum(mean_sl_penalty_coefs) / len(mean_sl_penalty_coefs)\n",
    "    \n",
    "    wandb.log(epoch_logs)\n",
    "    \n",
    "    run_val(epoch=epoch)\n",
    "\n",
    "    maybe_save_checkpoint(args, respeller, optimizer, \n",
    "                          epoch, total_iter, model_config)\n",
    "\n",
    "    logger.flush()\n",
    "        \n",
    "print(\"\\n *** Finished training! ***\")\n",
    "\n",
    "# wandb.finish() # useful in jupyter notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820c629e-f8e9-4265-bed3-165f2593b5aa",
   "metadata": {},
   "source": [
    "# DEBUG SOFTDTW len normalisation issue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "1dfd7f04-7b65-46ca-bf03-375c396f976c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_len=1, Y_len=1, sum(softdtw_loss).item()=32.0, normed_loss.item()=16.0\n",
      "X_len=1, Y_len=2, sum(softdtw_loss).item()=64.0, normed_loss.item()=21.33333396911621\n",
      "X_len=1, Y_len=3, sum(softdtw_loss).item()=96.0, normed_loss.item()=24.0\n",
      "X_len=1, Y_len=4, sum(softdtw_loss).item()=128.0, normed_loss.item()=25.600000381469727\n",
      "X_len=1, Y_len=5, sum(softdtw_loss).item()=160.0, normed_loss.item()=26.666667938232422\n",
      "X_len=1, Y_len=6, sum(softdtw_loss).item()=192.0, normed_loss.item()=27.428573608398438\n",
      "X_len=1, Y_len=7, sum(softdtw_loss).item()=224.0, normed_loss.item()=28.0\n",
      "X_len=1, Y_len=8, sum(softdtw_loss).item()=256.0, normed_loss.item()=28.44444465637207\n",
      "X_len=1, Y_len=9, sum(softdtw_loss).item()=288.0, normed_loss.item()=28.80000114440918\n",
      "X_len=2, Y_len=1, sum(softdtw_loss).item()=64.0, normed_loss.item()=21.33333396911621\n",
      "X_len=2, Y_len=2, sum(softdtw_loss).item()=63.999725341796875, normed_loss.item()=15.999931335449219\n",
      "X_len=2, Y_len=3, sum(softdtw_loss).item()=93.78173065185547, normed_loss.item()=18.756345748901367\n",
      "X_len=2, Y_len=4, sum(softdtw_loss).item()=124.48419189453125, normed_loss.item()=20.747365951538086\n",
      "X_len=2, Y_len=5, sum(softdtw_loss).item()=155.56365966796875, normed_loss.item()=22.22338104248047\n",
      "X_len=2, Y_len=6, sum(softdtw_loss).item()=186.84962463378906, normed_loss.item()=23.356203079223633\n",
      "X_len=2, Y_len=7, sum(softdtw_loss).item()=218.26614379882812, normed_loss.item()=24.251794815063477\n",
      "X_len=2, Y_len=8, sum(softdtw_loss).item()=249.77293395996094, normed_loss.item()=24.977293014526367\n",
      "X_len=2, Y_len=9, sum(softdtw_loss).item()=281.3456726074219, normed_loss.item()=25.576879501342773\n",
      "X_len=3, Y_len=1, sum(softdtw_loss).item()=96.0, normed_loss.item()=24.0\n",
      "X_len=3, Y_len=2, sum(softdtw_loss).item()=93.78173065185547, normed_loss.item()=18.756345748901367\n",
      "X_len=3, Y_len=3, sum(softdtw_loss).item()=95.99909973144531, normed_loss.item()=15.999850273132324\n",
      "X_len=3, Y_len=4, sum(softdtw_loss).item()=124.48387908935547, normed_loss.item()=17.78341293334961\n",
      "X_len=3, Y_len=5, sum(softdtw_loss).item()=154.2659149169922, normed_loss.item()=19.283239364624023\n",
      "X_len=3, Y_len=6, sum(softdtw_loss).item()=184.63133239746094, normed_loss.item()=20.51459312438965\n",
      "X_len=3, Y_len=7, sum(softdtw_loss).item()=215.3338623046875, normed_loss.item()=21.53338623046875\n",
      "X_len=3, Y_len=8, sum(softdtw_loss).item()=246.2572021484375, normed_loss.item()=22.38701820373535\n",
      "X_len=3, Y_len=9, sum(softdtw_loss).item()=277.3365478515625, normed_loss.item()=23.111379623413086\n",
      "X_len=4, Y_len=1, sum(softdtw_loss).item()=128.0, normed_loss.item()=25.600000381469727\n",
      "X_len=4, Y_len=2, sum(softdtw_loss).item()=124.48419189453125, normed_loss.item()=20.747365951538086\n",
      "X_len=4, Y_len=3, sum(softdtw_loss).item()=124.48387908935547, normed_loss.item()=17.78341293334961\n",
      "X_len=4, Y_len=4, sum(softdtw_loss).item()=127.99828338623047, normed_loss.item()=15.999785423278809\n",
      "X_len=4, Y_len=5, sum(softdtw_loss).item()=155.5627899169922, normed_loss.item()=17.284753799438477\n",
      "X_len=4, Y_len=6, sum(softdtw_loss).item()=184.630859375, normed_loss.item()=18.46308708190918\n",
      "X_len=4, Y_len=7, sum(softdtw_loss).item()=214.41294860839844, normed_loss.item()=19.49208641052246\n",
      "X_len=4, Y_len=8, sum(softdtw_loss).item()=244.62210083007812, normed_loss.item()=20.385175704956055\n",
      "X_len=4, Y_len=9, sum(softdtw_loss).item()=275.1181945800781, normed_loss.item()=21.162939071655273\n",
      "X_len=5, Y_len=1, sum(softdtw_loss).item()=160.0, normed_loss.item()=26.666667938232422\n",
      "X_len=5, Y_len=2, sum(softdtw_loss).item()=155.56365966796875, normed_loss.item()=22.22338104248047\n",
      "X_len=5, Y_len=3, sum(softdtw_loss).item()=154.2659149169922, normed_loss.item()=19.283239364624023\n",
      "X_len=5, Y_len=4, sum(softdtw_loss).item()=155.5627899169922, normed_loss.item()=17.284753799438477\n",
      "X_len=5, Y_len=5, sum(softdtw_loss).item()=159.99708557128906, normed_loss.item()=15.999709129333496\n",
      "X_len=5, Y_len=6, sum(softdtw_loss).item()=186.8480987548828, normed_loss.item()=16.986190795898438\n",
      "X_len=5, Y_len=7, sum(softdtw_loss).item()=215.3329620361328, normed_loss.item()=17.944414138793945\n",
      "X_len=5, Y_len=8, sum(softdtw_loss).item()=244.62167358398438, normed_loss.item()=18.817052841186523\n",
      "X_len=5, Y_len=9, sum(softdtw_loss).item()=274.40374755859375, normed_loss.item()=19.600269317626953\n",
      "X_len=6, Y_len=1, sum(softdtw_loss).item()=192.0, normed_loss.item()=27.428573608398438\n",
      "X_len=6, Y_len=2, sum(softdtw_loss).item()=186.84962463378906, normed_loss.item()=23.356203079223633\n",
      "X_len=6, Y_len=3, sum(softdtw_loss).item()=184.63133239746094, normed_loss.item()=20.51459312438965\n",
      "X_len=6, Y_len=4, sum(softdtw_loss).item()=184.630859375, normed_loss.item()=18.46308708190918\n",
      "X_len=6, Y_len=5, sum(softdtw_loss).item()=186.8480987548828, normed_loss.item()=16.986190795898438\n",
      "X_len=6, Y_len=6, sum(softdtw_loss).item()=191.99562072753906, normed_loss.item()=15.999635696411133\n",
      "X_len=6, Y_len=7, sum(softdtw_loss).item()=218.26377868652344, normed_loss.item()=16.789522171020508\n",
      "X_len=6, Y_len=8, sum(softdtw_loss).item()=246.2554931640625, normed_loss.item()=17.589679718017578\n",
      "X_len=6, Y_len=9, sum(softdtw_loss).item()=275.1172180175781, normed_loss.item()=18.341148376464844\n",
      "X_len=7, Y_len=1, sum(softdtw_loss).item()=224.0, normed_loss.item()=28.0\n",
      "X_len=7, Y_len=2, sum(softdtw_loss).item()=218.26614379882812, normed_loss.item()=24.251794815063477\n",
      "X_len=7, Y_len=3, sum(softdtw_loss).item()=215.3338623046875, normed_loss.item()=21.53338623046875\n",
      "X_len=7, Y_len=4, sum(softdtw_loss).item()=214.41294860839844, normed_loss.item()=19.49208641052246\n",
      "X_len=7, Y_len=5, sum(softdtw_loss).item()=215.3329620361328, normed_loss.item()=17.944414138793945\n",
      "X_len=7, Y_len=6, sum(softdtw_loss).item()=218.26377868652344, normed_loss.item()=16.789522171020508\n",
      "X_len=7, Y_len=7, sum(softdtw_loss).item()=223.99400329589844, normed_loss.item()=15.99957275390625\n",
      "X_len=7, Y_len=8, sum(softdtw_loss).item()=249.76956176757812, normed_loss.item()=16.651304244995117\n",
      "X_len=7, Y_len=9, sum(softdtw_loss).item()=277.3343505859375, normed_loss.item()=17.333396911621094\n",
      "X_len=8, Y_len=1, sum(softdtw_loss).item()=256.0, normed_loss.item()=28.44444465637207\n",
      "X_len=8, Y_len=2, sum(softdtw_loss).item()=249.77293395996094, normed_loss.item()=24.977293014526367\n",
      "X_len=8, Y_len=3, sum(softdtw_loss).item()=246.2572021484375, normed_loss.item()=22.38701820373535\n",
      "X_len=8, Y_len=4, sum(softdtw_loss).item()=244.62210083007812, normed_loss.item()=20.385175704956055\n",
      "X_len=8, Y_len=5, sum(softdtw_loss).item()=244.62167358398438, normed_loss.item()=18.817052841186523\n",
      "X_len=8, Y_len=6, sum(softdtw_loss).item()=246.2554931640625, normed_loss.item()=17.589679718017578\n",
      "X_len=8, Y_len=7, sum(softdtw_loss).item()=249.76956176757812, normed_loss.item()=16.651304244995117\n",
      "X_len=8, Y_len=8, sum(softdtw_loss).item()=255.9918212890625, normed_loss.item()=15.999488830566406\n",
      "X_len=8, Y_len=9, sum(softdtw_loss).item()=281.34124755859375, normed_loss.item()=16.54948616027832\n",
      "X_len=9, Y_len=1, sum(softdtw_loss).item()=288.0, normed_loss.item()=28.80000114440918\n",
      "X_len=9, Y_len=2, sum(softdtw_loss).item()=281.3456726074219, normed_loss.item()=25.576879501342773\n",
      "X_len=9, Y_len=3, sum(softdtw_loss).item()=277.3365478515625, normed_loss.item()=23.111379623413086\n",
      "X_len=9, Y_len=4, sum(softdtw_loss).item()=275.1181945800781, normed_loss.item()=21.162939071655273\n",
      "X_len=9, Y_len=5, sum(softdtw_loss).item()=274.40374755859375, normed_loss.item()=19.600269317626953\n",
      "X_len=9, Y_len=6, sum(softdtw_loss).item()=275.1172180175781, normed_loss.item()=18.341148376464844\n",
      "X_len=9, Y_len=7, sum(softdtw_loss).item()=277.3343505859375, normed_loss.item()=17.333396911621094\n",
      "X_len=9, Y_len=8, sum(softdtw_loss).item()=281.34124755859375, normed_loss.item()=16.54948616027832\n",
      "X_len=9, Y_len=9, sum(softdtw_loss).item()=287.9895935058594, normed_loss.item()=15.999422073364258\n"
     ]
    }
   ],
   "source": [
    "criterion = SoftDTW(use_cuda=True, gamma=0.1)\n",
    "\n",
    "# test softdtw func to see if it avgs for different len n or m\n",
    "device = y['mel_padded'].device\n",
    "\n",
    "for X_len in range(1,10):\n",
    "    for Y_len in range(1,10):\n",
    "\n",
    "        X = torch.zeros(32, X_len,1).to(device)\n",
    "        Y = torch.ones(32, Y_len,1).to(device)\n",
    "\n",
    "        softdtw_loss = criterion(X, Y)\n",
    "        \n",
    "        normed_loss = sum(softdtw_loss) / (X_len + Y_len)\n",
    "        \n",
    "        print(f\"{X_len=}, {Y_len=}, {sum(softdtw_loss).item()=}, {normed_loss.item()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "d044a615-5286-4310-87ea-9ade58cfe256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_num_ones=1, sum(softdtw_loss).item()=-18.46923065185547, -1.0260683298110962\n",
      "X_num_ones=2, sum(softdtw_loss).item()=-25.50031089782715, -1.4166839122772217\n",
      "X_num_ones=3, sum(softdtw_loss).item()=-30.352611541748047, -1.6862561702728271\n",
      "X_num_ones=4, sum(softdtw_loss).item()=-34.02057647705078, -1.8900320529937744\n",
      "X_num_ones=5, sum(softdtw_loss).item()=-36.93779373168945, -2.0520997047424316\n",
      "X_num_ones=6, sum(softdtw_loss).item()=-39.3445930480957, -2.1858108043670654\n",
      "X_num_ones=7, sum(softdtw_loss).item()=-41.3863639831543, -2.2992424964904785\n",
      "X_num_ones=8, sum(softdtw_loss).item()=-43.1561393737793, -2.3975632190704346\n",
      "X_num_ones=9, sum(softdtw_loss).item()=-44.71626663208008, -2.4842369556427\n"
     ]
    }
   ],
   "source": [
    "criterion = SoftDTW(use_cuda=True, gamma=0.1)\n",
    "\n",
    "# test softdtw func to see if it avgs for different len n or m\n",
    "device = y['mel_padded'].device\n",
    "\n",
    "for X_num_ones in range(1,10):\n",
    "    X = torch.cat([torch.ones(32, X_num_ones, 1), torch.zeros(32, 5, 1)], dim=1).to(device)\n",
    "    Y = torch.cat([torch.ones(32, 5, 1), torch.zeros(32, 5, 1)], dim=1).to(device)\n",
    "\n",
    "    softdtw_loss = criterion(X, Y)\n",
    "\n",
    "    normed_loss = sum(softdtw_loss) / (X_len + Y_len)\n",
    "\n",
    "    print(f\"{X_num_ones=}, {sum(softdtw_loss).item()=}, {normed_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "33007895-c3c9-4be3-8824-f01b039813e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_num_ones=1, sum(softdtw_loss).item()=-1.8468618392944336, -0.10260343551635742\n",
      "X_num_ones=2, sum(softdtw_loss).item()=-2.5499727725982666, -0.14166516065597534\n",
      "X_num_ones=3, sum(softdtw_loss).item()=-3.0352039337158203, -0.1686224490404129\n",
      "X_num_ones=4, sum(softdtw_loss).item()=-3.402000665664673, -0.18900004029273987\n",
      "X_num_ones=5, sum(softdtw_loss).item()=-3.693723678588867, -0.20520687103271484\n",
      "X_num_ones=6, sum(softdtw_loss).item()=-3.9344019889831543, -0.21857789158821106\n",
      "X_num_ones=7, sum(softdtw_loss).item()=-4.138580322265625, -0.2299211323261261\n",
      "X_num_ones=8, sum(softdtw_loss).item()=-4.315558433532715, -0.23975324630737305\n",
      "X_num_ones=9, sum(softdtw_loss).item()=-4.471567630767822, -0.24842043220996857\n"
     ]
    }
   ],
   "source": [
    "criterion = SoftDTW(use_cuda=True, gamma=0.01)\n",
    "\n",
    "# test softdtw func to see if it avgs for different len n or m\n",
    "device = y['mel_padded'].device\n",
    "\n",
    "for X_num_ones in range(1,10):\n",
    "    X = torch.cat([torch.ones(32, X_num_ones, 1), torch.zeros(32, 5, 1)], dim=1).to(device)\n",
    "    Y = torch.cat([torch.ones(32, 5, 1), torch.zeros(32, 5, 1)], dim=1).to(device)\n",
    "\n",
    "    softdtw_loss = criterion(X, Y)\n",
    "\n",
    "    normed_loss = sum(softdtw_loss) / (X_len + Y_len)\n",
    "\n",
    "    print(f\"{X_num_ones=}, {sum(softdtw_loss).item()=}, {normed_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "8320088e-6695-4972-83d4-0469df261e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_num_ones=1, sum(softdtw_loss).item()=-1.8468618392944336, -0.10260343551635742\n",
      "X_num_ones=2, sum(softdtw_loss).item()=-2.5499727725982666, -0.14166516065597534\n",
      "X_num_ones=3, sum(softdtw_loss).item()=-3.0352039337158203, -0.1686224490404129\n",
      "X_num_ones=4, sum(softdtw_loss).item()=-3.402000665664673, -0.18900004029273987\n",
      "X_num_ones=5, sum(softdtw_loss).item()=-3.693723678588867, -0.20520687103271484\n",
      "X_num_ones=6, sum(softdtw_loss).item()=-3.9344019889831543, -0.21857789158821106\n",
      "X_num_ones=7, sum(softdtw_loss).item()=-4.138580322265625, -0.2299211323261261\n",
      "X_num_ones=8, sum(softdtw_loss).item()=-4.315558433532715, -0.23975324630737305\n",
      "X_num_ones=9, sum(softdtw_loss).item()=-4.471567630767822, -0.24842043220996857\n"
     ]
    }
   ],
   "source": [
    "criterion = SoftDTW(use_cuda=True, gamma=0.01, bandwidth=120)\n",
    "\n",
    "# test softdtw func to see if it avgs for different len n or m\n",
    "device = y['mel_padded'].device\n",
    "\n",
    "for X_num_ones in range(1,10):\n",
    "    X = torch.cat([torch.ones(32, X_num_ones, 1), torch.zeros(32, 5, 1)], dim=1).to(device)\n",
    "    Y = torch.cat([torch.ones(32, 5, 1), torch.zeros(32, 5, 1)], dim=1).to(device)\n",
    "\n",
    "    softdtw_loss = criterion(X, Y)\n",
    "\n",
    "    normed_loss = sum(softdtw_loss) / (X_len + Y_len)\n",
    "\n",
    "    print(f\"{X_num_ones=}, {sum(softdtw_loss).item()=}, {normed_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd55e41-8cbd-4107-bb04-a259999aea17",
   "metadata": {},
   "source": [
    "# DEBUG L1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "d1241c26-2d29-4c01-85b2-a566029a7512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_num_ones=1, sum(softdtw_loss).item()=-1.8468618392944336, -0.10260343551635742\n",
      "X_num_ones=2, sum(softdtw_loss).item()=-2.5499727725982666, -0.14166516065597534\n",
      "X_num_ones=3, sum(softdtw_loss).item()=-3.0352039337158203, -0.1686224490404129\n",
      "X_num_ones=4, sum(softdtw_loss).item()=-3.402000665664673, -0.18900004029273987\n",
      "X_num_ones=5, sum(softdtw_loss).item()=-3.693723678588867, -0.20520687103271484\n",
      "X_num_ones=6, sum(softdtw_loss).item()=-3.9344019889831543, -0.21857789158821106\n",
      "X_num_ones=7, sum(softdtw_loss).item()=-4.138580322265625, -0.2299211323261261\n",
      "X_num_ones=8, sum(softdtw_loss).item()=-4.315558433532715, -0.23975324630737305\n",
      "X_num_ones=9, sum(softdtw_loss).item()=-4.471567630767822, -0.24842043220996857\n"
     ]
    }
   ],
   "source": [
    "def mean_absolute_error(x, y):\n",
    "    \"\"\"for calculating softdtw using L1 loss\n",
    "    Calculates the Euclidean distance between each element in x and y per timestep\n",
    "    \"\"\"\n",
    "    n = x.size(1)\n",
    "    m = y.size(1)\n",
    "    d = x.size(2)\n",
    "    x = x.unsqueeze(2).expand(-1, n, m, d)\n",
    "    y = y.unsqueeze(1).expand(-1, n, m, d)\n",
    "    return torch.abs(x - y).sum(3)\n",
    "\n",
    "criterion = SoftDTW(use_cuda=True, gamma=0.01, bandwidth=120, dist_func=mean_absolute_error)\n",
    "\n",
    "# test softdtw func to see if it avgs for different len n or m\n",
    "device = y['mel_padded'].device\n",
    "\n",
    "for X_num_ones in range(1,10):\n",
    "    X = torch.cat([torch.ones(32, X_num_ones, 1), torch.zeros(32, 5, 1)], dim=1).to(device)\n",
    "    Y = torch.cat([torch.ones(32, 5, 1), torch.zeros(32, 5, 1)], dim=1).to(device)\n",
    "\n",
    "    softdtw_loss = criterion(X, Y)\n",
    "\n",
    "    normed_loss = sum(softdtw_loss) / (X_len + Y_len)\n",
    "\n",
    "    print(f\"{X_num_ones=}, {sum(softdtw_loss).item()=}, {normed_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fbeda7-e49d-4d7c-acad-e244fee1e248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
