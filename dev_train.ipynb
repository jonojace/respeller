{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f5a3c3-31af-4835-88ee-4410bc5af326",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de0d84a-9335-48be-8bfb-ba9cbbf0dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53034b89-b847-4927-a604-cc6cb78516c2",
   "metadata": {},
   "source": [
    "# dev train loop + model for respeller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72cec08-fc1a-459b-87d4-49f1219cc86d",
   "metadata": {},
   "source": [
    "# command line args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# imitate CLAs\n",
    "import sys\n",
    "sys.argv = [\n",
    "    'train.py',\n",
    "    '--fastpitch-chkpt', 'fastpitch/exps/halved_ljspeech_data/FastPitch_checkpoint_1000.pt',\n",
    "    '--input-type', 'char',\n",
    "    '--symbol-set', 'english_basic_lowercase_no_arpabet',\n",
    "    '--use-mas',\n",
    "    '--cuda',\n",
    "    '--n-speakers', '1',\n",
    "    '--use-sepconv',\n",
    "    '--add-spaces',\n",
    "    '--eos-symbol', '$',\n",
    "    '--epochs', '10000', # NB small number for development!\n",
    "    '--batch-size', '32',\n",
    "    '--chkpt-save-dir', '/home/s1785140/respeller/exps/test', \n",
    "    '--val-log-interval', '10',\n",
    "    '--learning-rate', '0.1',\n",
    "    # '--resume', # resume from latest checkpoint\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Train respeller model\n",
    "\n",
    "We backpropagate loss from pretrained TTS model to a Grapheme-to-Grapheme (G2G) respeller model to help it respell words\n",
    "into a simpler form\n",
    "\n",
    "Intermediated respellings are discrete character sequences\n",
    "We can backpropagate through these using gumbel softmax and the straight through estimator\n",
    "'''\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "from fastpitch import models as fastpitch_model\n",
    "from fastpitch.common.text.text_processing import TextProcessor\n",
    "\n",
    "from modules.model import EncoderRespeller\n",
    "from modules.gumbel_vector_quantizer import GumbelVectorQuantizer\n",
    "from modules.sdtw_cuda_loss import SoftDTW\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## arguments to parse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_args(parser):\n",
    "    \"\"\"Parse commandline arguments\"\"\"\n",
    "    parser.add_argument('-o', '--chkpt-save-dir', type=str, required=True,\n",
    "                        help='Directory to save checkpoints')\n",
    "    parser.add_argument('-d', '--dataset-path', type=str, default='./',\n",
    "                        help='Path to dataset')\n",
    "    parser.add_argument('--log-file', type=str, default=None,\n",
    "                        help='Path to a DLLogger log file')\n",
    "\n",
    "    train = parser.add_argument_group('training setup')\n",
    "    train.add_argument('--cuda', action='store_true',\n",
    "                      help='Enable GPU training')\n",
    "    train.add_argument('--batch-size', type=int, default=16,\n",
    "                      help='Batchsize (this is divided by number of GPUs if running Data Distributed Parallel Training)')\n",
    "    train.add_argument('--seed', type=int, default=1337,\n",
    "                       help='Seed for PyTorch random number generators')\n",
    "    train.add_argument('--grad-accumulation', type=int, default=1,\n",
    "                       help='Training steps to accumulate gradients for')\n",
    "    train.add_argument('--epochs', type=int, default=100, #required=True,\n",
    "                       help='Number of total epochs to run')\n",
    "    train.add_argument('--epochs-per-checkpoint', type=int, default=10,\n",
    "                       help='Number of epochs per checkpoint')\n",
    "    train.add_argument('--checkpoint-path', type=str, default=None,\n",
    "                       help='Checkpoint path to resume train')\n",
    "    train.add_argument('--resume', action='store_true',\n",
    "                       help='Resume train from the last available checkpoint')\n",
    "    train.add_argument('--val-log-interval', type=int, default=5,\n",
    "                       help='How often to generate melspecs/audio for respellings and log to wandb')\n",
    "    \n",
    "    opt = parser.add_argument_group('optimization setup')\n",
    "    opt.add_argument('--optimizer', type=str, default='lamb', choices=['adam', 'lamb'],\n",
    "                     help='Optimization algorithm')\n",
    "    opt.add_argument('-lr', '--learning-rate', default=0.1, type=float,\n",
    "                     help='Learning rate')\n",
    "    opt.add_argument('--weight-decay', default=1e-6, type=float,\n",
    "                     help='Weight decay')\n",
    "    opt.add_argument('--grad-clip-thresh', default=1000.0, type=float,\n",
    "                     help='Clip threshold for gradients')\n",
    "    opt.add_argument('--warmup-steps', type=int, default=1000,\n",
    "                     help='Number of steps for lr warmup')\n",
    "\n",
    "    arch = parser.add_argument_group('architecture')\n",
    "    arch.add_argument('--d-model', type=int, default=512,\n",
    "                       help='Hidden dimension of tranformer')\n",
    "    arch.add_argument('--latent-temp', type=tuple, default=(2, 0.5, 0.999995),\n",
    "                       help='Temperature annealling parameters for Gumbel-Softmax (start, end, decay)')\n",
    "\n",
    "    pretrained_tts = parser.add_argument_group('pretrained tts model')\n",
    "    # pretrained_tts.add_argument('--fastpitch-with-mas', type=bool, default=True,\n",
    "    #                   help='Whether or not fastpitch was trained with Monotonic Alignment Search (MAS)')\n",
    "    pretrained_tts.add_argument('--fastpitch-chkpt', type=str, required=True,\n",
    "                      help='Path to pretrained fastpitch checkpoint')\n",
    "    pretrained_tts.add_argument('--input-type', type=str, default='char',\n",
    "                      choices=['char', 'phone', 'pf', 'unit'],\n",
    "                      help='Input symbols used, either char (text), phone, pf '\n",
    "                      '(phonological feature vectors) or unit (quantized acoustic '\n",
    "                      'representation IDs)')\n",
    "    pretrained_tts.add_argument('--symbol-set', type=str, default='english_basic_lowercase',\n",
    "                      help='Define symbol set for input sequences. For quantized '\n",
    "                      'unit inputs, pass the size of the vocabulary.')\n",
    "    pretrained_tts.add_argument('--n-speakers', type=int, default=1,\n",
    "                      help='Condition on speaker, value > 1 enables trainable '\n",
    "                      'speaker embeddings.')\n",
    "    # pretrained_tts.add_argument('--use-sepconv', type=bool, default=True,\n",
    "    #                   help='Use depthwise separable convolutions')\n",
    "    \n",
    "    audio = parser.add_argument_group('log generated audio')\n",
    "    audio.add_argument('--hifigan', type=str, default='/home/s1785140/pretrained_models/hifigan/ljspeech/LJ_V1/generator_v1',\n",
    "                       help='Path to HiFi-GAN audio checkpoint')\n",
    "    audio.add_argument('--hifigan-config', type=str, default='/home/s1785140/pretrained_models/hifigan/ljspeech/LJ_V1/config.json',\n",
    "                       help='Path to HiFi-GAN audio config file')\n",
    "    audio.add_argument('--sampling-rate', type=int, default=22050,\n",
    "                       help='Sampling rate for output audio')\n",
    "    audio.add_argument('--hop-length', type=int, default=256,\n",
    "                       help='STFT hop length for estimating audio length from mel size')\n",
    "    \n",
    "    data = parser.add_argument_group('dataset parameters')\n",
    "    cond = parser.add_argument_group('conditioning on additional attributes')\n",
    "    dist = parser.add_argument_group('distributed training setup')\n",
    "\n",
    "    return parser\n",
    "\n",
    "def load_checkpoint(args, model, filepath):\n",
    "    if args.local_rank == 0:\n",
    "        print(f'Loading model and optimizer state from {filepath}')\n",
    "    checkpoint = torch.load(filepath, map_location='cpu')\n",
    "    sd = {k.replace('module.', ''): v\n",
    "          for k, v in checkpoint['state_dict'].items()}\n",
    "    getattr(model, 'module', model).load_state_dict(sd)\n",
    "    return model\n",
    "\n",
    "def load_respeller_checkpoint(args, model, filepath, optimizer, epoch, total_iter):\n",
    "    if args.local_rank == 0:\n",
    "        print(f'Loading model and optimizer state from {filepath}')\n",
    "    checkpoint = torch.load(filepath, map_location='cpu')\n",
    "    epoch[0] = checkpoint['epoch'] + 1\n",
    "    total_iter[0] = checkpoint['iteration']\n",
    "    sd = {k.replace('module.', ''): v\n",
    "          for k, v in checkpoint['state_dict'].items()}\n",
    "    getattr(model, 'module', model).load_state_dict(sd)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model\n",
    "\n",
    "def last_checkpoint(output):\n",
    "    saved = sorted(\n",
    "        glob.glob(f'{output}/respeller_checkpoint_*.pt'),\n",
    "        key=lambda f: int(re.search('_(\\d+).pt', f).group(1)))\n",
    "\n",
    "    def corrupted(fpath):\n",
    "        try:\n",
    "            torch.load(fpath, map_location='cpu')\n",
    "            return False\n",
    "        except:\n",
    "            warnings.warn(f'Cannot load {fpath}')\n",
    "            return True\n",
    "\n",
    "    if len(saved) >= 1 and not corrupted(saved[-1]):\n",
    "        return saved[-1]\n",
    "    elif len(saved) >= 2:\n",
    "        return saved[-2]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def init_embedding_weights(source_tensor, target_tensor):\n",
    "    \"\"\"copy weights inplace from source tensor to target tensor\"\"\"\n",
    "    target_tensor.requires_grad = False\n",
    "    target_tensor.copy_(source_tensor.clone().detach())\n",
    "    target_tensor.requires_grad = True\n",
    "\n",
    "def load_pretrained_fastpitch(args):\n",
    "    # load chkpt\n",
    "    device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "    model_config = fastpitch_model.get_model_config('FastPitch', args)\n",
    "    fastpitch = fastpitch_model.get_model('FastPitch', model_config, device, forward_is_infer=True)\n",
    "    load_checkpoint(args, fastpitch, args.fastpitch_chkpt)\n",
    "    # get information about grapheme embedding table\n",
    "    n_symbols = fastpitch.encoder.word_emb.weight.size(0)\n",
    "    grapheme_embedding_dim = fastpitch.encoder.word_emb.weight.size(1)\n",
    "    return fastpitch, n_symbols, grapheme_embedding_dim, model_config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# beginning of main(), parse Command Line Args"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Respeller Training', allow_abbrev=False)\n",
    "parser = parse_args(parser)\n",
    "args, _unk_args = parser.parse_known_args()\n",
    "\n",
    "parser = fastpitch_model.parse_model_args('FastPitch', parser)\n",
    "args, unk_args = parser.parse_known_args()\n",
    "if len(unk_args) > 0:\n",
    "    raise ValueError(f'Invalid options {unk_args}')\n",
    "\n",
    "if args.cuda:\n",
    "    args.num_gpus = torch.cuda.device_count()\n",
    "    args.distributed_run = args.num_gpus > 1\n",
    "    args.batch_size = int(args.batch_size / args.num_gpus)\n",
    "else:\n",
    "    args.distributed_run = False\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "if args.distributed_run:\n",
    "    mp.spawn(train, nprocs=args.num_gpus, args=(args,))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## WANDB - weights and biases init"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login() # needed for wandb integration with jupyter notebook"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%env \"WANDB_NOTEBOOK_NAME\" \"respeller-dev-train-ipynb\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d/%m/%Y_%H:%M:%S\")\n",
    "print(\"date and time =\", dt_string)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = 0\n",
    "\n",
    "# store important information into WANDB config for easier tracking of experiments\n",
    "# add all key values from parser\n",
    "wandb_config = vars(args)\n",
    "wandb.init(\n",
    "    project=\"respeller-dev-train-ipynb\",\n",
    "    name=f\"experiment_{run}_{dt_string}\",\n",
    "    config=wandb_config,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 'train()' - forward pass through model to get loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## create / load models "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rank = 0\n",
    "device = 'cuda'\n",
    "\n",
    "args.local_rank = rank\n",
    "tts, n_symbols, grapheme_embedding_dim, model_config = load_pretrained_fastpitch(args)\n",
    "tts.to(device)\n",
    "\n",
    "respeller = EncoderRespeller(n_symbols=n_symbols, pretrained_tts=tts, d_model=args.d_model)\n",
    "respeller.to(device)\n",
    "\n",
    "# quantiser = GumbelVectorQuantizer(\n",
    "#     in_dim=args.d_model,\n",
    "#     codebook_size=n_symbols,  # number of codebook entries\n",
    "#     embedding_dim=grapheme_embedding_dim,\n",
    "#     temp=args.latent_temp,\n",
    "# )\n",
    "# quantiser.to(device)\n",
    "\n",
    "# init_embedding_weights(tts.encoder.word_emb.weight.unsqueeze(0), quantiser.vars)\n",
    "\n",
    "\n",
    "# batch_size, len_x, len_y, dims = 8, 15, 12, 5\n",
    "# x = torch.rand((batch_size, len_x, dims), requires_grad=True)\n",
    "# y = torch.rand((batch_size, len_y, dims))\n",
    "\n",
    "# criterion = SoftDTW(use_cuda=True, gamma=0.1, dist_func=F.mse_loss)\n",
    "criterion = SoftDTW(use_cuda=True, gamma=0.1)\n",
    "# input should be size [bsz, seqlen, dim]\n",
    "criterion.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### load HiFiGAN vocoder "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_vocoder(args, device):\n",
    "    \"\"\"Load HiFi-GAN vocoder from checkpoint\"\"\"\n",
    "    checkpoint_data = torch.load(args.hifigan)\n",
    "    vocoder_config = fastpitch_model.get_model_config('HiFi-GAN', args)\n",
    "    vocoder = fastpitch_model.get_model('HiFi-GAN', vocoder_config, device)\n",
    "    vocoder.load_state_dict(checkpoint_data['generator'])\n",
    "    vocoder.remove_weight_norm()\n",
    "    vocoder.eval()\n",
    "    return vocoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocoder = load_vocoder(args, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## forward pass through model with dummy data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batches = []\n",
    "symbol_set = 'english_basic_lowercase'\n",
    "text_cleaners = []\n",
    "gt_log_mel = torch.load('/home/s1785140/data/ljspeech_fastpitch/mels/LJ001-0001.pt').cuda().unsqueeze(0).transpose(1,2) # intro batch dimension + [bsz, seqlen, dim]\n",
    "raw_text = 'printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the exhibition'\n",
    "\n",
    "# process text using same processor as fastpitch\n",
    "tp = TextProcessor(symbol_set, text_cleaners)\n",
    "text = torch.LongTensor(tp.encode_text(raw_text)).unsqueeze(0).cuda()\n",
    "\n",
    "batches.append((text, gt_log_mel))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text.device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for batch in batches:\n",
    "batch = batches[0]\n",
    "    \n",
    "###############################################################################################################\n",
    "# text, ssl_reps, e2e_asr_predictions, gt_log_mel = batch\n",
    "text, gt_log_mel = batch\n",
    "\n",
    "###############################################################################################################\n",
    "# create inputs\n",
    "# if args.use_acoustic_input:\n",
    "#     inputs = inputs.concat(ssl_reps)\n",
    "\n",
    "###############################################################################################################\n",
    "# forward pass\n",
    "g_embeddings, g_embedding_indices = respeller(text[:13])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_symbols"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g_embedding_indices.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g_embeddings.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "padding_idx = 0\n",
    "mask = (g_embedding_indices != padding_idx).unsqueeze(2)\n",
    "mask.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_mel, dec_lens, _dur_pred, _pitch_pred = tts(g_embeddings, skip_embeddings=True, ids=g_embedding_indices)\n",
    "# log_mel [bsz, dim, seqlen]\n",
    "log_mel = log_mel.transpose(1,2)\n",
    "# log_mel [bsz, seqlen, dim]\n",
    "\n",
    "print(f'{log_mel.size()=}')\n",
    "print(f'{gt_log_mel.size()=}')\n",
    "\n",
    "###############################################################################################################\n",
    "# calculate val_losses\n",
    "# respelling_loss = respelling_loss_fn(respelling, e2e_asr_predictions)\n",
    "acoustic_loss = criterion(log_mel, gt_log_mel)\n",
    "\n",
    "# average loss over frames \n",
    "acoustic_loss = acoustic_loss / dec_lens\n",
    "# mel_loss = (mel_loss * mel_mask).sum() / mel_mask.sum()\n",
    "\n",
    "###############################################################################################################\n",
    "# backward pass\n",
    "loss = acoustic_loss \n",
    "\n",
    "print(f'{loss=}')\n",
    "\n",
    "# loss.backward()\n",
    "\n",
    "###############################################################################################################\n",
    "# log tensorboard metrics\n",
    "\n",
    "###############################################################################################################\n",
    "# validation set evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_spectrogram(log_mel, figsize=(15,5), wandb_log=False, image_name=\"\"):\n",
    "    plt.figure(figsize=figsize)\n",
    "    librosa.display.specshow(log_mel, x_axis='frames', y_axis='linear')\n",
    "    plt.colorbar()\n",
    "    if wandb_log:\n",
    "        wandb.log({image_name: wandb.Image(plt, caption=image_name)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_index = 0\n",
    "print(f'{log_mel[batch_index].transpose(0,1).size()=}')\n",
    "plot_spectrogram(log_mel[batch_index].transpose(0,1).detach().cpu().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # play audio\n",
    "# import IPython.display as ipd\n",
    "# audio = vocoder(log_mel[batch_index].transpose(0,1).detach().unsqueeze(0))\n",
    "# ipd.Audio(audio, rate=22050)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_index = 0\n",
    "plot_spectrogram(gt_log_mel[batch_index].transpose(0,1).detach().cpu().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# develop respeller dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wordaligned_speechreps_dir = '/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels' # path to directory that contains folders of word aligned speech reps\n",
    "wordlist = ['identifies','mash','player','russias','techniques'] # txt file for the words to include speech reps "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token_and_melfilepaths = []\n",
    "for word in wordlist:\n",
    "    # find all word aligned mels for the word\n",
    "    word_dir = os.path.join(wordaligned_speechreps_dir, word)\n",
    "    mel_files = os.listdir(word_dir)\n",
    "    for mel_file in mel_files:\n",
    "        mel_file_path = os.path.join(word_dir, mel_file)\n",
    "        token_and_melfilepaths.append((word, mel_file_path))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token_and_melfilepaths"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## process text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from fastpitch.common.text.text_processing import TextProcessor\n",
    "text_cleaners = []\n",
    "symbol_set = \"english_basic_lowercase_no_arpabet\"\n",
    "tp = TextProcessor(symbol_set, text_cleaners, add_spaces=True, eos_symbol=\"$\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoded = torch.IntTensor(tp.encode_text('identifies'))\n",
    "encoded"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "decoded = [tp.id_to_symbol[id] for id in encoded.tolist()]\n",
    "decoded"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## process mel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word, fp = token_and_melfilepaths[0]\n",
    "wordaligned_mel = torch.load(fp)\n",
    "wordaligned_mel.size() # [seqlen, feats]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 'class'-ified dataset class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from fastpitch.common.text.text_processing import TextProcessor\n",
    "\n",
    "class RespellerDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "        1) loads word + word-aligned mel spec for all words in a wordlist\n",
    "        2) converts text to sequences of one-hot vectors (corresponding to grapheme indices in fastpitch)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        wordaligned_speechreps_dir, # path to directory that contains folders of word aligned speech reps\n",
    "        wordlist, # txt file for the words to include speech reps from\n",
    "        max_examples_per_wordtype=None,\n",
    "        text_cleaners=[],\n",
    "        symbol_set=\"english_basic_lowercase_no_arpabet\",\n",
    "        add_spaces=True,\n",
    "        eos_symbol=\"$\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # load wordlist as a python list\n",
    "        if type(wordlist) == str:\n",
    "            if wordlist.endswith('.json'):\n",
    "                with open(wordlist) as f:\n",
    "                    wordlist = json.load(f)\n",
    "            else:\n",
    "                with open(wordlist) as f:\n",
    "                    wordlist = f.read().splitlines()\n",
    "        elif type(wordlist) == list:\n",
    "            pass # dont need to do anything, already in expected form\n",
    "        elif type(wordlist) == set:\n",
    "            wordlist = list(wordlist)\n",
    "        \n",
    "        wordlist = sorted(wordlist)\n",
    "        \n",
    "        # create list of all word tokens and their word aligned speech reps\n",
    "        self.word_freq = Counter()\n",
    "        self.token_and_melfilepaths = []\n",
    "        print(\"Initialising respeller dataset\")\n",
    "        for word in tqdm(wordlist):\n",
    "            # find all word aligned mels for the word\n",
    "            word_dir = os.path.join(wordaligned_speechreps_dir, word)\n",
    "            mel_files = os.listdir(word_dir)\n",
    "            if max_examples_per_wordtype:\n",
    "                mel_files = mel_files[:max_examples_per_wordtype]\n",
    "            for mel_file in mel_files:\n",
    "                mel_file_path = os.path.join(word_dir, mel_file)\n",
    "                self.token_and_melfilepaths.append((word, mel_file_path))\n",
    "                self.word_freq[word] += 1\n",
    "                \n",
    "        self.tp = TextProcessor(symbol_set, text_cleaners, add_spaces=add_spaces, eos_symbol=eos_symbol)\n",
    "\n",
    "    def get_mel(self, filename):\n",
    "        return torch.load(filename)\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        \"\"\"encode raw text into indices defined by grapheme embedding table of the TTS model\"\"\"\n",
    "        return torch.IntTensor(self.tp.encode_text(text))\n",
    "    \n",
    "    def decode_text(self, encoded):\n",
    "        return [self.tp.id_to_symbol[id] for id in encoded.tolist()]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_mel_len(melfilepath):\n",
    "        return int(melfilepath.split('seqlen')[1].split('.pt')[0])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        word, mel_filepath = self.token_and_melfilepaths[index]\n",
    "        encoded_word = self.encode_text(word)\n",
    "        mel = self.get_mel(mel_filepath)\n",
    "        \n",
    "        return {\n",
    "            'word': word, \n",
    "            'encoded_word': encoded_word, \n",
    "            'mel_filepath': mel_filepath,\n",
    "            'mel': mel,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_and_melfilepaths)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels', # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist=['identifies','mash','player','russias','techniques'],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch = []\n",
    "\n",
    "for itemdict in dataset:\n",
    "    # unpack dict\n",
    "    word = itemdict['word'] \n",
    "    encoded_word = itemdict['encoded_word'] \n",
    "    mel = itemdict['mel'] \n",
    "    \n",
    "    # check\n",
    "    print(word, encoded_word, mel.size())\n",
    "    \n",
    "    batch.append(itemdict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## collate function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Collate():\n",
    "    \"\"\" Zero-pads model inputs and targets based on number of frames per setep\n",
    "    \"\"\"\n",
    "    # def __init__(self):\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Collate's training batch from encoded word token and its \n",
    "        corresponding word-aligned mel spectrogram\n",
    "        \n",
    "        batch: [encoded_token, wordaligned_mel]\n",
    "        \"\"\"\n",
    "        # Right zero-pad all one-hot text sequences to max input length\n",
    "        input_lengths, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([len(x['encoded_word']) for x in batch]),\n",
    "            dim=0, descending=True)\n",
    "        max_input_len = input_lengths[0]\n",
    "\n",
    "        words = []\n",
    "        mel_filepaths = []\n",
    "        text_padded = torch.LongTensor(len(batch), max_input_len)\n",
    "        text_padded.zero_()\n",
    "        text_lengths = torch.LongTensor(len(batch))\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            words.append(batch[ids_sorted_decreasing[i]]['word'])\n",
    "            mel_filepaths.append(batch[ids_sorted_decreasing[i]]['mel_filepath'])\n",
    "            text = batch[ids_sorted_decreasing[i]]['encoded_word']\n",
    "            text_padded[i, :text.size(0)] = text\n",
    "            text_lengths[i] = text.size(0)\n",
    "\n",
    "        # Right zero-pad mel-spec\n",
    "        num_mels = batch[0]['mel'].size(1)\n",
    "        max_target_len = max([x['mel'].size(0) for x in batch])\n",
    "\n",
    "        mel_padded = torch.FloatTensor(len(batch), max_target_len, num_mels)\n",
    "        mel_padded.zero_()\n",
    "        mel_lengths = torch.LongTensor(len(batch))\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            mel = batch[ids_sorted_decreasing[i]]['mel']\n",
    "            mel_padded[i, :mel.size(0), :] = mel\n",
    "            mel_lengths[i] = mel.size(0)\n",
    "            \n",
    "\n",
    "        return {\n",
    "            'words': words,\n",
    "            'text_padded': text_padded,\n",
    "            'text_lengths': text_lengths,\n",
    "            'mel_padded': mel_padded, \n",
    "            'mel_lengths': mel_lengths,\n",
    "            'mel_filepaths': mel_filepaths\n",
    "        }\n",
    "                # input_lengths, mel_padded, output_lengths,\n",
    "                # len_x, dur_padded, dur_lens, pitch_padded, speaker)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collate_fn = Collate()\n",
    "collated = collate_fn(batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collated['text_padded'].size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collated['text_padded']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collated['words']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collated['mel_padded'].size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## put batch on gpu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def to_gpu(x):\n",
    "    x = x.contiguous()\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda(non_blocking=True)\n",
    "    return torch.autograd.Variable(x)\n",
    "\n",
    "def batch_to_gpu(collated_batch):\n",
    "    \"\"\"put elements that are used throughout training onto gpu\"\"\"\n",
    "    words = collated_batch['words']\n",
    "    text_padded = collated_batch['text_padded']\n",
    "    text_lengths = collated_batch['text_lengths']\n",
    "    mel_padded = collated_batch['mel_padded']\n",
    "    mel_lengths = collated_batch['mel_lengths']\n",
    "    \n",
    "    # no need to put words on gpu, its only used during eval loop\n",
    "    text_padded = to_gpu(text_padded).long()\n",
    "    text_lengths = to_gpu(text_lengths).long()\n",
    "    mel_padded = to_gpu(mel_padded).float()\n",
    "    mel_lengths = to_gpu(mel_lengths).long()\n",
    "    \n",
    "    # x: inputs\n",
    "    x = {\n",
    "        'words': words,\n",
    "        'text_padded': text_padded,\n",
    "        'text_lengths': text_lengths,\n",
    "    }\n",
    "    # y: targets\n",
    "    y = {\n",
    "        'mel_padded': mel_padded, \n",
    "        'mel_lengths': mel_lengths,\n",
    "    }\n",
    "    \n",
    "    return (x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# batch_to_gpu(collated)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# full train + dev datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels', # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist='/home/s1785140/data/ljspeech_fastpitch/respeller_train_words.json',\n",
    "    max_examples_per_wordtype=2,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum(train_dataset.word_freq.values())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels', # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist='/home/s1785140/data/ljspeech_fastpitch/respeller_dev_words.json',\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(val_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum(val_dataset.word_freq.values())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# create torch dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO, implement distributed training?\n",
    "train_sampler = None\n",
    "shuffle = True\n",
    "num_cpus = 1 \n",
    "train_loader = DataLoader(train_dataset, num_workers=2*num_cpus, shuffle=shuffle,\n",
    "                          sampler=train_sampler, batch_size=args.batch_size,\n",
    "                          pin_memory=False, drop_last=True,\n",
    "                          collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#     print(batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FULL train() loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## init dl logger"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import fastpitch.common.tb_dllogger as logger\n",
    "\n",
    "def touch_file(path):\n",
    "    if not os.path.exists(path):\n",
    "        basedir = os.path.dirname(path)\n",
    "        os.makedirs(basedir, exist_ok=True)\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(\"\")\n",
    "\n",
    "# initialise logger\n",
    "tb_subsets = ['train', 'val']\n",
    "log_fpath = args.log_file or os.path.join(args.chkpt_save_dir, 'nvlog.json')\n",
    "touch_file(log_fpath)\n",
    "\n",
    "try: \n",
    "    logger.init(log_fpath, args.chkpt_save_dir, enabled=(args.local_rank == 0),\n",
    "                tb_subsets=tb_subsets)\n",
    "    logger.parameters(vars(args), tb_subset='train')\n",
    "except:\n",
    "    print(\"WARNING DLLLoggerAlreadyInitialized error raised\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch_optimizer import Lamb\n",
    "import time\n",
    "from fastpitch.common.utils import mask_from_lens\n",
    "from collections import OrderedDict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adjust_learning_rate(total_iter, opt, learning_rate, warmup_iters=None):\n",
    "    if warmup_iters == 0:\n",
    "        scale = 1.0\n",
    "    elif total_iter > warmup_iters:\n",
    "        scale = 1. / (total_iter ** 0.5)\n",
    "    else:\n",
    "        scale = total_iter / (warmup_iters ** 1.5)\n",
    "\n",
    "    for param_group in opt.param_groups:\n",
    "        param_group['lr'] = learning_rate * scale"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def log_stdout(logger, subset, epoch_iters, total_steps, loss, took):\n",
    "    logger_data = [\n",
    "        ('Loss/Total', loss),\n",
    "    ]\n",
    "    logger_data.append(('Time/Iter time', took))\n",
    "    logger.log(epoch_iters,\n",
    "               tb_total_steps=total_steps,\n",
    "               subset=subset,\n",
    "               data=OrderedDict(logger_data)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def maybe_save_checkpoint(args, model, optimizer, epoch,\n",
    "                          total_iter, config):\n",
    "    if args.local_rank != 0:\n",
    "        return\n",
    "\n",
    "    intermediate = (args.epochs_per_checkpoint > 0\n",
    "                    and epoch % args.epochs_per_checkpoint == 0)\n",
    "\n",
    "    if not intermediate and epoch < args.epochs:\n",
    "        return\n",
    "\n",
    "    fpath = os.path.join(args.chkpt_save_dir, f\"respeller_checkpoint_{epoch}.pt\")\n",
    "    print(f\"Saving model and optimizer state at epoch {epoch} to {fpath}\")\n",
    "    checkpoint = {'epoch': epoch,\n",
    "                  'iteration': total_iter,\n",
    "                  'config': config,\n",
    "                  'state_dict': model.state_dict(),\n",
    "                  'optimizer': optimizer.state_dict()}\n",
    "    torch.save(checkpoint, fpath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## pre-training loop stuff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def train(rank, args):\n",
    "\n",
    "\n",
    "# handle GPU\n",
    "rank = 0\n",
    "args.local_rank = rank\n",
    "device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "\n",
    "# load models\n",
    "tts, n_symbols, grapheme_embedding_dim, model_config = load_pretrained_fastpitch(args)\n",
    "respeller = EncoderRespeller(n_symbols=n_symbols, pretrained_tts=tts, d_model=args.d_model)\n",
    "# quantiser = GumbelVectorQuantizer(\n",
    "#     in_dim=args.d_model,\n",
    "#     codebook_size=n_symbols,  # number of codebook entries\n",
    "#     embedding_dim=grapheme_embedding_dim,\n",
    "#     temp=args.latent_temp,\n",
    "# )\n",
    "# init_embedding_weights(tts.encoder.word_emb.weight.unsqueeze(0), quantiser.vars)\n",
    "criterion = SoftDTW(use_cuda=True, gamma=0.1) # input should be size [bsz, seqlen, dim]\n",
    "\n",
    "tts.to(device)\n",
    "respeller.to(device)\n",
    "# quantiser.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "# load optimiser and assign to it the weights to be trained\n",
    "kw = dict(lr=args.learning_rate, betas=(0.9, 0.98), eps=1e-9,\n",
    "          weight_decay=args.weight_decay)\n",
    "optimizer = Lamb(respeller.trainable_parameters(), **kw)\n",
    "\n",
    "# (optional) load checkpoint for respeller\n",
    "start_epoch = [1]\n",
    "start_iter = [0]\n",
    "assert args.checkpoint_path is None or args.resume is False, (\n",
    "    \"Specify a single checkpoint source\")\n",
    "if args.checkpoint_path is not None:\n",
    "    ch_fpath = args.checkpoint_path\n",
    "elif args.resume:\n",
    "    ch_fpath = last_checkpoint(args.chkpt_save_dir)\n",
    "else:\n",
    "    ch_fpath = None\n",
    "if ch_fpath is not None:\n",
    "    load_respeller_checkpoint(args, respeller, ch_fpath, optimizer, start_epoch, start_iter)\n",
    "    \n",
    "start_epoch = start_epoch[0]\n",
    "total_iter = start_iter[0]\n",
    "    \n",
    "# create datasets, collate func, dataloader\n",
    "train_dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels', # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist='/home/s1785140/data/ljspeech_fastpitch/respeller_train_words.json',\n",
    "    max_examples_per_wordtype=2,\n",
    ")\n",
    "val_dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels', # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist='/home/s1785140/data/ljspeech_fastpitch/respeller_dev_words.json',\n",
    ")\n",
    "num_cpus = 1 # TODO change to CLA?\n",
    "train_loader = DataLoader(train_dataset, num_workers=2*num_cpus, shuffle=True,\n",
    "                          sampler=None, batch_size=args.batch_size,\n",
    "                          pin_memory=False, drop_last=True,\n",
    "                          collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, num_workers=2*num_cpus, shuffle=False,\n",
    "                          sampler=None, batch_size=args.batch_size,\n",
    "                          pin_memory=False, collate_fn=collate_fn)\n",
    "\n",
    "# load pretrained hifigan\n",
    "\n",
    "# log spectrograms and generated audio for first few validation wordtypes\n",
    "\n",
    "# train loop\n",
    "respeller.train()\n",
    "# quantiser.train()\n",
    "tts.eval()\n",
    "\n",
    "print('Finished setting up models + dataloaders')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## validate() fn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import PIL\n",
    "import plotly\n",
    "\n",
    "def log_spectrogram(log_mel, figsize=(15,5), image_name=\"\"):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    img = librosa.display.specshow(log_mel, ax=ax, x_axis='frames', y_axis='linear')\n",
    "    ax.set_title(image_name)\n",
    "    fig.colorbar(img, ax=ax)\n",
    "    return fig\n",
    "\n",
    "def get_spectrograms_plots(y, fnames, step, n=4, label='Predicted spectrogram', mas=False):\n",
    "    \"\"\"Plot spectrograms for n utterances in batch\"\"\"\n",
    "    bs = len(fnames)\n",
    "    n = min(n, bs)\n",
    "    s = bs // n\n",
    "    fnames = fnames[::s]\n",
    "    # print(f\"inside get_spectrograms_plots(), {fnames=}\")\n",
    "    if label == 'Predicted spectrogram':\n",
    "        # y: mel_out, dec_mask, dur_pred, log_dur_pred, pitch_pred\n",
    "        mel_specs = y[0][::s].transpose(1, 2).cpu().numpy()\n",
    "        mel_lens = y[1][::s].squeeze().cpu().numpy() - 1\n",
    "    elif label == 'Reference spectrogram':\n",
    "        # y: mel_padded, dur_padded, dur_lens, pitch_padded\n",
    "        mel_specs = y[0][::s].cpu().numpy()\n",
    "        if mas:\n",
    "            mel_lens = y[2][::s].cpu().numpy()  # output_lengths\n",
    "        else:\n",
    "            mel_lens = y[1][::s].cpu().numpy().sum(axis=1) - 1\n",
    "            \n",
    "    image_names = []\n",
    "    spectrogram_figs = []\n",
    "    for mel_spec, mel_len, fname in zip(mel_specs, mel_lens, fnames):\n",
    "        mel_spec = mel_spec[:, :mel_len]\n",
    "        utt_id = os.path.splitext(os.path.basename(fname))[0]\n",
    "        # if mode == 'tb':\n",
    "        #     logger.log_spectrogram_tb(\n",
    "        #         step, '{}/{}'.format(label, utt_id), mel_spec, tb_subset='val')\n",
    "        # elif mode == 'wandb':\n",
    "        image_name = f'val/{label}/{utt_id}'\n",
    "        fig = log_spectrogram(mel_spec, image_name=image_name)\n",
    "        image_names.append(image_name)\n",
    "        spectrogram_figs.append(fig)\n",
    "            # wandb.log({image_name: \n",
    "            #             wandb.Image(plt, caption=image_name)})\n",
    "            # wandb.log({image_name: plt}) # requires plotly\n",
    "    return image_names, spectrogram_figs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_audio(y, fnames, step, vocoder=None, sampling_rate=22050, hop_length=256,\n",
    "                   n=4, label='Predicted audio', mas=False):\n",
    "    \"\"\"Generate audio from spectrograms for n utterances in batch\"\"\"\n",
    "    bs = len(fnames)\n",
    "    n = min(n, bs)\n",
    "    s = bs // n\n",
    "    fnames = fnames[::s]\n",
    "    # print(f\"inside generate_audio(), {fnames=}\")\n",
    "    with torch.no_grad():\n",
    "        if label == 'Predicted audio':\n",
    "            # y: mel_out, dec_mask, dur_pred, log_dur_pred, pitch_pred\n",
    "            audios = vocoder(y[0][::s].transpose(1, 2)).cpu().squeeze().numpy()\n",
    "            mel_lens = y[1][::s].squeeze().cpu().numpy() - 1\n",
    "        elif label == 'Copy synthesis':\n",
    "            # y: mel_padded, dur_padded, dur_lens, pitch_padded\n",
    "            audios = vocoder(y[0][::s]).cpu().squeeze().numpy()\n",
    "            if mas:\n",
    "                mel_lens = y[2][::s].cpu().numpy()  # output_lengths\n",
    "            else:\n",
    "                mel_lens = y[1][::s].cpu().numpy().sum(axis=1) - 1\n",
    "        elif label == 'Reference audio':\n",
    "            audios = []\n",
    "            for fname in fnames:\n",
    "                wav = re.sub(r'mels/(.+)\\.pt', r'wavs/\\1.wav', fname)\n",
    "                audio, _ = librosa.load(wav, sr=sampling_rate)\n",
    "                audios.append(audio)\n",
    "            if mas:\n",
    "                mel_lens = y[2][::s].cpu().numpy()  # output_lengths\n",
    "            else:\n",
    "                mel_lens = y[1][::s].cpu().numpy().sum(axis=1) - 1\n",
    "    audios_to_return = []\n",
    "    for audio, mel_len, fname in zip(audios, mel_lens, fnames):\n",
    "        audio = audio[:mel_len * hop_length]\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "        utt_id = os.path.splitext(os.path.basename(fname))[0]\n",
    "        # if mode == 'tb':\n",
    "        #     logger.log_audio_tb(\n",
    "        #         step, '{}/{}'.format(label, utt_id), audio, sampling_rate, tb_subset='val')\n",
    "        # elif mode == 'wandb':\n",
    "            # audio_name = f\"val/{label}/{utt_id}\"\n",
    "            # wandb.log({audio_name:\n",
    "            #    [wandb.Audio(audio, caption=audio_name, sample_rate=sampling_rate)]})\n",
    "        audios_to_return.append(audio)\n",
    "        \n",
    "    return audios_to_return\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def decode_indices(indices):\n",
    "    \"\"\"decode batch of indices to text\n",
    "    [bsz, seqlen]\"\"\"\n",
    "    decodings = []\n",
    "    for batch_idx in range(indices.size(0)):\n",
    "        decodings.append(''.join(tp.id_to_symbol[id] for id in indices[batch_idx].tolist()))\n",
    "    return decodings\n",
    "\n",
    "def select(x, bsz, n):\n",
    "    \"\"\"select items in batch that will be visualised/converted to audio\"\"\"\n",
    "    n = min(n, bsz)\n",
    "    s = bsz // n\n",
    "    return x[::s]\n",
    "\n",
    "def log_wandb_table(\n",
    "    names, \n",
    "    vocoded_gt_audios,\n",
    "    orig_words,\n",
    "    respellings,\n",
    "    orig_pred_spec_figs,\n",
    "    orig_pred_audios,\n",
    "    pred_spec_figs,\n",
    "    pred_audios,\n",
    "    sampling_rate=22050,\n",
    "    train=False,\n",
    "):  \n",
    "    # define table\n",
    "    table = wandb.Table(columns=[\n",
    "        \"names\", \n",
    "        \"orig spelling\", \n",
    "        \"orig spelling spec\", \n",
    "        \"orig spelling audio\",\n",
    "        \"vocoded gt audio\",\n",
    "        \"respelling\", \n",
    "        \"respelling spec\", \n",
    "        \"respelling audio\",\n",
    "    ])\n",
    "    # add rows to table\n",
    "    for name, orig_word, orig_pred_spec_fig, orig_pred_audio, vocoded_gt_audio, respelling, pred_spec_fig, pred_audio in zip(\n",
    "        names, \n",
    "        orig_words, \n",
    "        orig_pred_spec_figs,\n",
    "        orig_pred_audios,\n",
    "        vocoded_gt_audios,\n",
    "        respellings, \n",
    "        pred_spec_figs, \n",
    "        pred_audios,\n",
    "    ):\n",
    "        table.add_data(\n",
    "            name, \n",
    "            orig_word,\n",
    "            wandb.Image(orig_pred_spec_fig, caption=name),\n",
    "            wandb.Audio(orig_pred_audio, caption=name, sample_rate=sampling_rate),\n",
    "            wandb.Audio(vocoded_gt_audio, caption=name, sample_rate=sampling_rate),\n",
    "            respelling,\n",
    "            wandb.Image(pred_spec_fig, caption=name),\n",
    "            wandb.Audio(pred_audio, caption=name, sample_rate=sampling_rate),\n",
    "        )\n",
    "        \n",
    "    if train:\n",
    "        wandb.log({\"train_table\": table})\n",
    "    else:\n",
    "        wandb.log({\"val_table\": table})\n",
    "    \n",
    "    # close figures to save memory\n",
    "    for fig in orig_pred_spec_figs + pred_spec_figs:\n",
    "        plt.close(fig)\n",
    "          \n",
    "def validate(\n",
    "    respeller_model, \n",
    "    tts_model, \n",
    "    vocoder,\n",
    "    criterion,\n",
    "    valset, \n",
    "    epoch, \n",
    "    batch_size, \n",
    "    collate_fn, \n",
    "    sampling_rate,\n",
    "    hop_length,\n",
    "    audio_interval=5,\n",
    "    n=None, # how many tokens to plot and generate audio for, if None then do the whole first batch\n",
    "    only_log_table=False,\n",
    "    train=False,\n",
    "):\n",
    "    \"\"\"Handles all the validation scoring and printing\n",
    "    GT (beginning of training):\n",
    "    - log GT mel spec and vocoded audio for several validation set words\n",
    "    \n",
    "    Model outputs:\n",
    "    - log predicted mel spec and vocoded audio from fastpitch\n",
    "    - log respelled word from respeller\n",
    "    \"\"\"\n",
    "    was_training = respeller_model.training\n",
    "    respeller_model.eval()\n",
    "    \n",
    "    tik = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        val_loader = DataLoader(valset, num_workers=4, shuffle=False,\n",
    "                                sampler=None,\n",
    "                                batch_size=batch_size, pin_memory=False,\n",
    "                                collate_fn=collate_fn)\n",
    "        val_meta = defaultdict(float)\n",
    "        val_losses = 0.0\n",
    "        epoch_iter = 0\n",
    "        \n",
    "        for i, batch in enumerate(val_loader):\n",
    "            epoch_iter += 1\n",
    "            \n",
    "            # get loss over batch\n",
    "            x, y = batch_to_gpu(batch)\n",
    "            pred_mel, dec_lens, g_embedding_indices = forward_pass(respeller_model, tts_model, x)\n",
    "            iter_loss = (criterion(pred_mel, y[\"mel_padded\"]) / dec_lens).mean().item()\n",
    "            val_losses += iter_loss\n",
    "    \n",
    "            # log spectrograms and generated audio for first few utterances\n",
    "            log_table = (epoch % audio_interval == 0 if epoch is not None else True)\n",
    "            if (i == 0) and log_table:\n",
    "                fnames = batch['mel_filepaths']\n",
    "                bsz = len(fnames)\n",
    "                if n is None:\n",
    "                    n = bsz\n",
    "                \n",
    "                # get original word and respellings for logging\n",
    "                original_words = decode_indices(x['text_padded'])\n",
    "                respellings = decode_indices(g_embedding_indices)\n",
    "                \n",
    "                # vocode original recorded speech\n",
    "                gt_mel = y['mel_padded']\n",
    "                gt_mel_lens = y['mel_lengths']\n",
    "                vocoded_gt = generate_audio((gt_mel, gt_mel_lens), fnames, total_iter, vocoder, sampling_rate, hop_length, n=n, label='Predicted audio', mas=True)\n",
    "                \n",
    "                # get melspec + generated audio for original spellings\n",
    "                orig_pred_mel, orig_dec_lens, _dur_pred, _pitch_pred = tts(\n",
    "                    inputs=x['text_padded'],\n",
    "                    skip_embeddings=False,\n",
    "                )\n",
    "                orig_pred_mel = orig_pred_mel.transpose(1,2)\n",
    "                _orig_token_names, orig_pred_spec_figs = get_spectrograms_plots((orig_pred_mel, orig_dec_lens), fnames, total_iter, n=n, label='Predicted spectrogram', mas=True)\n",
    "                orig_pred_audios = generate_audio((orig_pred_mel, orig_dec_lens), fnames, total_iter, vocoder, sampling_rate, hop_length, n=n, label='Predicted audio', mas=True)\n",
    "            \n",
    "                # get melspec + generated audio for respellings\n",
    "                token_names, pred_spec_figs = get_spectrograms_plots((pred_mel, dec_lens), fnames, total_iter, n=n, label='Predicted spectrogram', mas=True)\n",
    "                pred_audios = generate_audio((pred_mel, dec_lens), fnames, total_iter, vocoder, sampling_rate, hop_length, n=n, label='Predicted audio', mas=True)\n",
    "                \n",
    "                # log everything to wandb table\n",
    "                token_names = [n.split('/')[-1] for n in token_names]\n",
    "                log_wandb_table(\n",
    "                    names=token_names,\n",
    "                    vocoded_gt_audios=vocoded_gt,\n",
    "                    orig_words=select(original_words, bsz, n=n),\n",
    "                    orig_pred_spec_figs=orig_pred_spec_figs,\n",
    "                    orig_pred_audios=orig_pred_audios,\n",
    "                    respellings=select(respellings, bsz, n=n),\n",
    "                    pred_spec_figs=pred_spec_figs,\n",
    "                    pred_audios=pred_audios,\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    train=train,\n",
    "                )\n",
    "                \n",
    "            if log_table and only_log_table:\n",
    "                break # leave for loop after first iteration\n",
    "        \n",
    "        if not only_log_table:\n",
    "            wandb.log({'val/epoch_loss': val_losses/epoch_iter})\n",
    "    \n",
    "    if was_training:\n",
    "        respeller_model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def forward_pass(respeller, tts, x):\n",
    "    \"\"\"x: inputs\n",
    "    x = {\n",
    "        'words': words,\n",
    "        'text_padded': text_padded,\n",
    "        'text_lengths': text_lengths,\n",
    "    }\"\"\"\n",
    "    g_embeddings, g_embedding_indices = respeller(x['text_padded'])\n",
    "    \n",
    "    # quantiser_outdict = quantiser(logits, produce_targets=True)\n",
    "    # g_embedding_indices = quantiser_outdict[\"targets\"].squeeze(2)\n",
    "    # g_embeddings = quantiser_outdict[\"x\"]\n",
    "\n",
    "    log_mel, dec_lens, _dur_pred, _pitch_pred = tts(\n",
    "        inputs=g_embeddings,\n",
    "        ids=g_embedding_indices,\n",
    "        skip_embeddings=True,\n",
    "    )\n",
    "    \n",
    "    # log_mel: [bsz, dim, seqlen]\n",
    "    log_mel = log_mel.transpose(1,2)\n",
    "    # log_mel: [bsz, seqlen, dim]\n",
    "    \n",
    "    # return mask for masking acoustic loss\n",
    "    # padding_idx = 0\n",
    "    # mask = (g_embedding_indices != padding_idx).unsqueeze(2)\n",
    "    # mask.size()\n",
    "    # dec_mask = mask_from_lens(dec_lens).unsqueeze(2)\n",
    "    \n",
    "    return log_mel, dec_lens, g_embedding_indices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, args.epochs + 1):\n",
    "    # logging metrics\n",
    "    epoch_start_time = time.perf_counter()\n",
    "    iter_loss = 0\n",
    "    epoch_loss = 0.0\n",
    "    epoch_iter = 0\n",
    "    num_iters = len(train_loader)\n",
    "    # epoch_mel_loss = 0.0\n",
    "    # epoch_num_frames = 0\n",
    "    # epoch_frames_per_sec = 0.0\n",
    "    # iter_num_frames = 0\n",
    "    # iter_meta = {}\n",
    "\n",
    "    # iterate over all batches in epoch\n",
    "    for batch in train_loader:        \n",
    "        # if epoch_iter == 100:\n",
    "        #     break # NB quit training loop, FOR DEVELOPMENT!!!\n",
    "        \n",
    "        if epoch_iter == num_iters: # useful for gradient accumulation\n",
    "            break\n",
    "                    \n",
    "        total_iter += 1\n",
    "        epoch_iter += 1\n",
    "        iter_start_time = time.perf_counter()\n",
    "\n",
    "        adjust_learning_rate(total_iter, optimizer, args.learning_rate,\n",
    "                             args.warmup_steps)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x, y = batch_to_gpu(batch) # x: inputs, y: targets\n",
    "        gt_mel = y[\"mel_padded\"]\n",
    "        \n",
    "        # # y: targets\n",
    "        # y = {\n",
    "        #     'mel_padded': mel_padded, \n",
    "        #     'mel_lengths': mel_lengths,\n",
    "        # }\n",
    "        \n",
    "        # forward pass through models (respeller -> quantiser -> tts)\n",
    "        pred_mel, dec_lens, _g_embedding_indices = forward_pass(respeller, tts, x)\n",
    "        \n",
    "        # TODO: DO WE NEED MASK IF WE USE SOFTDTW LOSS? \n",
    "        # I THINK IT AUTOMATICALLY WILL ALIGN PADDED FRAMES WITH EACH OTHER???\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(pred_mel, gt_mel)\n",
    "        # print('raw loss from softdtw', loss)\n",
    "        \n",
    "        loss = loss / dec_lens\n",
    "        # print('loss avg according to dec seqlens', loss)\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        # print('loss avged across batch', loss)\n",
    "        \n",
    "        # backpropagation of loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # clip gradients and run optimizer\n",
    "        torch.nn.utils.clip_grad_norm_(respeller.trainable_parameters(), args.grad_clip_thresh)\n",
    "        optimizer.step()\n",
    "        # logger.log_grads_tb(total_iter, model)\n",
    "        \n",
    "        # log metrics to terminal and to wandb\n",
    "        iter_loss = loss.item()\n",
    "        iter_time = time.perf_counter() - iter_start_time\n",
    "        epoch_loss += iter_loss\n",
    "        \n",
    "        # NB commented out to avoid crashing jupyter notebook\n",
    "        # log_stdout(\n",
    "        #     logger,\n",
    "        #     'train',\n",
    "        #     (epoch, epoch_iter, num_iters),\n",
    "        #     total_iter,\n",
    "        #     iter_loss,\n",
    "        #     iter_time,\n",
    "        # )\n",
    "        \n",
    "        wandb.log({\n",
    "            \"train/iter_loss\": iter_loss,\n",
    "            \"train/iter_time\": iter_time,\n",
    "        })\n",
    "        ### Finished Epoch!\n",
    "             \n",
    "    epoch_time = time.perf_counter() - epoch_start_time\n",
    "    \n",
    "    wandb.log({\n",
    "        \"train/epoch_num\": epoch,\n",
    "        \"train/epoch_time\": epoch_time,\n",
    "        \"train/epoch_loss\": epoch_loss / epoch_iter,\n",
    "    })\n",
    "    \n",
    "    # log audio and respellings for training set words\n",
    "    validate(\n",
    "        respeller_model=respeller, \n",
    "        tts_model=tts, \n",
    "        vocoder=vocoder,\n",
    "        criterion=criterion,\n",
    "        valset=train_dataset, \n",
    "        batch_size=args.batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        epoch=epoch,\n",
    "        sampling_rate=args.sampling_rate,\n",
    "        hop_length=args.hop_length,\n",
    "        audio_interval=args.val_log_interval,\n",
    "        only_log_table=True,\n",
    "        train=True,\n",
    "    )\n",
    "        \n",
    "    # log audio and respellings for val set words\n",
    "    validate(\n",
    "        respeller_model=respeller, \n",
    "        tts_model=tts, \n",
    "        vocoder=vocoder,\n",
    "        criterion=criterion,\n",
    "        valset=val_dataset, \n",
    "        batch_size=args.batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        epoch=epoch,\n",
    "        sampling_rate=args.sampling_rate,\n",
    "        hop_length=args.hop_length,\n",
    "        audio_interval=args.val_log_interval,\n",
    "    )\n",
    "\n",
    "    maybe_save_checkpoint(args, respeller, optimizer, \n",
    "                          epoch, total_iter, model_config)\n",
    "\n",
    "    logger.flush()\n",
    "        \n",
    "print(\"\\n *** Finished training! ***\")\n",
    "\n",
    "# wandb.finish() # useful in jupyter notebooks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b565bf9-5cf9-4aa0-941b-74fd3cc19c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}