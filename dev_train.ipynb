{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b9f5a3c3-31af-4835-88ee-4410bc5af326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2de0d84a-9335-48be-8bfb-ba9cbbf0dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53034b89-b847-4927-a604-cc6cb78516c2",
   "metadata": {},
   "source": [
    "# dev train loop + model for respeller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72cec08-fc1a-459b-87d4-49f1219cc86d",
   "metadata": {},
   "source": [
    "# command line args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# imitate CLAs\n",
    "import sys\n",
    "sys.argv = [\n",
    "    'train.py',\n",
    "    '--fastpitch-chkpt', 'fastpitch/exps/halved_ljspeech_data/FastPitch_checkpoint_1000.pt',\n",
    "    '--input-type', 'char',\n",
    "    '--symbol-set', 'english_basic_lowercase_no_arpabet',\n",
    "    '--use-mas',\n",
    "    '--cuda',\n",
    "    '--n-speakers', '1',\n",
    "    '--use-sepconv',\n",
    "    '--add-spaces',\n",
    "    '--eos-symbol', '$',\n",
    "    '--epochs', '5', # NB for development!\n",
    "    '--chkpt-save-dir', '/home/s1785140/respeller/exps/test', \n",
    "    # '--resume', # resume from latest checkpoint\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Train respeller model\n",
    "\n",
    "We backpropagate loss from pretrained TTS model to a Grapheme-to-Grapheme (G2G) respeller model to help it respell words\n",
    "into a simpler form\n",
    "\n",
    "Intermediated respellings are discrete character sequences\n",
    "We can backpropagate through these using gumbel softmax and the straight through estimator\n",
    "'''\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "from fastpitch import models as fastpitch_model\n",
    "from fastpitch.common.text.text_processing import TextProcessor\n",
    "\n",
    "from modules.model import EncoderRespeller\n",
    "from modules.gumbel_vector_quantizer import GumbelVectorQuantizer\n",
    "from modules.sdtw_cuda_loss import SoftDTW\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## arguments to parse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_args(parser):\n",
    "    \"\"\"Parse commandline arguments\"\"\"\n",
    "    parser.add_argument('-o', '--chkpt-save-dir', type=str, required=True,\n",
    "                        help='Directory to save checkpoints')\n",
    "    parser.add_argument('-d', '--dataset-path', type=str, default='./',\n",
    "                        help='Path to dataset')\n",
    "    parser.add_argument('--log-file', type=str, default=None,\n",
    "                        help='Path to a DLLogger log file')\n",
    "\n",
    "    train = parser.add_argument_group('training setup')\n",
    "    train.add_argument('--cuda', action='store_true',\n",
    "                      help='Enable GPU training')\n",
    "    train.add_argument('--batch-size', type=int, default=16,\n",
    "                      help='Batchsize (this is divided by number of GPUs if running Data Distributed Parallel Training)')\n",
    "    train.add_argument('--seed', type=int, default=1337,\n",
    "                       help='Seed for PyTorch random number generators')\n",
    "    train.add_argument('--grad-accumulation', type=int, default=1,\n",
    "                       help='Training steps to accumulate gradients for')\n",
    "    train.add_argument('--epochs', type=int, default=100, #required=True,\n",
    "                       help='Number of total epochs to run')\n",
    "    train.add_argument('--epochs-per-checkpoint', type=int, default=10,\n",
    "                       help='Number of epochs per checkpoint')\n",
    "    train.add_argument('--checkpoint-path', type=str, default=None,\n",
    "                       help='Checkpoint path to resume train')\n",
    "    train.add_argument('--resume', action='store_true',\n",
    "                       help='Resume train from the last available checkpoint')\n",
    "    \n",
    "    opt = parser.add_argument_group('optimization setup')\n",
    "    opt.add_argument('--optimizer', type=str, default='lamb', choices=['adam', 'lamb'],\n",
    "                     help='Optimization algorithm')\n",
    "    opt.add_argument('-lr', '--learning-rate', default=0.1, type=float,\n",
    "                     help='Learning rate')\n",
    "    opt.add_argument('--weight-decay', default=1e-6, type=float,\n",
    "                     help='Weight decay')\n",
    "    opt.add_argument('--grad-clip-thresh', default=1000.0, type=float,\n",
    "                     help='Clip threshold for gradients')\n",
    "    opt.add_argument('--warmup-steps', type=int, default=1000,\n",
    "                     help='Number of steps for lr warmup')\n",
    "\n",
    "    arch = parser.add_argument_group('architecture')\n",
    "    arch.add_argument('--d-model', type=int, default=512,\n",
    "                       help='Hidden dimension of tranformer')\n",
    "    arch.add_argument('--latent-temp', type=tuple, default=(2, 0.5, 0.999995),\n",
    "                       help='Temperature annealling parameters for Gumbel-Softmax (start, end, decay)')\n",
    "\n",
    "    pretrained_tts = parser.add_argument_group('pretrained tts model')\n",
    "    # pretrained_tts.add_argument('--fastpitch-with-mas', type=bool, default=True,\n",
    "    #                   help='Whether or not fastpitch was trained with Monotonic Alignment Search (MAS)')\n",
    "    pretrained_tts.add_argument('--fastpitch-chkpt', type=str, required=True,\n",
    "                      help='Path to pretrained fastpitch checkpoint')\n",
    "    pretrained_tts.add_argument('--input-type', type=str, default='char',\n",
    "                      choices=['char', 'phone', 'pf', 'unit'],\n",
    "                      help='Input symbols used, either char (text), phone, pf '\n",
    "                      '(phonological feature vectors) or unit (quantized acoustic '\n",
    "                      'representation IDs)')\n",
    "    pretrained_tts.add_argument('--symbol-set', type=str, default='english_basic_lowercase',\n",
    "                      help='Define symbol set for input sequences. For quantized '\n",
    "                      'unit inputs, pass the size of the vocabulary.')\n",
    "    pretrained_tts.add_argument('--n-speakers', type=int, default=1,\n",
    "                      help='Condition on speaker, value > 1 enables trainable '\n",
    "                      'speaker embeddings.')\n",
    "    # pretrained_tts.add_argument('--use-sepconv', type=bool, default=True,\n",
    "    #                   help='Use depthwise separable convolutions')\n",
    "    \n",
    "    audio = parser.add_argument_group('log generated audio')\n",
    "    audio.add_argument('--hifigan', type=str, default='/home/s1785140/pretrained_models/hifigan/ljspeech/LJ_V1/generator_v1',\n",
    "                       help='Path to HiFi-GAN audio checkpoint')\n",
    "    audio.add_argument('--hifigan-config', type=str, default='/home/s1785140/pretrained_models/hifigan/ljspeech/LJ_V1/config.json',\n",
    "                       help='Path to HiFi-GAN audio config file')\n",
    "    audio.add_argument('--sampling-rate', type=int, default=22050,\n",
    "                       help='Sampling rate for output audio')\n",
    "    audio.add_argument('--hop-length', type=int, default=256,\n",
    "                       help='STFT hop length for estimating audio length from mel size')\n",
    "    \n",
    "    data = parser.add_argument_group('dataset parameters')\n",
    "    cond = parser.add_argument_group('conditioning on additional attributes')\n",
    "    dist = parser.add_argument_group('distributed training setup')\n",
    "\n",
    "    return parser\n",
    "\n",
    "def load_checkpoint(args, model, filepath):\n",
    "    if args.local_rank == 0:\n",
    "        print(f'Loading model and optimizer state from {filepath}')\n",
    "    checkpoint = torch.load(filepath, map_location='cpu')\n",
    "    sd = {k.replace('module.', ''): v\n",
    "          for k, v in checkpoint['state_dict'].items()}\n",
    "    getattr(model, 'module', model).load_state_dict(sd)\n",
    "    return model\n",
    "\n",
    "def load_respeller_checkpoint(args, model, filepath, optimizer, epoch, total_iter):\n",
    "    if args.local_rank == 0:\n",
    "        print(f'Loading model and optimizer state from {filepath}')\n",
    "    checkpoint = torch.load(filepath, map_location='cpu')\n",
    "    epoch[0] = checkpoint['epoch'] + 1\n",
    "    total_iter[0] = checkpoint['iteration']\n",
    "    sd = {k.replace('module.', ''): v\n",
    "          for k, v in checkpoint['state_dict'].items()}\n",
    "    getattr(model, 'module', model).load_state_dict(sd)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model\n",
    "\n",
    "def last_checkpoint(output):\n",
    "    saved = sorted(\n",
    "        glob.glob(f'{output}/respeller_checkpoint_*.pt'),\n",
    "        key=lambda f: int(re.search('_(\\d+).pt', f).group(1)))\n",
    "\n",
    "    def corrupted(fpath):\n",
    "        try:\n",
    "            torch.load(fpath, map_location='cpu')\n",
    "            return False\n",
    "        except:\n",
    "            warnings.warn(f'Cannot load {fpath}')\n",
    "            return True\n",
    "\n",
    "    if len(saved) >= 1 and not corrupted(saved[-1]):\n",
    "        return saved[-1]\n",
    "    elif len(saved) >= 2:\n",
    "        return saved[-2]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def init_embedding_weights(source_tensor, target_tensor):\n",
    "    \"\"\"copy weights inplace from source tensor to target tensor\"\"\"\n",
    "    target_tensor.requires_grad = False\n",
    "    target_tensor.copy_(source_tensor.clone().detach())\n",
    "    target_tensor.requires_grad = True\n",
    "\n",
    "def load_pretrained_fastpitch(args):\n",
    "    # load chkpt\n",
    "    device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "    model_config = fastpitch_model.get_model_config('FastPitch', args)\n",
    "    fastpitch = fastpitch_model.get_model('FastPitch', model_config, device, forward_is_infer=True)\n",
    "    load_checkpoint(args, fastpitch, args.fastpitch_chkpt)\n",
    "    # get information about grapheme embedding table\n",
    "    n_symbols = fastpitch.encoder.word_emb.weight.size(0)\n",
    "    grapheme_embedding_dim = fastpitch.encoder.word_emb.weight.size(1)\n",
    "    return fastpitch, n_symbols, grapheme_embedding_dim, model_config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# beginning of main(), parse Command Line Args"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Respeller Training', allow_abbrev=False)\n",
    "parser = parse_args(parser)\n",
    "args, _unk_args = parser.parse_known_args()\n",
    "\n",
    "parser = fastpitch_model.parse_model_args('FastPitch', parser)\n",
    "args, unk_args = parser.parse_known_args()\n",
    "if len(unk_args) > 0:\n",
    "    raise ValueError(f'Invalid options {unk_args}')\n",
    "\n",
    "if args.cuda:\n",
    "    args.num_gpus = torch.cuda.device_count()\n",
    "    args.distributed_run = args.num_gpus > 1\n",
    "    args.batch_size = int(args.batch_size / args.num_gpus)\n",
    "else:\n",
    "    args.distributed_run = False\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "if args.distributed_run:\n",
    "    mp.spawn(train, nprocs=args.num_gpus, args=(args,))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## WANDB - weights and biases init"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login() # needed for wandb integration with jupyter notebook"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%env \"WANDB_NOTEBOOK_NAME\" \"respeller-dev-train-ipynb\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = 0\n",
    "\n",
    "# store important information into WANDB config for easier tracking of experiments\n",
    "# add all key values from parser\n",
    "wandb_config = vars(args)\n",
    "wandb.init(\n",
    "    project=\"respeller-dev-train-ipynb\",\n",
    "    name=f\"experiment_{run}\",\n",
    "    config=wandb_config,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 'train()' - forward pass through model to get loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## create / load models "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rank = 0\n",
    "device = 'cuda'\n",
    "\n",
    "args.local_rank = rank\n",
    "tts, n_symbols, grapheme_embedding_dim, model_config = load_pretrained_fastpitch(args)\n",
    "tts.to(device)\n",
    "\n",
    "respeller = EncoderRespeller(n_symbols=n_symbols, pretrained_tts=tts, d_model=args.d_model)\n",
    "respeller.to(device)\n",
    "\n",
    "# quantiser = GumbelVectorQuantizer(\n",
    "#     in_dim=args.d_model,\n",
    "#     codebook_size=n_symbols,  # number of codebook entries\n",
    "#     embedding_dim=grapheme_embedding_dim,\n",
    "#     temp=args.latent_temp,\n",
    "# )\n",
    "# quantiser.to(device)\n",
    "\n",
    "# init_embedding_weights(tts.encoder.word_emb.weight.unsqueeze(0), quantiser.vars)\n",
    "\n",
    "\n",
    "# batch_size, len_x, len_y, dims = 8, 15, 12, 5\n",
    "# x = torch.rand((batch_size, len_x, dims), requires_grad=True)\n",
    "# y = torch.rand((batch_size, len_y, dims))\n",
    "\n",
    "# criterion = SoftDTW(use_cuda=True, gamma=0.1, dist_func=F.mse_loss)\n",
    "criterion = SoftDTW(use_cuda=True, gamma=0.1)\n",
    "# input should be size [bsz, seqlen, dim]\n",
    "criterion.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### load HiFiGAN vocoder "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_vocoder(args, device):\n",
    "    \"\"\"Load HiFi-GAN vocoder from checkpoint\"\"\"\n",
    "    checkpoint_data = torch.load(args.hifigan)\n",
    "    vocoder_config = fastpitch_model.get_model_config('HiFi-GAN', args)\n",
    "    vocoder = fastpitch_model.get_model('HiFi-GAN', vocoder_config, device)\n",
    "    vocoder.load_state_dict(checkpoint_data['generator'])\n",
    "    vocoder.remove_weight_norm()\n",
    "    vocoder.eval()\n",
    "    return vocoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocoder = load_vocoder(args, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## forward pass through model with dummy data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batches = []\n",
    "symbol_set = 'english_basic_lowercase'\n",
    "text_cleaners = []\n",
    "gt_log_mel = torch.load('/home/s1785140/data/ljspeech_fastpitch/mels/LJ001-0001.pt').cuda().unsqueeze(0).transpose(1,2) # intro batch dimension + [bsz, seqlen, dim]\n",
    "raw_text = 'printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the exhibition'\n",
    "\n",
    "# process text using same processor as fastpitch\n",
    "tp = TextProcessor(symbol_set, text_cleaners)\n",
    "text = torch.LongTensor(tp.encode_text(raw_text)).unsqueeze(0).cuda()\n",
    "\n",
    "batches.append((text, gt_log_mel))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text.device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for batch in batches:\n",
    "batch = batches[0]\n",
    "    \n",
    "###############################################################################################################\n",
    "# text, ssl_reps, e2e_asr_predictions, gt_log_mel = batch\n",
    "text, gt_log_mel = batch\n",
    "\n",
    "###############################################################################################################\n",
    "# create inputs\n",
    "# if args.use_acoustic_input:\n",
    "#     inputs = inputs.concat(ssl_reps)\n",
    "\n",
    "###############################################################################################################\n",
    "# forward pass\n",
    "g_embeddings, g_embedding_indices = respeller(text[:13])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_symbols"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g_embedding_indices.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g_embeddings.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "padding_idx = 0\n",
    "mask = (g_embedding_indices != padding_idx).unsqueeze(2)\n",
    "mask.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_mel, dec_lens, _dur_pred, _pitch_pred = tts(g_embeddings, skip_embeddings=True, ids=g_embedding_indices)\n",
    "# log_mel [bsz, dim, seqlen]\n",
    "log_mel = log_mel.transpose(1,2)\n",
    "# log_mel [bsz, seqlen, dim]\n",
    "\n",
    "print(f'{log_mel.size()=}')\n",
    "print(f'{gt_log_mel.size()=}')\n",
    "\n",
    "###############################################################################################################\n",
    "# calculate val_losses\n",
    "# respelling_loss = respelling_loss_fn(respelling, e2e_asr_predictions)\n",
    "acoustic_loss = criterion(log_mel, gt_log_mel)\n",
    "\n",
    "# average loss over frames \n",
    "acoustic_loss = acoustic_loss / dec_lens\n",
    "# mel_loss = (mel_loss * mel_mask).sum() / mel_mask.sum()\n",
    "\n",
    "###############################################################################################################\n",
    "# backward pass\n",
    "loss = acoustic_loss \n",
    "\n",
    "print(f'{loss=}')\n",
    "\n",
    "# loss.backward()\n",
    "\n",
    "###############################################################################################################\n",
    "# log tensorboard metrics\n",
    "\n",
    "###############################################################################################################\n",
    "# validation set evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_spectrogram(log_mel, figsize=(15,5), wandb_log=False, image_name=\"\"):\n",
    "    plt.figure(figsize=figsize)\n",
    "    librosa.display.specshow(log_mel, x_axis='frames', y_axis='linear')\n",
    "    plt.colorbar()\n",
    "    if wandb_log:\n",
    "        wandb.log({image_name: wandb.Image(plt, caption=image_name)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_index = 0\n",
    "print(f'{log_mel[batch_index].transpose(0,1).size()=}')\n",
    "plot_spectrogram(log_mel[batch_index].transpose(0,1).detach().cpu().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # play audio\n",
    "# import IPython.display as ipd\n",
    "# audio = vocoder(log_mel[batch_index].transpose(0,1).detach().unsqueeze(0))\n",
    "# ipd.Audio(audio, rate=22050)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_index = 0\n",
    "plot_spectrogram(gt_log_mel[batch_index].transpose(0,1).detach().cpu().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# develop respeller dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wordaligned_speechreps_dir = '/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels' # path to directory that contains folders of word aligned speech reps\n",
    "wordlist = ['identifies','mash','player','russias','techniques'] # txt file for the words to include speech reps "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token_and_melfilepaths = []\n",
    "for word in wordlist:\n",
    "    # find all word aligned mels for the word\n",
    "    word_dir = os.path.join(wordaligned_speechreps_dir, word)\n",
    "    mel_files = os.listdir(word_dir)\n",
    "    for mel_file in mel_files:\n",
    "        mel_file_path = os.path.join(word_dir, mel_file)\n",
    "        token_and_melfilepaths.append((word, mel_file_path))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token_and_melfilepaths"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## process text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from fastpitch.common.text.text_processing import TextProcessor\n",
    "text_cleaners = []\n",
    "symbol_set = \"english_basic_lowercase_no_arpabet\"\n",
    "tp = TextProcessor(symbol_set, text_cleaners, add_spaces=True, eos_symbol=\"$\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoded = torch.IntTensor(tp.encode_text('identifies'))\n",
    "encoded"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "decoded = [tp.id_to_symbol[id] for id in encoded.tolist()]\n",
    "decoded"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## process mel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word, fp = token_and_melfilepaths[0]\n",
    "wordaligned_mel = torch.load(fp)\n",
    "wordaligned_mel.size() # [seqlen, feats]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 'class'-ified dataset class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from fastpitch.common.text.text_processing import TextProcessor\n",
    "\n",
    "class RespellerDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "        1) loads word + word-aligned mel spec for all words in a wordlist\n",
    "        2) converts text to sequences of one-hot vectors (corresponding to grapheme indices in fastpitch)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        wordaligned_speechreps_dir, # path to directory that contains folders of word aligned speech reps\n",
    "        wordlist, # txt file for the words to include speech reps from\n",
    "        max_examples_per_wordtype=None,\n",
    "        text_cleaners=[],\n",
    "        symbol_set=\"english_basic_lowercase_no_arpabet\",\n",
    "        add_spaces=True,\n",
    "        eos_symbol=\"$\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # load wordlist as a python list\n",
    "        if type(wordlist) == str:\n",
    "            if wordlist.endswith('.json'):\n",
    "                with open(wordlist) as f:\n",
    "                    wordlist = json.load(f)\n",
    "            else:\n",
    "                with open(wordlist) as f:\n",
    "                    wordlist = f.read().splitlines()\n",
    "        elif type(wordlist) == list:\n",
    "            pass # dont need to do anything, already in expected form\n",
    "        elif type(wordlist) == set:\n",
    "            wordlist = list(wordlist)\n",
    "        \n",
    "        wordlist = sorted(wordlist)\n",
    "        \n",
    "        # create list of all word tokens and their word aligned speech reps\n",
    "        self.word_freq = Counter()\n",
    "        self.token_and_melfilepaths = []\n",
    "        print(\"Initialising respeller dataset\")\n",
    "        for word in tqdm(wordlist):\n",
    "            # find all word aligned mels for the word\n",
    "            word_dir = os.path.join(wordaligned_speechreps_dir, word)\n",
    "            mel_files = os.listdir(word_dir)\n",
    "            if max_examples_per_wordtype:\n",
    "                mel_files = mel_files[:max_examples_per_wordtype]\n",
    "            for mel_file in mel_files:\n",
    "                mel_file_path = os.path.join(word_dir, mel_file)\n",
    "                self.token_and_melfilepaths.append((word, mel_file_path))\n",
    "                self.word_freq[word] += 1\n",
    "                \n",
    "        self.tp = TextProcessor(symbol_set, text_cleaners, add_spaces=add_spaces, eos_symbol=eos_symbol)\n",
    "\n",
    "    def get_mel(self, filename):\n",
    "        return torch.load(filename)\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        \"\"\"encode raw text into indices defined by grapheme embedding table of the TTS model\"\"\"\n",
    "        return torch.IntTensor(self.tp.encode_text(text))\n",
    "    \n",
    "    def decode_text(self, encoded):\n",
    "        return [self.tp.id_to_symbol[id] for id in encoded.tolist()]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_mel_len(melfilepath):\n",
    "        return int(melfilepath.split('seqlen')[1].split('.pt')[0])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        word, mel_filepath = self.token_and_melfilepaths[index]\n",
    "        encoded_word = self.encode_text(word)\n",
    "        mel = self.get_mel(mel_filepath)\n",
    "        \n",
    "        return {\n",
    "            'word': word, \n",
    "            'encoded_word': encoded_word, \n",
    "            'mel_filepath': mel_filepath,\n",
    "            'mel': mel,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_and_melfilepaths)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels', # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist=['identifies','mash','player','russias','techniques'],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch = []\n",
    "\n",
    "for itemdict in dataset:\n",
    "    # unpack dict\n",
    "    word = itemdict['word'] \n",
    "    encoded_word = itemdict['encoded_word'] \n",
    "    mel = itemdict['mel'] \n",
    "    \n",
    "    # check\n",
    "    print(word, encoded_word, mel.size())\n",
    "    \n",
    "    batch.append(itemdict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## collate function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Collate():\n",
    "    \"\"\" Zero-pads model inputs and targets based on number of frames per setep\n",
    "    \"\"\"\n",
    "    # def __init__(self):\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Collate's training batch from encoded word token and its \n",
    "        corresponding word-aligned mel spectrogram\n",
    "        \n",
    "        batch: [encoded_token, wordaligned_mel]\n",
    "        \"\"\"\n",
    "        # Right zero-pad all one-hot text sequences to max input length\n",
    "        input_lengths, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([len(x['encoded_word']) for x in batch]),\n",
    "            dim=0, descending=True)\n",
    "        max_input_len = input_lengths[0]\n",
    "\n",
    "        words = []\n",
    "        mel_filepaths = []\n",
    "        text_padded = torch.LongTensor(len(batch), max_input_len)\n",
    "        text_padded.zero_()\n",
    "        text_lengths = torch.LongTensor(len(batch))\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            words.append(batch[ids_sorted_decreasing[i]]['word'])\n",
    "            mel_filepaths.append(batch[ids_sorted_decreasing[i]]['mel_filepath'])\n",
    "            text = batch[ids_sorted_decreasing[i]]['encoded_word']\n",
    "            text_padded[i, :text.size(0)] = text\n",
    "            text_lengths[i] = text.size(0)\n",
    "\n",
    "        # Right zero-pad mel-spec\n",
    "        num_mels = batch[0]['mel'].size(1)\n",
    "        max_target_len = max([x['mel'].size(0) for x in batch])\n",
    "\n",
    "        mel_padded = torch.FloatTensor(len(batch), max_target_len, num_mels)\n",
    "        mel_padded.zero_()\n",
    "        mel_lengths = torch.LongTensor(len(batch))\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            mel = batch[ids_sorted_decreasing[i]]['mel']\n",
    "            mel_padded[i, :mel.size(0), :] = mel\n",
    "            mel_lengths[i] = mel.size(0)\n",
    "            \n",
    "\n",
    "        return {\n",
    "            'words': words,\n",
    "            'text_padded': text_padded,\n",
    "            'text_lengths': text_lengths,\n",
    "            'mel_padded': mel_padded, \n",
    "            'mel_lengths': mel_lengths,\n",
    "            'mel_filepaths': mel_filepaths\n",
    "        }\n",
    "                # input_lengths, mel_padded, output_lengths,\n",
    "                # len_x, dur_padded, dur_lens, pitch_padded, speaker)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collate_fn = Collate()\n",
    "collated = collate_fn(batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collated['text_padded'].size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collated['text_padded']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collated['words']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collated['mel_padded'].size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## put batch on gpu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def to_gpu(x):\n",
    "    x = x.contiguous()\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda(non_blocking=True)\n",
    "    return torch.autograd.Variable(x)\n",
    "\n",
    "def batch_to_gpu(collated_batch):\n",
    "    \"\"\"put elements that are used throughout training onto gpu\"\"\"\n",
    "    words = collated_batch['words']\n",
    "    text_padded = collated_batch['text_padded']\n",
    "    text_lengths = collated_batch['text_lengths']\n",
    "    mel_padded = collated_batch['mel_padded']\n",
    "    mel_lengths = collated_batch['mel_lengths']\n",
    "    \n",
    "    # no need to put words on gpu, its only used during eval loop\n",
    "    text_padded = to_gpu(text_padded).long()\n",
    "    text_lengths = to_gpu(text_lengths).long()\n",
    "    mel_padded = to_gpu(mel_padded).float()\n",
    "    mel_lengths = to_gpu(mel_lengths).long()\n",
    "    \n",
    "    # x: inputs\n",
    "    x = {\n",
    "        'words': words,\n",
    "        'text_padded': text_padded,\n",
    "        'text_lengths': text_lengths,\n",
    "    }\n",
    "    # y: targets\n",
    "    y = {\n",
    "        'mel_padded': mel_padded, \n",
    "        'mel_lengths': mel_lengths,\n",
    "    }\n",
    "    \n",
    "    return (x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# batch_to_gpu(collated)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# full train + dev datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels', # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist='/home/s1785140/data/ljspeech_fastpitch/respeller_train_words.json',\n",
    "    max_examples_per_wordtype=2,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum(train_dataset.word_freq.values())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels', # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist='/home/s1785140/data/ljspeech_fastpitch/respeller_dev_words.json',\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(val_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum(val_dataset.word_freq.values())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# create torch dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO, implement distributed training?\n",
    "train_sampler = None\n",
    "shuffle = True\n",
    "num_cpus = 1 \n",
    "train_loader = DataLoader(train_dataset, num_workers=2*num_cpus, shuffle=shuffle,\n",
    "                          sampler=train_sampler, batch_size=args.batch_size,\n",
    "                          pin_memory=False, drop_last=True,\n",
    "                          collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#     print(batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FULL train() loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## init dl logger"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import fastpitch.common.tb_dllogger as logger\n",
    "\n",
    "def touch_file(path):\n",
    "    if not os.path.exists(path):\n",
    "        basedir = os.path.dirname(path)\n",
    "        os.makedirs(basedir, exist_ok=True)\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(\"\")\n",
    "\n",
    "# initialise logger\n",
    "tb_subsets = ['train', 'val']\n",
    "log_fpath = args.log_file or os.path.join(args.chkpt_save_dir, 'nvlog.json')\n",
    "touch_file(log_fpath)\n",
    "\n",
    "try: \n",
    "    logger.init(log_fpath, args.chkpt_save_dir, enabled=(args.local_rank == 0),\n",
    "                tb_subsets=tb_subsets)\n",
    "    logger.parameters(vars(args), tb_subset='train')\n",
    "except:\n",
    "    print(\"WARNING DLLLoggerAlreadyInitialized error raised\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch_optimizer import Lamb\n",
    "import time\n",
    "from fastpitch.common.utils import mask_from_lens\n",
    "from collections import OrderedDict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adjust_learning_rate(total_iter, opt, learning_rate, warmup_iters=None):\n",
    "    if warmup_iters == 0:\n",
    "        scale = 1.0\n",
    "    elif total_iter > warmup_iters:\n",
    "        scale = 1. / (total_iter ** 0.5)\n",
    "    else:\n",
    "        scale = total_iter / (warmup_iters ** 1.5)\n",
    "\n",
    "    for param_group in opt.param_groups:\n",
    "        param_group['lr'] = learning_rate * scale"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def log_stdout(logger, subset, epoch_iters, total_steps, loss, took):\n",
    "    logger_data = [\n",
    "        ('Loss/Total', loss),\n",
    "    ]\n",
    "    logger_data.append(('Time/Iter time', took))\n",
    "    logger.log(epoch_iters,\n",
    "               tb_total_steps=total_steps,\n",
    "               subset=subset,\n",
    "               data=OrderedDict(logger_data)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def maybe_save_checkpoint(args, model, optimizer, epoch,\n",
    "                          total_iter, config):\n",
    "    if args.local_rank != 0:\n",
    "        return\n",
    "\n",
    "    intermediate = (args.epochs_per_checkpoint > 0\n",
    "                    and epoch % args.epochs_per_checkpoint == 0)\n",
    "\n",
    "    if not intermediate and epoch < args.epochs:\n",
    "        return\n",
    "\n",
    "    fpath = os.path.join(args.chkpt_save_dir, f\"respeller_checkpoint_{epoch}.pt\")\n",
    "    print(f\"Saving model and optimizer state at epoch {epoch} to {fpath}\")\n",
    "    checkpoint = {'epoch': epoch,\n",
    "                  'iteration': total_iter,\n",
    "                  'config': config,\n",
    "                  'state_dict': model.state_dict(),\n",
    "                  'optimizer': optimizer.state_dict()}\n",
    "    torch.save(checkpoint, fpath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## pre-training loop stuff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def train(rank, args):\n",
    "\n",
    "\n",
    "# handle GPU\n",
    "rank = 0\n",
    "args.local_rank = rank\n",
    "device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "\n",
    "# load models\n",
    "tts, n_symbols, grapheme_embedding_dim, model_config = load_pretrained_fastpitch(args)\n",
    "respeller = EncoderRespeller(n_symbols=n_symbols, pretrained_tts=tts, d_model=args.d_model)\n",
    "# quantiser = GumbelVectorQuantizer(\n",
    "#     in_dim=args.d_model,\n",
    "#     codebook_size=n_symbols,  # number of codebook entries\n",
    "#     embedding_dim=grapheme_embedding_dim,\n",
    "#     temp=args.latent_temp,\n",
    "# )\n",
    "# init_embedding_weights(tts.encoder.word_emb.weight.unsqueeze(0), quantiser.vars)\n",
    "criterion = SoftDTW(use_cuda=True, gamma=0.1) # input should be size [bsz, seqlen, dim]\n",
    "\n",
    "tts.to(device)\n",
    "respeller.to(device)\n",
    "# quantiser.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "# load optimiser and assign to it the weights to be trained\n",
    "kw = dict(lr=args.learning_rate, betas=(0.9, 0.98), eps=1e-9,\n",
    "          weight_decay=args.weight_decay)\n",
    "optimizer = Lamb(respeller.trainable_parameters(), **kw)\n",
    "\n",
    "# (optional) load checkpoint for respeller\n",
    "start_epoch = [1]\n",
    "start_iter = [0]\n",
    "assert args.checkpoint_path is None or args.resume is False, (\n",
    "    \"Specify a single checkpoint source\")\n",
    "if args.checkpoint_path is not None:\n",
    "    ch_fpath = args.checkpoint_path\n",
    "elif args.resume:\n",
    "    ch_fpath = last_checkpoint(args.chkpt_save_dir)\n",
    "else:\n",
    "    ch_fpath = None\n",
    "if ch_fpath is not None:\n",
    "    load_respeller_checkpoint(args, respeller, ch_fpath, optimizer, start_epoch, start_iter)\n",
    "    \n",
    "start_epoch = start_epoch[0]\n",
    "total_iter = start_iter[0]\n",
    "    \n",
    "# create datasets, collate func, dataloader\n",
    "train_dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels', # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist='/home/s1785140/data/ljspeech_fastpitch/respeller_train_words.json',\n",
    "    max_examples_per_wordtype=2,\n",
    ")\n",
    "val_dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels', # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist='/home/s1785140/data/ljspeech_fastpitch/respeller_dev_words.json',\n",
    ")\n",
    "num_cpus = 1 # TODO change to CLA?\n",
    "train_loader = DataLoader(train_dataset, num_workers=2*num_cpus, shuffle=True,\n",
    "                          sampler=None, batch_size=args.batch_size,\n",
    "                          pin_memory=False, drop_last=True,\n",
    "                          collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, num_workers=2*num_cpus, shuffle=False,\n",
    "                          sampler=None, batch_size=args.batch_size,\n",
    "                          pin_memory=False, collate_fn=collate_fn)\n",
    "\n",
    "# load pretrained hifigan\n",
    "\n",
    "# log spectrograms and generated audio for first few validation wordtypes\n",
    "\n",
    "# train loop\n",
    "respeller.train()\n",
    "# quantiser.train()\n",
    "tts.eval()\n",
    "\n",
    "print('Finished setting up models + dataloaders')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## validate() fn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import PIL\n",
    "import plotly\n",
    "\n",
    "def log_spectrogram(log_mel, figsize=(15,5), image_name=\"\"):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    img = librosa.display.specshow(log_mel, ax=ax, x_axis='frames', y_axis='linear')\n",
    "    ax.set_title(image_name)\n",
    "    fig.colorbar(img, ax=ax)\n",
    "    return fig\n",
    "\n",
    "def get_spectrograms_plots(y, fnames, step, n=4, label='Predicted spectrogram', mas=False):\n",
    "    \"\"\"Plot spectrograms for n utterances in batch\"\"\"\n",
    "    bs = len(fnames)\n",
    "    n = min(n, bs)\n",
    "    s = bs // n\n",
    "    fnames = fnames[::s]\n",
    "    # print(f\"inside get_spectrograms_plots(), {fnames=}\")\n",
    "    if label == 'Predicted spectrogram':\n",
    "        # y: mel_out, dec_mask, dur_pred, log_dur_pred, pitch_pred\n",
    "        mel_specs = y[0][::s].transpose(1, 2).cpu().numpy()\n",
    "        mel_lens = y[1][::s].squeeze().cpu().numpy() - 1\n",
    "    elif label == 'Reference spectrogram':\n",
    "        # y: mel_padded, dur_padded, dur_lens, pitch_padded\n",
    "        mel_specs = y[0][::s].cpu().numpy()\n",
    "        if mas:\n",
    "            mel_lens = y[2][::s].cpu().numpy()  # output_lengths\n",
    "        else:\n",
    "            mel_lens = y[1][::s].cpu().numpy().sum(axis=1) - 1\n",
    "            \n",
    "    image_names = []\n",
    "    spectrogram_figs = []\n",
    "    for mel_spec, mel_len, fname in zip(mel_specs, mel_lens, fnames):\n",
    "        mel_spec = mel_spec[:, :mel_len]\n",
    "        utt_id = os.path.splitext(os.path.basename(fname))[0]\n",
    "        # if mode == 'tb':\n",
    "        #     logger.log_spectrogram_tb(\n",
    "        #         step, '{}/{}'.format(label, utt_id), mel_spec, tb_subset='val')\n",
    "        # elif mode == 'wandb':\n",
    "        image_name = f'val/{label}/{utt_id}'\n",
    "        fig = log_spectrogram(mel_spec, image_name=image_name)\n",
    "        image_names.append(image_name)\n",
    "        spectrogram_figs.append(fig)\n",
    "            # wandb.log({image_name: \n",
    "            #             wandb.Image(plt, caption=image_name)})\n",
    "            # wandb.log({image_name: plt}) # requires plotly\n",
    "    return image_names, spectrogram_figs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_audio(y, fnames, step, vocoder=None, sampling_rate=22050, hop_length=256,\n",
    "                   n=4, label='Predicted audio', mas=False):\n",
    "    \"\"\"Generate audio from spectrograms for n utterances in batch\"\"\"\n",
    "    bs = len(fnames)\n",
    "    n = min(n, bs)\n",
    "    s = bs // n\n",
    "    fnames = fnames[::s]\n",
    "    # print(f\"inside generate_audio(), {fnames=}\")\n",
    "    with torch.no_grad():\n",
    "        if label == 'Predicted audio':\n",
    "            # y: mel_out, dec_mask, dur_pred, log_dur_pred, pitch_pred\n",
    "            audios = vocoder(y[0][::s].transpose(1, 2)).cpu().squeeze().numpy()\n",
    "            mel_lens = y[1][::s].squeeze().cpu().numpy() - 1\n",
    "        elif label == 'Copy synthesis':\n",
    "            # y: mel_padded, dur_padded, dur_lens, pitch_padded\n",
    "            audios = vocoder(y[0][::s]).cpu().squeeze().numpy()\n",
    "            if mas:\n",
    "                mel_lens = y[2][::s].cpu().numpy()  # output_lengths\n",
    "            else:\n",
    "                mel_lens = y[1][::s].cpu().numpy().sum(axis=1) - 1\n",
    "        elif label == 'Reference audio':\n",
    "            audios = []\n",
    "            for fname in fnames:\n",
    "                wav = re.sub(r'mels/(.+)\\.pt', r'wavs/\\1.wav', fname)\n",
    "                audio, _ = librosa.load(wav, sr=sampling_rate)\n",
    "                audios.append(audio)\n",
    "            if mas:\n",
    "                mel_lens = y[2][::s].cpu().numpy()  # output_lengths\n",
    "            else:\n",
    "                mel_lens = y[1][::s].cpu().numpy().sum(axis=1) - 1\n",
    "    audios_to_return = []\n",
    "    for audio, mel_len, fname in zip(audios, mel_lens, fnames):\n",
    "        audio = audio[:mel_len * hop_length]\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "        utt_id = os.path.splitext(os.path.basename(fname))[0]\n",
    "        # if mode == 'tb':\n",
    "        #     logger.log_audio_tb(\n",
    "        #         step, '{}/{}'.format(label, utt_id), audio, sampling_rate, tb_subset='val')\n",
    "        # elif mode == 'wandb':\n",
    "            # audio_name = f\"val/{label}/{utt_id}\"\n",
    "            # wandb.log({audio_name:\n",
    "            #    [wandb.Audio(audio, caption=audio_name, sample_rate=sampling_rate)]})\n",
    "        audios_to_return.append(audio)\n",
    "        \n",
    "    return audios_to_return\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def decode_indices(indices):\n",
    "    \"\"\"decode batch of indices to text\n",
    "    [bsz, seqlen]\"\"\"\n",
    "    decodings = []\n",
    "    for batch_idx in range(indices.size(0)):\n",
    "        decodings.append(''.join(tp.id_to_symbol[id] for id in indices[batch_idx].tolist()))\n",
    "    return decodings\n",
    "\n",
    "def select(x, bsz, n):\n",
    "    \"\"\"select items in batch that will be visualised/converted to audio\"\"\"\n",
    "    n = min(n, bsz)\n",
    "    s = bsz // n\n",
    "    return x[::s]\n",
    "\n",
    "def log_wandb_table(\n",
    "    names, \n",
    "    vocoded_gt_audios,\n",
    "    orig_words,\n",
    "    respellings,\n",
    "    orig_pred_spec_figs,\n",
    "    orig_pred_audios,\n",
    "    pred_spec_figs,\n",
    "    pred_audios,\n",
    "    sampling_rate=22050,\n",
    "):  \n",
    "    # define table\n",
    "    table = wandb.Table(columns=[\n",
    "        \"names\", \n",
    "        \"orig spelling\", \n",
    "        \"orig spelling spec\", \n",
    "        \"orig spelling audio\",\n",
    "        \"vocoded gt audio\",\n",
    "        \"respelling\", \n",
    "        \"respelling spec\", \n",
    "        \"respelling audio\",\n",
    "    ])\n",
    "    # add rows to table\n",
    "    for name, orig_word, orig_pred_spec_fig, orig_pred_audio, vocoded_gt_audio, respelling, pred_spec_fig, pred_audio in zip(\n",
    "        names, \n",
    "        orig_words, \n",
    "        orig_pred_spec_figs,\n",
    "        orig_pred_audios,\n",
    "        vocoded_gt_audios,\n",
    "        respellings, \n",
    "        pred_spec_figs, \n",
    "        pred_audios,\n",
    "    ):\n",
    "        table.add_data(\n",
    "            name, \n",
    "            orig_word,\n",
    "            wandb.Image(orig_pred_spec_fig, caption=name),\n",
    "            wandb.Audio(orig_pred_audio, caption=name, sample_rate=sampling_rate),\n",
    "            wandb.Audio(vocoded_gt_audio, caption=name, sample_rate=sampling_rate),\n",
    "            respelling,\n",
    "            wandb.Image(pred_spec_fig, caption=name),\n",
    "            wandb.Audio(pred_audio, caption=name, sample_rate=sampling_rate),\n",
    "        )\n",
    "        \n",
    "    wandb.log({\"val_table\": table})\n",
    "    \n",
    "    # close figures to save memory\n",
    "    for fig in orig_pred_spec_figs + pred_spec_figs:\n",
    "        plt.close(fig)\n",
    "\n",
    "def validate(\n",
    "    respeller_model, \n",
    "    tts_model, \n",
    "    vocoder,\n",
    "    criterion,\n",
    "    valset, \n",
    "    epoch, \n",
    "    batch_size, \n",
    "    collate_fn, \n",
    "    sampling_rate,\n",
    "    hop_length,\n",
    "    audio_interval=5,\n",
    "    n=None, # how many tokens to plot and generate audio for, if None then do the whole first batch\n",
    "):\n",
    "    \"\"\"Handles all the validation scoring and printing\n",
    "    GT (beginning of training):\n",
    "    - log GT mel spec and vocoded audio for several validation set words\n",
    "    \n",
    "    Model outputs:\n",
    "    - log predicted mel spec and vocoded audio from fastpitch\n",
    "    - log respelled word from respeller\n",
    "    \"\"\"\n",
    "    was_training = respeller_model.training\n",
    "    respeller_model.eval()\n",
    "    \n",
    "    tik = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        val_loader = DataLoader(valset, num_workers=4, shuffle=False,\n",
    "                                sampler=None,\n",
    "                                batch_size=batch_size, pin_memory=False,\n",
    "                                collate_fn=collate_fn)\n",
    "        val_meta = defaultdict(float)\n",
    "        val_losses = 0.0\n",
    "        epoch_iter = 0\n",
    "        \n",
    "        for i, batch in enumerate(val_loader):\n",
    "            epoch_iter += 1\n",
    "            \n",
    "            # get loss over batch\n",
    "            x, y = batch_to_gpu(batch)\n",
    "            pred_mel, dec_lens, g_embedding_indices = forward_pass(respeller_model, tts_model, x)\n",
    "            iter_loss = (criterion(pred_mel, y[\"mel_padded\"]) / dec_lens).mean().item()\n",
    "            val_losses += iter_loss\n",
    "    \n",
    "            # log spectrograms and generated audio for first few utterances\n",
    "            if (i == 0) and (epoch % audio_interval == 0 if epoch is not None else True):\n",
    "                fnames = batch['mel_filepaths']\n",
    "                bsz = len(fnames)\n",
    "                if n is None:\n",
    "                    n = bsz\n",
    "                \n",
    "                # get original word and respellings for logging\n",
    "                original_words = decode_indices(x['text_padded'])\n",
    "                respellings = decode_indices(g_embedding_indices)\n",
    "                \n",
    "                # vocode original recorded speech\n",
    "                gt_mel = y['mel_padded']\n",
    "                gt_mel_lens = y['mel_lengths']\n",
    "                vocoded_gt = generate_audio((gt_mel, gt_mel_lens), fnames, total_iter, vocoder, sampling_rate, hop_length, n=n, label='Predicted audio', mas=True)\n",
    "                \n",
    "                # get melspec + generated audio for original spellings\n",
    "                orig_pred_mel, orig_dec_lens, _dur_pred, _pitch_pred = tts(\n",
    "                    inputs=x['text_padded'],\n",
    "                    skip_embeddings=False,\n",
    "                )\n",
    "                orig_pred_mel = orig_pred_mel.transpose(1,2)\n",
    "                _orig_token_names, orig_pred_spec_figs = get_spectrograms_plots((orig_pred_mel, orig_dec_lens), fnames, total_iter, n=n, label='Predicted spectrogram', mas=True)\n",
    "                orig_pred_audios = generate_audio((orig_pred_mel, orig_dec_lens), fnames, total_iter, vocoder, sampling_rate, hop_length, n=n, label='Predicted audio', mas=True)\n",
    "            \n",
    "                # get melspec + generated audio for respellings\n",
    "                token_names, pred_spec_figs = get_spectrograms_plots((pred_mel, dec_lens), fnames, total_iter, n=n, label='Predicted spectrogram', mas=True)\n",
    "                pred_audios = generate_audio((pred_mel, dec_lens), fnames, total_iter, vocoder, sampling_rate, hop_length, n=n, label='Predicted audio', mas=True)\n",
    "                \n",
    "                # log everything to wandb table\n",
    "                token_names = [n.split('/')[-1] for n in token_names]\n",
    "                log_wandb_table(\n",
    "                    names=token_names,\n",
    "                    vocoded_gt_audios=vocoded_gt,\n",
    "                    orig_words=select(original_words, bsz, n=n),\n",
    "                    orig_pred_spec_figs=orig_pred_spec_figs,\n",
    "                    orig_pred_audios=orig_pred_audios,\n",
    "                    respellings=select(respellings, bsz, n=n),\n",
    "                    pred_spec_figs=pred_spec_figs,\n",
    "                    pred_audios=pred_audios,\n",
    "                    sampling_rate=sampling_rate,\n",
    "                )\n",
    "                \n",
    "        wandb.log({'val/epoch_loss': val_losses/epoch_iter})\n",
    "    \n",
    "    val_meta['took'] = time.perf_counter() - tik\n",
    "    \n",
    "    if was_training:\n",
    "        respeller_model.train()\n",
    "        \n",
    "    return val_meta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # test the validate function\n",
    "# validate(\n",
    "#     respeller_model=respeller, \n",
    "#     tts_model=tts, \n",
    "#     vocoder=vocoder,\n",
    "#     criterion=criterion,\n",
    "#     valset=val_dataset, \n",
    "#     batch_size=args.batch_size,\n",
    "#     collate_fn=collate_fn,\n",
    "#     epoch=0,\n",
    "#     sampling_rate=args.sampling_rate,\n",
    "#     hop_length=args.hop_length,\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def forward_pass(respeller, tts, x):\n",
    "    \"\"\"x: inputs\n",
    "    x = {\n",
    "        'words': words,\n",
    "        'text_padded': text_padded,\n",
    "        'text_lengths': text_lengths,\n",
    "    }\"\"\"\n",
    "    g_embeddings, g_embedding_indices = respeller(x['text_padded'])\n",
    "    \n",
    "    # quantiser_outdict = quantiser(logits, produce_targets=True)\n",
    "    # g_embedding_indices = quantiser_outdict[\"targets\"].squeeze(2)\n",
    "    # g_embeddings = quantiser_outdict[\"x\"]\n",
    "\n",
    "    log_mel, dec_lens, _dur_pred, _pitch_pred = tts(\n",
    "        inputs=g_embeddings,\n",
    "        ids=g_embedding_indices,\n",
    "        skip_embeddings=True,\n",
    "    )\n",
    "    \n",
    "    # log_mel: [bsz, dim, seqlen]\n",
    "    log_mel = log_mel.transpose(1,2)\n",
    "    # log_mel: [bsz, seqlen, dim]\n",
    "    \n",
    "    # return mask for masking acoustic loss\n",
    "    # padding_idx = 0\n",
    "    # mask = (g_embedding_indices != padding_idx).unsqueeze(2)\n",
    "    # mask.size()\n",
    "    # dec_mask = mask_from_lens(dec_lens).unsqueeze(2)\n",
    "    \n",
    "    return log_mel, dec_lens, g_embedding_indices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, args.epochs + 1):\n",
    "    # logging metrics\n",
    "    epoch_start_time = time.perf_counter()\n",
    "    epoch_loss = 0.0\n",
    "    iter_loss = 0\n",
    "    epoch_iter = 0\n",
    "    num_iters = len(train_loader)\n",
    "    # epoch_mel_loss = 0.0\n",
    "    # epoch_num_frames = 0\n",
    "    # epoch_frames_per_sec = 0.0\n",
    "    # iter_num_frames = 0\n",
    "    # iter_meta = {}\n",
    "\n",
    "    # iterate over all batches in epoch\n",
    "    for batch in train_loader:        \n",
    "        if epoch_iter == 50:\n",
    "            break # NB quit training loop, FOR DEVELOPMENT!!!\n",
    "        \n",
    "        if epoch_iter == num_iters: # useful for gradient accumulation\n",
    "            break\n",
    "                    \n",
    "        total_iter += 1\n",
    "        epoch_iter += 1\n",
    "        iter_start_time = time.perf_counter()\n",
    "\n",
    "        adjust_learning_rate(total_iter, optimizer, args.learning_rate,\n",
    "                             args.warmup_steps)\n",
    "\n",
    "        respeller.zero_grad()\n",
    "        # quantiser.zero_grad()\n",
    "\n",
    "        x, y = batch_to_gpu(batch) # x: inputs, y: targets\n",
    "        gt_mel = y[\"mel_padded\"]\n",
    "        \n",
    "        # # y: targets\n",
    "        # y = {\n",
    "        #     'mel_padded': mel_padded, \n",
    "        #     'mel_lengths': mel_lengths,\n",
    "        # }\n",
    "        \n",
    "        # forward pass through models (respeller -> quantiser -> tts)\n",
    "        pred_mel, dec_lens, _g_embedding_indices = forward_pass(respeller, tts, x)\n",
    "        \n",
    "        # TODO: DO WE NEED MASK IF WE USE SOFTDTW LOSS? \n",
    "        # I THINK IT AUTOMATICALLY WILL ALIGN PADDED FRAMES WITH EACH OTHER???\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(pred_mel, gt_mel)\n",
    "        # print('raw loss from softdtw', loss)\n",
    "        \n",
    "        loss = loss / dec_lens\n",
    "        # print('loss avg according to dec seqlens', loss)\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        # print('loss avged across batch', loss)\n",
    "        \n",
    "        # backpropagation of loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # clip gradients and run optimizer\n",
    "        torch.nn.utils.clip_grad_norm_(respeller.trainable_parameters(), args.grad_clip_thresh)\n",
    "        optimizer.step()\n",
    "        # logger.log_grads_tb(total_iter, model)\n",
    "        \n",
    "        # log metrics to terminal and to wandb\n",
    "        iter_loss = loss.item()\n",
    "        iter_time = time.perf_counter() - iter_start_time\n",
    "        epoch_loss += iter_loss\n",
    "        \n",
    "        log_stdout(\n",
    "            logger,\n",
    "            'train',\n",
    "            (epoch, epoch_iter, num_iters),\n",
    "            total_iter,\n",
    "            iter_loss,\n",
    "            iter_time,\n",
    "        )\n",
    "        \n",
    "        wandb.log({\n",
    "            \"train/iter_loss\": iter_loss,\n",
    "            \"train/iter_time\": iter_time,\n",
    "        })\n",
    "        ### Finished Epoch!\n",
    "             \n",
    "    epoch_time = time.perf_counter() - epoch_start_time\n",
    "    \n",
    "    wandb.log({\n",
    "        \"train/epoch_num\": epoch,\n",
    "        \"train/epoch_time\": epoch_time,\n",
    "        \"train/epoch_loss\": epoch_loss / epoch_iter,\n",
    "    })\n",
    "        \n",
    "    validate(\n",
    "        respeller_model=respeller, \n",
    "        tts_model=tts, \n",
    "        vocoder=vocoder,\n",
    "        criterion=criterion,\n",
    "        valset=val_dataset, \n",
    "        batch_size=args.batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        epoch=epoch,\n",
    "        sampling_rate=args.sampling_rate,\n",
    "        hop_length=args.hop_length,\n",
    "    )\n",
    "\n",
    "    maybe_save_checkpoint(args, respeller, optimizer, \n",
    "                          epoch, total_iter, model_config)\n",
    "\n",
    "    logger.flush()\n",
    "        \n",
    "print(\"\\n *** Finished training! ***\")\n",
    "\n",
    "# wandb.finish() # useful in jupyter notebooks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1b1c4b39-189d-4af5-a5c0-7489fbc95833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:43 - epoch    1 | iter   1/647 | loss 373.87 | took 0.16 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:43 - epoch    1 | iter   2/647 | loss 396.27 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:43 - epoch    1 | iter   3/647 | loss 397.08 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:43 - epoch    1 | iter   4/647 | loss 430.97 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:43 - epoch    1 | iter   5/647 | loss 388.25 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:43 - epoch    1 | iter   6/647 | loss 437.01 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:44 - epoch    1 | iter   7/647 | loss 417.55 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:44 - epoch    1 | iter   8/647 | loss 370.61 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:44 - epoch    1 | iter   9/647 | loss 510.60 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:44 - epoch    1 | iter  10/647 | loss 410.19 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:44 - epoch    1 | iter  11/647 | loss 406.79 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:44 - epoch    1 | iter  12/647 | loss 417.51 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:44 - epoch    1 | iter  13/647 | loss 413.16 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:44 - epoch    1 | iter  14/647 | loss 493.52 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:45 - epoch    1 | iter  15/647 | loss 347.56 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:45 - epoch    1 | iter  16/647 | loss 523.87 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:45 - epoch    1 | iter  17/647 | loss 337.99 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:45 - epoch    1 | iter  18/647 | loss 419.78 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:45 - epoch    1 | iter  19/647 | loss 288.45 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:45 - epoch    1 | iter  20/647 | loss 279.20 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:45 - epoch    1 | iter  21/647 | loss 382.44 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:45 - epoch    1 | iter  22/647 | loss 371.13 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:45 - epoch    1 | iter  23/647 | loss 316.50 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:46 - epoch    1 | iter  24/647 | loss 354.88 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:46 - epoch    1 | iter  25/647 | loss 388.13 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:46 - epoch    1 | iter  26/647 | loss 539.34 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:46 - epoch    1 | iter  27/647 | loss 408.78 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:46 - epoch    1 | iter  28/647 | loss 430.23 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:46 - epoch    1 | iter  29/647 | loss 449.56 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:46 - epoch    1 | iter  30/647 | loss 377.48 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:46 - epoch    1 | iter  31/647 | loss 438.09 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:46 - epoch    1 | iter  32/647 | loss 544.27 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:47 - epoch    1 | iter  33/647 | loss 451.82 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:47 - epoch    1 | iter  34/647 | loss 473.39 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:47 - epoch    1 | iter  35/647 | loss 363.52 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:47 - epoch    1 | iter  36/647 | loss 407.01 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:47 - epoch    1 | iter  37/647 | loss 504.57 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:47 - epoch    1 | iter  38/647 | loss 565.73 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:47 - epoch    1 | iter  39/647 | loss 465.67 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:47 - epoch    1 | iter  40/647 | loss 429.12 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:48 - epoch    1 | iter  41/647 | loss 542.64 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:48 - epoch    1 | iter  42/647 | loss 376.90 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:48 - epoch    1 | iter  43/647 | loss 424.07 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:48 - epoch    1 | iter  44/647 | loss 435.44 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:48 - epoch    1 | iter  45/647 | loss 448.05 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:48 - epoch    1 | iter  46/647 | loss 451.68 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:48 - epoch    1 | iter  47/647 | loss 390.76 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:48 - epoch    1 | iter  48/647 | loss 382.76 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:48 - epoch    1 | iter  49/647 | loss 522.62 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:49 - epoch    1 | iter  50/647 | loss 469.27 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:51 - epoch    2 | iter   1/647 | loss 514.33 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:51 - epoch    2 | iter   2/647 | loss 436.06 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:51 - epoch    2 | iter   3/647 | loss 472.03 | took 0.16 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:51 - epoch    2 | iter   4/647 | loss 443.35 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:51 - epoch    2 | iter   5/647 | loss 446.72 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:51 - epoch    2 | iter   6/647 | loss 474.36 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:52 - epoch    2 | iter   7/647 | loss 468.41 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:52 - epoch    2 | iter   8/647 | loss 304.44 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:52 - epoch    2 | iter   9/647 | loss 447.08 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:52 - epoch    2 | iter  10/647 | loss 494.30 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:52 - epoch    2 | iter  11/647 | loss 577.10 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:52 - epoch    2 | iter  12/647 | loss 415.08 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:52 - epoch    2 | iter  13/647 | loss 509.69 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:52 - epoch    2 | iter  14/647 | loss 575.08 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:52 - epoch    2 | iter  15/647 | loss 361.63 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:53 - epoch    2 | iter  16/647 | loss 484.99 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:53 - epoch    2 | iter  17/647 | loss 484.89 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:53 - epoch    2 | iter  18/647 | loss 973.58 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:53 - epoch    2 | iter  19/647 | loss 441.95 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:53 - epoch    2 | iter  20/647 | loss 514.26 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:53 - epoch    2 | iter  21/647 | loss 542.92 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:53 - epoch    2 | iter  22/647 | loss 493.38 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:53 - epoch    2 | iter  23/647 | loss 2510.99 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:54 - epoch    2 | iter  24/647 | loss 2317.06 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:54 - epoch    2 | iter  25/647 | loss 2176.76 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:54 - epoch    2 | iter  26/647 | loss 1978.66 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:54 - epoch    2 | iter  27/647 | loss 2401.62 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:54 - epoch    2 | iter  28/647 | loss 3143.93 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:54 - epoch    2 | iter  29/647 | loss 2233.48 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:54 - epoch    2 | iter  30/647 | loss 4284.28 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:54 - epoch    2 | iter  31/647 | loss 2253.31 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:55 - epoch    2 | iter  32/647 | loss 2279.32 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:55 - epoch    2 | iter  33/647 | loss 2770.40 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:55 - epoch    2 | iter  34/647 | loss 2442.85 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:55 - epoch    2 | iter  35/647 | loss 2214.28 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:55 - epoch    2 | iter  36/647 | loss 2742.06 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:55 - epoch    2 | iter  37/647 | loss 2271.89 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:55 - epoch    2 | iter  38/647 | loss 3055.12 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:55 - epoch    2 | iter  39/647 | loss 2658.16 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:55 - epoch    2 | iter  40/647 | loss 2588.70 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:56 - epoch    2 | iter  41/647 | loss 2611.83 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:56 - epoch    2 | iter  42/647 | loss 2574.65 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:56 - epoch    2 | iter  43/647 | loss 3270.69 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:56 - epoch    2 | iter  44/647 | loss 1548.10 | took 0.15 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:56 - epoch    2 | iter  45/647 | loss 2221.91 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:56 - epoch    2 | iter  46/647 | loss 2089.05 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:56 - epoch    2 | iter  47/647 | loss 2857.76 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:56 - epoch    2 | iter  48/647 | loss 2543.84 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:57 - epoch    2 | iter  49/647 | loss 3122.45 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:57 - epoch    2 | iter  50/647 | loss 3063.67 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:59 - epoch    3 | iter   1/647 | loss 2905.22 | took 0.15 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:59 - epoch    3 | iter   2/647 | loss 2244.67 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:59 - epoch    3 | iter   3/647 | loss 3227.62 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:59 - epoch    3 | iter   4/647 | loss 2176.44 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:59 - epoch    3 | iter   5/647 | loss 2441.08 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:59 - epoch    3 | iter   6/647 | loss 1948.57 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:57:59 - epoch    3 | iter   7/647 | loss 1747.30 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:00 - epoch    3 | iter   8/647 | loss 4281.67 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:00 - epoch    3 | iter   9/647 | loss 2088.85 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:00 - epoch    3 | iter  10/647 | loss 2293.02 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:00 - epoch    3 | iter  11/647 | loss 1859.19 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:00 - epoch    3 | iter  12/647 | loss 2802.71 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:00 - epoch    3 | iter  13/647 | loss 3039.42 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:00 - epoch    3 | iter  14/647 | loss 2337.10 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:00 - epoch    3 | iter  15/647 | loss 2122.26 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:01 - epoch    3 | iter  16/647 | loss 2522.63 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:01 - epoch    3 | iter  17/647 | loss 2726.63 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:01 - epoch    3 | iter  18/647 | loss 2296.64 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:01 - epoch    3 | iter  19/647 | loss 2269.77 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:01 - epoch    3 | iter  20/647 | loss 3678.78 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:01 - epoch    3 | iter  21/647 | loss 2084.52 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:01 - epoch    3 | iter  22/647 | loss 2014.66 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:01 - epoch    3 | iter  23/647 | loss 1810.67 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:02 - epoch    3 | iter  24/647 | loss 2175.80 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:02 - epoch    3 | iter  25/647 | loss 2393.45 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:02 - epoch    3 | iter  26/647 | loss 2792.81 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:02 - epoch    3 | iter  27/647 | loss 2547.35 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:02 - epoch    3 | iter  28/647 | loss 2336.40 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:02 - epoch    3 | iter  29/647 | loss 2880.00 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:02 - epoch    3 | iter  30/647 | loss 2493.16 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:02 - epoch    3 | iter  31/647 | loss 2449.01 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:03 - epoch    3 | iter  32/647 | loss 2093.32 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:03 - epoch    3 | iter  33/647 | loss 2610.54 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:03 - epoch    3 | iter  34/647 | loss 2606.20 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:03 - epoch    3 | iter  35/647 | loss 3376.50 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:03 - epoch    3 | iter  36/647 | loss 2060.89 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:03 - epoch    3 | iter  37/647 | loss 1890.27 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:03 - epoch    3 | iter  38/647 | loss 3098.47 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:03 - epoch    3 | iter  39/647 | loss 1657.87 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:04 - epoch    3 | iter  40/647 | loss 3598.50 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:04 - epoch    3 | iter  41/647 | loss 2772.92 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:04 - epoch    3 | iter  42/647 | loss 1907.20 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:04 - epoch    3 | iter  43/647 | loss 1823.70 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:04 - epoch    3 | iter  44/647 | loss 2036.20 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:04 - epoch    3 | iter  45/647 | loss 2107.09 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:04 - epoch    3 | iter  46/647 | loss 4157.50 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:05 - epoch    3 | iter  47/647 | loss 3146.47 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:05 - epoch    3 | iter  48/647 | loss 3664.55 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:05 - epoch    3 | iter  49/647 | loss 3318.78 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:05 - epoch    3 | iter  50/647 | loss 1791.73 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:07 - epoch    4 | iter   1/647 | loss 2518.17 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:07 - epoch    4 | iter   2/647 | loss 2728.48 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:07 - epoch    4 | iter   3/647 | loss 2306.60 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:07 - epoch    4 | iter   4/647 | loss 2292.63 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:07 - epoch    4 | iter   5/647 | loss 2588.80 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:08 - epoch    4 | iter   6/647 | loss 2533.83 | took 0.17 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:08 - epoch    4 | iter   7/647 | loss 1867.25 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:08 - epoch    4 | iter   8/647 | loss 3144.50 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:08 - epoch    4 | iter   9/647 | loss 3445.14 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:08 - epoch    4 | iter  10/647 | loss 1684.54 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:08 - epoch    4 | iter  11/647 | loss 2378.05 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:08 - epoch    4 | iter  12/647 | loss 3385.65 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:08 - epoch    4 | iter  13/647 | loss 2588.17 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:09 - epoch    4 | iter  14/647 | loss 1749.47 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:09 - epoch    4 | iter  15/647 | loss 3111.12 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:09 - epoch    4 | iter  16/647 | loss 2237.97 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:09 - epoch    4 | iter  17/647 | loss 2454.35 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:09 - epoch    4 | iter  18/647 | loss 2452.33 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:09 - epoch    4 | iter  19/647 | loss 2359.91 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:09 - epoch    4 | iter  20/647 | loss 3595.88 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:09 - epoch    4 | iter  21/647 | loss 2565.38 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:10 - epoch    4 | iter  22/647 | loss 2599.96 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:10 - epoch    4 | iter  23/647 | loss 2754.20 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:10 - epoch    4 | iter  24/647 | loss 3079.04 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:10 - epoch    4 | iter  25/647 | loss 2515.26 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:10 - epoch    4 | iter  26/647 | loss 2663.66 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:10 - epoch    4 | iter  27/647 | loss 2907.85 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:10 - epoch    4 | iter  28/647 | loss 2716.29 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:10 - epoch    4 | iter  29/647 | loss 2852.74 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:11 - epoch    4 | iter  30/647 | loss 2921.55 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:11 - epoch    4 | iter  31/647 | loss 2789.54 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:11 - epoch    4 | iter  32/647 | loss 2320.56 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:11 - epoch    4 | iter  33/647 | loss 2332.21 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:11 - epoch    4 | iter  34/647 | loss 2157.38 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:11 - epoch    4 | iter  35/647 | loss 2870.61 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:11 - epoch    4 | iter  36/647 | loss 1860.69 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:11 - epoch    4 | iter  37/647 | loss 2111.35 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:12 - epoch    4 | iter  38/647 | loss 1873.74 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:12 - epoch    4 | iter  39/647 | loss 1640.67 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:12 - epoch    4 | iter  40/647 | loss 2762.79 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:12 - epoch    4 | iter  41/647 | loss 3969.20 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:12 - epoch    4 | iter  42/647 | loss 2337.23 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:12 - epoch    4 | iter  43/647 | loss 2619.42 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:12 - epoch    4 | iter  44/647 | loss 2556.22 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:13 - epoch    4 | iter  45/647 | loss 2066.53 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:13 - epoch    4 | iter  46/647 | loss 1986.38 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:13 - epoch    4 | iter  47/647 | loss 2264.23 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:13 - epoch    4 | iter  48/647 | loss 2683.90 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:13 - epoch    4 | iter  49/647 | loss 2765.99 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:13 - epoch    4 | iter  50/647 | loss 2172.07 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:15 - epoch    5 | iter   1/647 | loss 1853.20 | took 0.15 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:15 - epoch    5 | iter   2/647 | loss 2373.98 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:15 - epoch    5 | iter   3/647 | loss 2258.18 | took 0.11 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:16 - epoch    5 | iter   4/647 | loss 2929.50 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:16 - epoch    5 | iter   5/647 | loss 2681.18 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:16 - epoch    5 | iter   6/647 | loss 1875.10 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:16 - epoch    5 | iter   7/647 | loss 2450.12 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:16 - epoch    5 | iter   8/647 | loss 2797.67 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:16 - epoch    5 | iter   9/647 | loss 2291.20 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:16 - epoch    5 | iter  10/647 | loss 1988.05 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:17 - epoch    5 | iter  11/647 | loss 2312.41 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:17 - epoch    5 | iter  12/647 | loss 2747.39 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:17 - epoch    5 | iter  13/647 | loss 2333.26 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:17 - epoch    5 | iter  14/647 | loss 2010.61 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:17 - epoch    5 | iter  15/647 | loss 2743.38 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:17 - epoch    5 | iter  16/647 | loss 2407.64 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:17 - epoch    5 | iter  17/647 | loss 2135.17 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:17 - epoch    5 | iter  18/647 | loss 2883.01 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:18 - epoch    5 | iter  19/647 | loss 2668.57 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:18 - epoch    5 | iter  20/647 | loss 1981.53 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:18 - epoch    5 | iter  21/647 | loss 2449.74 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:18 - epoch    5 | iter  22/647 | loss 3517.37 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:18 - epoch    5 | iter  23/647 | loss 2834.22 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:18 - epoch    5 | iter  24/647 | loss 3004.98 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:18 - epoch    5 | iter  25/647 | loss 2221.87 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:19 - epoch    5 | iter  26/647 | loss 2232.96 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:19 - epoch    5 | iter  27/647 | loss 3003.25 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:19 - epoch    5 | iter  28/647 | loss 2666.35 | took 0.20 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:19 - epoch    5 | iter  29/647 | loss 2826.40 | took 0.14 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:19 - epoch    5 | iter  30/647 | loss 2787.50 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:19 - epoch    5 | iter  31/647 | loss 2679.78 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:19 - epoch    5 | iter  32/647 | loss 3198.58 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:20 - epoch    5 | iter  33/647 | loss 2459.94 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:20 - epoch    5 | iter  34/647 | loss 2308.02 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:20 - epoch    5 | iter  35/647 | loss 3832.04 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:20 - epoch    5 | iter  36/647 | loss 2622.75 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:20 - epoch    5 | iter  37/647 | loss 3075.24 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:20 - epoch    5 | iter  38/647 | loss 2059.64 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:20 - epoch    5 | iter  39/647 | loss 3396.90 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:20 - epoch    5 | iter  40/647 | loss 3432.08 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:21 - epoch    5 | iter  41/647 | loss 2447.62 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:21 - epoch    5 | iter  42/647 | loss 3295.22 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:21 - epoch    5 | iter  43/647 | loss 3438.67 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:21 - epoch    5 | iter  44/647 | loss 2146.10 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:21 - epoch    5 | iter  45/647 | loss 1863.41 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:21 - epoch    5 | iter  46/647 | loss 1933.54 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:21 - epoch    5 | iter  47/647 | loss 1528.56 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:22 - epoch    5 | iter  48/647 | loss 2617.45 | took 0.12 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:22 - epoch    5 | iter  49/647 | loss 2072.79 | took 0.13 s\n",
      "Frozen weights: embedding.weight torch.Size([40, 384])\n",
      "Frozen weights: quantiser.vars torch.Size([1, 40, 384])\n",
      "DLL 2022-12-01 12:58:22 - epoch    5 | iter  50/647 | loss 1687.19 | took 0.12 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20678/918057965.py:5: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, ax = plt.subplots(figsize=figsize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model and optimizer state at epoch 5 to /home/s1785140/respeller/exps/test/respeller_checkpoint_5.pt\n",
      "\n",
      " *** Finished training! ***\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, args.epochs + 1):\n",
    "    # logging metrics\n",
    "    epoch_start_time = time.perf_counter()\n",
    "    epoch_loss = 0.0\n",
    "    iter_loss = 0\n",
    "    epoch_iter = 0\n",
    "    num_iters = len(train_loader)\n",
    "    # epoch_mel_loss = 0.0\n",
    "    # epoch_num_frames = 0\n",
    "    # epoch_frames_per_sec = 0.0\n",
    "    # iter_num_frames = 0\n",
    "    # iter_meta = {}\n",
    "\n",
    "    # iterate over all batches in epoch\n",
    "    for batch in train_loader:        \n",
    "        if epoch_iter == 50:\n",
    "            break # NB quit training loop, FOR DEVELOPMENT!!!\n",
    "        \n",
    "        if epoch_iter == num_iters: # useful for gradient accumulation\n",
    "            break\n",
    "                    \n",
    "        total_iter += 1\n",
    "        epoch_iter += 1\n",
    "        iter_start_time = time.perf_counter()\n",
    "\n",
    "        adjust_learning_rate(total_iter, optimizer, args.learning_rate,\n",
    "                             args.warmup_steps)\n",
    "\n",
    "        respeller.zero_grad()\n",
    "        # quantiser.zero_grad()\n",
    "\n",
    "        x, y = batch_to_gpu(batch) # x: inputs, y: targets\n",
    "        gt_mel = y[\"mel_padded\"]\n",
    "        \n",
    "        # # y: targets\n",
    "        # y = {\n",
    "        #     'mel_padded': mel_padded, \n",
    "        #     'mel_lengths': mel_lengths,\n",
    "        # }\n",
    "        \n",
    "        # forward pass through models (respeller -> quantiser -> tts)\n",
    "        pred_mel, dec_lens, _g_embedding_indices = forward_pass(respeller, tts, x)\n",
    "        \n",
    "        # TODO: DO WE NEED MASK IF WE USE SOFTDTW LOSS? \n",
    "        # I THINK IT AUTOMATICALLY WILL ALIGN PADDED FRAMES WITH EACH OTHER???\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(pred_mel, gt_mel)\n",
    "        # print('raw loss from softdtw', loss)\n",
    "        \n",
    "        loss = loss / dec_lens\n",
    "        # print('loss avg according to dec seqlens', loss)\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        # print('loss avged across batch', loss)\n",
    "        \n",
    "        # backpropagation of loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # clip gradients and run optimizer\n",
    "        torch.nn.utils.clip_grad_norm_(respeller.trainable_parameters(), args.grad_clip_thresh)\n",
    "        optimizer.step()\n",
    "        # logger.log_grads_tb(total_iter, model)\n",
    "        \n",
    "        # log metrics to terminal and to wandb\n",
    "        iter_loss = loss.item()\n",
    "        iter_time = time.perf_counter() - iter_start_time\n",
    "        epoch_loss += iter_loss\n",
    "        \n",
    "        log_stdout(\n",
    "            logger,\n",
    "            'train',\n",
    "            (epoch, epoch_iter, num_iters),\n",
    "            total_iter,\n",
    "            iter_loss,\n",
    "            iter_time,\n",
    "        )\n",
    "        \n",
    "        wandb.log({\n",
    "            \"train/iter_loss\": iter_loss,\n",
    "            \"train/iter_time\": iter_time,\n",
    "        })\n",
    "        ### Finished Epoch!\n",
    "             \n",
    "    epoch_time = time.perf_counter() - epoch_start_time\n",
    "    \n",
    "    wandb.log({\n",
    "        \"train/epoch_num\": epoch,\n",
    "        \"train/epoch_time\": epoch_time,\n",
    "        \"train/epoch_loss\": epoch_loss / epoch_iter,\n",
    "    })\n",
    "        \n",
    "    validate(\n",
    "        respeller_model=respeller, \n",
    "        tts_model=tts, \n",
    "        vocoder=vocoder,\n",
    "        criterion=criterion,\n",
    "        valset=val_dataset, \n",
    "        batch_size=args.batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        epoch=epoch,\n",
    "        sampling_rate=args.sampling_rate,\n",
    "        hop_length=args.hop_length,\n",
    "    )\n",
    "\n",
    "    maybe_save_checkpoint(args, respeller, optimizer, \n",
    "                          epoch, total_iter, model_config)\n",
    "\n",
    "    logger.flush()\n",
    "        \n",
    "print(\"\\n *** Finished training! ***\")\n",
    "\n",
    "# wandb.finish() # useful in jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b565bf9-5cf9-4aa0-941b-74fd3cc19c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}