{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f5a3c3-31af-4835-88ee-4410bc5af326",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de0d84a-9335-48be-8bfb-ba9cbbf0dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53034b89-b847-4927-a604-cc6cb78516c2",
   "metadata": {},
   "source": [
    "# dev train loop + model for respeller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72cec08-fc1a-459b-87d4-49f1219cc86d",
   "metadata": {},
   "source": [
    "# command line args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc78f43f-f8f6-4ffc-94b2-5aa10bdb06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imitate CLAs\n",
    "import sys\n",
    "sys.argv = [\n",
    "    'train.py',\n",
    "    '--fastpitch-chkpt', 'fastpitch/exps/test_addspaces_1_eos_dollar/FastPitch_checkpoint_50.pt',\n",
    "    '--input-type', 'char',\n",
    "    '--symbol-set', 'english_basic_lowercase',\n",
    "    '--use-mas',\n",
    "    '--cuda',\n",
    "    '--n-speakers', '1',\n",
    "    '--use-sepconv',\n",
    "    '--add-spaces',\n",
    "    '--eos-symbol', '$',\n",
    "    '--epochs', '1', #Â for development!\n",
    "    '--chkpt-save-dir', '/home/s1785140/respeller/exps/test', \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cfb56b-58ed-447f-9308-a25d1e92cbc0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f776e058-d463-414a-b612-c6540f40a650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s1785140/miniconda3/envs/fastpitch/lib/python3.8/site-packages/resampy/interpn.py:114: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 6103. The TBB threading layer is disabled.\n",
      "  _resample_loop_p(x, t_out, interp_win, interp_delta, num_table, scale, y)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train respeller model\n",
    "\n",
    "We backpropagate loss from pretrained TTS model to a Grapheme-to-Grapheme (G2G) respeller model to help it respell words\n",
    "into a simpler form\n",
    "\n",
    "Intermediated respellings are discrete character sequences\n",
    "We can backpropagate through these using gumbel softmax and the straight through estimator\n",
    "'''\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "\n",
    "from fastpitch import models as fastpitch_model\n",
    "from fastpitch.common.text.text_processing import TextProcessor\n",
    "\n",
    "from modules.model import EncoderRespeller\n",
    "from modules.gumbel_vector_quantizer import GumbelVectorQuantizer\n",
    "from modules.sdtw_cuda_loss import SoftDTW\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752f248-d859-4a78-8c79-4c65080948b6",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cbcac7-2394-41a3-8fe2-c8c486309b08",
   "metadata": {},
   "source": [
    "## arguments to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_args(parser):\n",
    "    \"\"\"Parse commandline arguments\"\"\"\n",
    "    parser.add_argument('-o', '--chkpt-save-dir', type=str, required=True,\n",
    "                        help='Directory to save checkpoints')\n",
    "    parser.add_argument('-d', '--dataset-path', type=str, default='./',\n",
    "                        help='Path to dataset')\n",
    "    parser.add_argument('--log-file', type=str, default=None,\n",
    "                        help='Path to a DLLogger log file')\n",
    "\n",
    "    train = parser.add_argument_group('training setup')\n",
    "    train.add_argument('--cuda', action='store_true',\n",
    "                      help='Enable GPU training')\n",
    "    train.add_argument('--batch-size', type=int, default=16,\n",
    "                      help='Batchsize (this is divided by number of GPUs if running Data Distributed Parallel Training)')\n",
    "    train.add_argument('--seed', type=int, default=1337,\n",
    "                       help='Seed for PyTorch random number generators')\n",
    "    train.add_argument('--grad-accumulation', type=int, default=1,\n",
    "                       help='Training steps to accumulate gradients for')\n",
    "    train.add_argument('--epochs', type=int, default=100, #required=True,\n",
    "                       help='Number of total epochs to run')\n",
    "    train.add_argument('--epochs-per-checkpoint', type=int, default=10,\n",
    "                       help='Number of epochs per checkpoint')\n",
    "    train.add_argument('--checkpoint-path', type=str, default=None,\n",
    "                       help='Checkpoint path to resume train')\n",
    "    train.add_argument('--resume', action='store_true',\n",
    "                       help='Resume train from the last available checkpoint')\n",
    "    \n",
    "    opt = parser.add_argument_group('optimization setup')\n",
    "    opt.add_argument('--optimizer', type=str, default='lamb', choices=['adam', 'lamb'],\n",
    "                     help='Optimization algorithm')\n",
    "    opt.add_argument('-lr', '--learning-rate', default=0.1, type=float,\n",
    "                     help='Learning rate')\n",
    "    opt.add_argument('--weight-decay', default=1e-6, type=float,\n",
    "                     help='Weight decay')\n",
    "    # opt.add_argument('--grad-clip-thresh', default=1000.0, type=float,\n",
    "    #                  help='Clip threshold for gradients')\n",
    "    opt.add_argument('--warmup-steps', type=int, default=1000,\n",
    "                     help='Number of steps for lr warmup')\n",
    "\n",
    "    arch = parser.add_argument_group('architecture')\n",
    "    arch.add_argument('--d-model', type=int, default=512,\n",
    "                       help='Hidden dimension of tranformer')\n",
    "    arch.add_argument('--latent-temp', type=tuple, default=(2, 0.5, 0.999995),\n",
    "                       help='Temperature annealling parameters for Gumbel-Softmax (start, end, decay)')\n",
    "\n",
    "    pretrained_tts = parser.add_argument_group('pretrained tts model')\n",
    "    # pretrained_tts.add_argument('--fastpitch-with-mas', type=bool, default=True,\n",
    "    #                   help='Whether or not fastpitch was trained with Monotonic Alignment Search (MAS)')\n",
    "    pretrained_tts.add_argument('--fastpitch-chkpt', type=str, required=True,\n",
    "                      help='Path to pretrained fastpitch checkpoint')\n",
    "    pretrained_tts.add_argument('--input-type', type=str, default='char',\n",
    "                      choices=['char', 'phone', 'pf', 'unit'],\n",
    "                      help='Input symbols used, either char (text), phone, pf '\n",
    "                      '(phonological feature vectors) or unit (quantized acoustic '\n",
    "                      'representation IDs)')\n",
    "    pretrained_tts.add_argument('--symbol-set', type=str, default='english_basic_lowercase',\n",
    "                      help='Define symbol set for input sequences. For quantized '\n",
    "                      'unit inputs, pass the size of the vocabulary.')\n",
    "    pretrained_tts.add_argument('--n-speakers', type=int, default=1,\n",
    "                      help='Condition on speaker, value > 1 enables trainable '\n",
    "                      'speaker embeddings.')\n",
    "    # pretrained_tts.add_argument('--use-sepconv', type=bool, default=True,\n",
    "    #                   help='Use depthwise separable convolutions')\n",
    "    \n",
    "    audio = parser.add_argument_group('log generated audio')\n",
    "    audio.add_argument('--hifigan', type=str, default='/home/s1785140/pretrained_models/hifigan/ljspeech/LJ_V1/generator_v1',\n",
    "                       help='Path to HiFi-GAN audio checkpoint')\n",
    "    audio.add_argument('--hifigan-config', type=str, default='/home/s1785140/pretrained_models/hifigan/ljspeech/LJ_V1/config.json',\n",
    "                       help='Path to HiFi-GAN audio config file')\n",
    "    \n",
    "    data = parser.add_argument_group('dataset parameters')\n",
    "    cond = parser.add_argument_group('conditioning on additional attributes')\n",
    "    dist = parser.add_argument_group('distributed training setup')\n",
    "\n",
    "    return parser\n",
    "\n",
    "def load_checkpoint(args, model, filepath):\n",
    "    if args.local_rank == 0:\n",
    "        print(f'Loading model and optimizer state from {filepath}')\n",
    "    checkpoint = torch.load(filepath, map_location='cpu')\n",
    "    sd = {k.replace('module.', ''): v\n",
    "          for k, v in checkpoint['state_dict'].items()}\n",
    "    getattr(model, 'module', model).load_state_dict(sd)\n",
    "    return model\n",
    "\n",
    "def freeze_weights(model):\n",
    "    # NB wait... won't this stop backprop of gradients?\n",
    "    # We just don't want to add the fastpitch/quantiser model weights to the optimiser...\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def init_embedding_weights(source_tensor, target_tensor):\n",
    "    \"\"\"copy weights inplace from source tensor to target tensor\"\"\"\n",
    "    target_tensor.requires_grad = False\n",
    "    target_tensor.copy_(source_tensor.clone().detach())\n",
    "    target_tensor.requires_grad = True\n",
    "\n",
    "def load_pretrained_fastpitch(args):\n",
    "    # load chkpt\n",
    "    device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "    model_config = fastpitch_model.get_model_config('FastPitch', args)\n",
    "    fastpitch = fastpitch_model.get_model('FastPitch', model_config, device, forward_is_infer=True)\n",
    "    load_checkpoint(args, fastpitch, args.fastpitch_chkpt)\n",
    "    # get information about grapheme embedding table\n",
    "    n_symbols = fastpitch.encoder.word_emb.weight.size(0)\n",
    "    grapheme_embedding_dim = fastpitch.encoder.word_emb.weight.size(1)\n",
    "    return fastpitch, n_symbols, grapheme_embedding_dim, model_config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# beginning of main(), parse Command Line Args"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Respeller Training', allow_abbrev=False)\n",
    "parser = parse_args(parser)\n",
    "args, _unk_args = parser.parse_known_args()\n",
    "\n",
    "parser = fastpitch_model.parse_model_args('FastPitch', parser)\n",
    "args, unk_args = parser.parse_known_args()\n",
    "if len(unk_args) > 0:\n",
    "    raise ValueError(f'Invalid options {unk_args}')\n",
    "\n",
    "if args.cuda:\n",
    "    args.num_gpus = torch.cuda.device_count()\n",
    "    args.distributed_run = args.num_gpus > 1\n",
    "    args.batch_size = int(args.batch_size / args.num_gpus)\n",
    "else:\n",
    "    args.distributed_run = False\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "if args.distributed_run:\n",
    "    mp.spawn(train, nprocs=args.num_gpus, args=(args,))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 'train()' - forward pass through model to get loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## create / load models "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rank = 0\n",
    "device = 'cuda'\n",
    "\n",
    "args.local_rank = rank\n",
    "tts, n_symbols, grapheme_embedding_dim, model_config = load_pretrained_fastpitch(args)\n",
    "tts.to(device)\n",
    "\n",
    "respeller = EncoderRespeller(n_symbols=n_symbols, d_model=args.d_model)\n",
    "respeller.to(device)\n",
    "\n",
    "quantiser = GumbelVectorQuantizer(\n",
    "    in_dim=args.d_model,\n",
    "    codebook_size=n_symbols,  # number of codebook entries\n",
    "    embedding_dim=grapheme_embedding_dim,\n",
    "    temp=args.latent_temp,\n",
    ")\n",
    "quantiser.to(device)\n",
    "\n",
    "init_embedding_weights(tts.encoder.word_emb.weight.unsqueeze(0), quantiser.vars)\n",
    "\n",
    "# batch_size, len_x, len_y, dims = 8, 15, 12, 5\n",
    "# x = torch.rand((batch_size, len_x, dims), requires_grad=True)\n",
    "# y = torch.rand((batch_size, len_y, dims))\n",
    "\n",
    "# criterion = SoftDTW(use_cuda=True, gamma=0.1, dist_func=F.mse_loss)\n",
    "criterion = SoftDTW(use_cuda=True, gamma=0.1)\n",
    "# input should be size [bsz, seqlen, dim]\n",
    "criterion.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### load HiFiGAN vocoder "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_vocoder(args, device):\n",
    "    \"\"\"Load HiFi-GAN vocoder from checkpoint\"\"\"\n",
    "    checkpoint_data = torch.load(args.hifigan)\n",
    "    vocoder_config = fastpitch_model.get_model_config('HiFi-GAN', args)\n",
    "    vocoder = fastpitch_model.get_model('HiFi-GAN', vocoder_config, device)\n",
    "    vocoder.load_state_dict(checkpoint_data['generator'])\n",
    "    vocoder.remove_weight_norm()\n",
    "    vocoder.eval()\n",
    "    return vocoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocoder = load_vocoder(args, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## forward pass through model with dummy data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batches = []\n",
    "symbol_set = 'english_basic_lowercase'\n",
    "text_cleaners = []\n",
    "gt_log_mel = torch.load('/home/s1785140/data/ljspeech_fastpitch/mels/LJ001-0001.pt').cuda().unsqueeze(0).transpose(1,2) # intro batch dimension + [bsz, seqlen, dim]\n",
    "raw_text = 'printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the exhibition'\n",
    "\n",
    "# process text using same processor as fastpitch\n",
    "tp = TextProcessor(symbol_set, text_cleaners)\n",
    "text = torch.LongTensor(tp.encode_text(raw_text)).unsqueeze(0).cuda()\n",
    "\n",
    "batches.append((text, gt_log_mel))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text.device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for batch in batches:\n",
    "batch = batches[0]\n",
    "    \n",
    "###############################################################################################################\n",
    "# text, ssl_reps, e2e_asr_predictions, gt_log_mel = batch\n",
    "text, gt_log_mel = batch\n",
    "\n",
    "###############################################################################################################\n",
    "# create inputs\n",
    "# if args.use_acoustic_input:\n",
    "#     inputs = inputs.concat(ssl_reps)\n",
    "\n",
    "###############################################################################################################\n",
    "# forward pass\n",
    "logits = respeller(text[:13])\n",
    "logits.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_symbols"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logits.device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g_embedding_indices = quantiser(logits, produce_targets=True)[\"targets\"].squeeze(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g_embedding_indices.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g_embeddings = quantiser(logits, produce_targets=False)[\"x\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g_embeddings.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "padding_idx = 0\n",
    "mask = (g_embedding_indices != padding_idx).unsqueeze(2)\n",
    "mask.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_mel, dec_lens, _dur_pred, _pitch_pred = tts(g_embeddings, skip_embeddings=True, ids=g_embedding_indices)\n",
    "# log_mel [bsz, dim, seqlen]\n",
    "log_mel = log_mel.transpose(1,2)\n",
    "# log_mel [bsz, seqlen, dim]\n",
    "\n",
    "print(f'{log_mel.size()=}')\n",
    "print(f'{gt_log_mel.size()=}')\n",
    "\n",
    "###############################################################################################################\n",
    "# calculate losses\n",
    "# respelling_loss = respelling_loss_fn(respelling, e2e_asr_predictions)\n",
    "acoustic_loss = criterion(log_mel, gt_log_mel)\n",
    "\n",
    "# average loss over frames \n",
    "acoustic_loss = acoustic_loss / dec_lens\n",
    "# mel_loss = (mel_loss * mel_mask).sum() / mel_mask.sum()\n",
    "\n",
    "###############################################################################################################\n",
    "# backward pass\n",
    "loss = acoustic_loss \n",
    "\n",
    "print(f'{loss=}')\n",
    "\n",
    "# loss.backward()\n",
    "\n",
    "###############################################################################################################\n",
    "# log tensorboard metrics\n",
    "\n",
    "###############################################################################################################\n",
    "# validation set evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_spectrogram(log_mel, batch_index):\n",
    "    print(f'{log_mel[batch_index].transpose(0,1).size()=}')\n",
    "    log_mel = log_mel[batch_index].transpose(0,1).detach().cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    librosa.display.specshow(log_mel, x_axis='frames', y_axis='linear')\n",
    "    plt.colorbar()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_spectrogram(log_mel, batch_index=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_spectrogram(gt_log_mel, batch_index=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# develop respeller dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wordaligned_speechreps_dir = '/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels' # path to directory that contains folders of word aligned speech reps\n",
    "wordlist = ['identifies','mash','player','russias','techniques'] # txt file for the words to include speech reps "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token_and_melfilepaths = []\n",
    "for word in wordlist:\n",
    "    # find all word aligned mels for the word\n",
    "    word_dir = os.path.join(wordaligned_speechreps_dir, word)\n",
    "    mel_files = os.listdir(word_dir)\n",
    "    for mel_file in mel_files:\n",
    "        mel_file_path = os.path.join(word_dir, mel_file)\n",
    "        token_and_melfilepaths.append((word, mel_file_path))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token_and_melfilepaths"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## process text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from fastpitch.common.text.text_processing import TextProcessor\n",
    "text_cleaners = []\n",
    "symbol_set = \"english_basic_lowercase_no_arpabet\"\n",
    "tp = TextProcessor(symbol_set, text_cleaners, add_spaces=True, eos_symbol=\"$\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoded = torch.IntTensor(tp.encode_text('identifies'))\n",
    "encoded"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "decoded = [tp.id_to_symbol[id] for id in encoded.tolist()]\n",
    "decoded"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## process mel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word, fp = token_and_melfilepaths[0]\n",
    "wordaligned_mel = torch.load(fp)\n",
    "wordaligned_mel.size() # [seqlen, feats]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 'class'-ified dataset class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from fastpitch.common.text.text_processing import TextProcessor\n",
    "\n",
    "class RespellerDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "        1) loads word + word-aligned mel spec for all words in a wordlist\n",
    "        2) converts text to sequences of one-hot vectors (corresponding to grapheme indices in fastpitch)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        wordaligned_speechreps_dir, # path to directory that contains folders of word aligned speech reps\n",
    "        wordlist, # txt file for the words to include speech reps from\n",
    "        max_examples_per_wordtype=None,\n",
    "        text_cleaners=[],\n",
    "        symbol_set=\"english_basic_lowercase_no_arpabet\",\n",
    "        add_spaces=True,\n",
    "        eos_symbol=\"$\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # load wordlist as a python list\n",
    "        if type(wordlist) == str:\n",
    "            if wordlist.endswith('.json'):\n",
    "                with open(wordlist) as f:\n",
    "                    wordlist = json.load(f)\n",
    "            else:\n",
    "                with open(wordlist) as f:\n",
    "                    wordlist = f.read().splitlines()\n",
    "        elif type(wordlist) == list:\n",
    "            pass # dont need to do anything, already in expected form\n",
    "        elif type(wordlist) == set:\n",
    "            wordlist = list(wordlist)\n",
    "        \n",
    "        wordlist = sorted(wordlist)\n",
    "        \n",
    "        # create list of all word tokens and their word aligned speech reps\n",
    "        self.word_freq = Counter()\n",
    "        self.token_and_melfilepaths = []\n",
    "        print(\"Initialising respeller dataset\")\n",
    "        for word in tqdm(wordlist):\n",
    "            # find all word aligned mels for the word\n",
    "            word_dir = os.path.join(wordaligned_speechreps_dir, word)\n",
    "            mel_files = os.listdir(word_dir)\n",
    "            if max_examples_per_wordtype:\n",
    "                mel_files = mel_files[:max_examples_per_wordtype]\n",
    "            for mel_file in mel_files:\n",
    "                mel_file_path = os.path.join(word_dir, mel_file)\n",
    "                self.token_and_melfilepaths.append((word, mel_file_path))\n",
    "                self.word_freq[word] += 1\n",
    "                \n",
    "        self.tp = TextProcessor(symbol_set, text_cleaners, add_spaces=add_spaces, eos_symbol=eos_symbol)\n",
    "\n",
    "    def get_mel(self, filename):\n",
    "        return torch.load(filename)\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        \"\"\"encode raw text into indices defined by grapheme embedding table of the TTS model\"\"\"\n",
    "        return torch.IntTensor(self.tp.encode_text(text))\n",
    "    \n",
    "    def decode_text(self, encoded):\n",
    "        return [self.tp.id_to_symbol[id] for id in encoded.tolist()]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        word, mel_filepath = self.token_and_melfilepaths[index]\n",
    "        encoded_word = self.encode_text(word)\n",
    "        mel = self.get_mel(mel_filepath)\n",
    "        \n",
    "        return {\n",
    "            'word': word, \n",
    "            'encoded_word': encoded_word, \n",
    "            'mel': mel,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_and_melfilepaths)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels', # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist=['identifies','mash','player','russias','techniques'],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch = []\n",
    "\n",
    "for itemdict in dataset:\n",
    "    # unpack dict\n",
    "    word = itemdict['word'] \n",
    "    encoded_word = itemdict['encoded_word'] \n",
    "    mel = itemdict['mel'] \n",
    "    \n",
    "    # check\n",
    "    print(word, encoded_word, mel.size())\n",
    "    \n",
    "    batch.append(itemdict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## collate function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Collate():\n",
    "    \"\"\" Zero-pads model inputs and targets based on number of frames per setep\n",
    "    \"\"\"\n",
    "    # def __init__(self):\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Collate's training batch from encoded word token and its \n",
    "        corresponding word-aligned mel spectrogram\n",
    "        \n",
    "        batch: [encoded_token, wordaligned_mel]\n",
    "        \"\"\"\n",
    "        # Right zero-pad all one-hot text sequences to max input length\n",
    "        input_lengths, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([len(x['encoded_word']) for x in batch]),\n",
    "            dim=0, descending=True)\n",
    "        max_input_len = input_lengths[0]\n",
    "\n",
    "        words = []\n",
    "        text_padded = torch.LongTensor(len(batch), max_input_len)\n",
    "        text_padded.zero_()\n",
    "        text_lengths = torch.LongTensor(len(batch))\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            words.append(batch[ids_sorted_decreasing[i]]['word'])\n",
    "            text = batch[ids_sorted_decreasing[i]]['encoded_word']\n",
    "            text_padded[i, :text.size(0)] = text\n",
    "            text_lengths[i] = text.size(0)\n",
    "\n",
    "        # Right zero-pad mel-spec\n",
    "        num_mels = batch[0]['mel'].size(1)\n",
    "        max_target_len = max([x['mel'].size(0) for x in batch])\n",
    "\n",
    "        mel_padded = torch.FloatTensor(len(batch), max_target_len, num_mels)\n",
    "        mel_padded.zero_()\n",
    "        mel_lengths = torch.LongTensor(len(batch))\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            mel = batch[ids_sorted_decreasing[i]]['mel']\n",
    "            mel_padded[i, :mel.size(0), :] = mel\n",
    "            mel_lengths[i] = mel.size(0)\n",
    "\n",
    "        return {\n",
    "            'words': words,\n",
    "            'text_padded': text_padded,\n",
    "            'text_lengths': text_lengths,\n",
    "            'mel_padded': mel_padded, \n",
    "            'mel_lengths': mel_lengths,\n",
    "        }\n",
    "                # input_lengths, mel_padded, output_lengths,\n",
    "                # len_x, dur_padded, dur_lens, pitch_padded, speaker)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collate_fn = Collate()\n",
    "collated = collate_fn(batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collated['text_padded'].size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collated['text_padded']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collated['words']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collated['mel_padded'].size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Â put batch on gpu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def to_gpu(x):\n",
    "    x = x.contiguous()\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda(non_blocking=True)\n",
    "    return torch.autograd.Variable(x)\n",
    "\n",
    "def batch_to_gpu(collated_batch):\n",
    "    \"\"\"put elements that are used throughout training onto gpu\"\"\"\n",
    "    words = collated_batch['words']\n",
    "    text_padded = collated_batch['text_padded']\n",
    "    text_lengths = collated_batch['text_lengths']\n",
    "    mel_padded = collated_batch['mel_padded']\n",
    "    mel_lengths = collated_batch['mel_lengths']\n",
    "    \n",
    "    # no need to put words on gpu, its only used during eval loop\n",
    "    text_padded = to_gpu(text_padded).long()\n",
    "    text_lengths = to_gpu(text_lengths).long()\n",
    "    mel_padded = to_gpu(mel_padded).float()\n",
    "    mel_lengths = to_gpu(mel_lengths).long()\n",
    "    \n",
    "    # x: inputs\n",
    "    x = {\n",
    "        'words': words,\n",
    "        'text_padded': text_padded,\n",
    "        'text_lengths': text_lengths,\n",
    "    }\n",
    "    # y: targets\n",
    "    y = {\n",
    "        'mel_padded': mel_padded, \n",
    "        'mel_lengths': mel_lengths,\n",
    "    }\n",
    "    \n",
    "    return (x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Â batch_to_gpu(collated)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# full train + dev datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels', # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist='/home/s1785140/data/ljspeech_fastpitch/respeller_train_words.json',\n",
    "    max_examples_per_wordtype=2,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum(train_dataset.word_freq.values())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels', # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist='/home/s1785140/data/ljspeech_fastpitch/respeller_dev_words.json',\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(dev_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum(dev_dataset.word_freq.values())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# create torch dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO, implement distributed training?\n",
    "train_sampler = None\n",
    "shuffle = True\n",
    "num_cpus = 1 \n",
    "train_loader = DataLoader(train_dataset, num_workers=2*num_cpus, shuffle=shuffle,\n",
    "                          sampler=train_sampler, batch_size=args.batch_size,\n",
    "                          pin_memory=False, drop_last=True,\n",
    "                          collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#     print(batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FULL train() loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch_optimizer import Lamb\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adjust_learning_rate(total_iter, opt, learning_rate, warmup_iters=None):\n",
    "    if warmup_iters == 0:\n",
    "        scale = 1.0\n",
    "    elif total_iter > warmup_iters:\n",
    "        scale = 1. / (total_iter ** 0.5)\n",
    "    else:\n",
    "        scale = total_iter / (warmup_iters ** 1.5)\n",
    "\n",
    "    for param_group in opt.param_groups:\n",
    "        param_group['lr'] = learning_rate * scale"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## pre-training loop stuff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def train(rank, args):\n",
    "\n",
    "\n",
    "# handle GPU\n",
    "rank = 0\n",
    "args.local_rank = rank\n",
    "device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "\n",
    "# load models\n",
    "tts, n_symbols, grapheme_embedding_dim = load_pretrained_fastpitch(args)\n",
    "respeller = EncoderRespeller(n_symbols=n_symbols, d_model=args.d_model)\n",
    "quantiser = GumbelVectorQuantizer(\n",
    "    in_dim=args.d_model,\n",
    "    codebook_size=n_symbols,  # number of codebook entries\n",
    "    embedding_dim=grapheme_embedding_dim,\n",
    "    temp=args.latent_temp,\n",
    ")\n",
    "init_embedding_weights(tts.encoder.word_emb.weight.unsqueeze(0), quantiser.vars)\n",
    "criterion = SoftDTW(use_cuda=True, gamma=0.1) # input should be size [bsz, seqlen, dim]\n",
    "\n",
    "tts.to(device)\n",
    "respeller.to(device)\n",
    "quantiser.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "# load optimiser and assign to it the weights to be trained\n",
    "kw = dict(lr=args.learning_rate, betas=(0.9, 0.98), eps=1e-9,\n",
    "          weight_decay=args.weight_decay)\n",
    "optimizer = Lamb(list(respeller.parameters()) + list(quantiser.trainable_parameters()), **kw)\n",
    "\n",
    "# (optional) load checkpoint for respeller\n",
    "start_epoch = [1]\n",
    "start_iter = [0]\n",
    "assert args.checkpoint_path is None or args.resume is False, (\n",
    "    \"Specify a single checkpoint source\")\n",
    "if args.checkpoint_path is not None:\n",
    "    ch_fpath = args.checkpoint_path\n",
    "elif args.resume:\n",
    "    ch_fpath = last_checkpoint(args.chkpt_save_dir)\n",
    "else:\n",
    "    ch_fpath = None\n",
    "if ch_fpath is not None:\n",
    "    load_checkpoint(args, respeller, ema_model, optimizer, scaler,\n",
    "                    start_epoch, start_iter, model_config, ch_fpath)\n",
    "start_epoch = start_epoch[0]\n",
    "total_iter = start_iter[0]\n",
    "    \n",
    "# create datasets, collate func, dataloader\n",
    "train_dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels', # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist='/home/s1785140/data/ljspeech_fastpitch/respeller_train_words.json',\n",
    "    max_examples_per_wordtype=2,\n",
    ")\n",
    "dev_dataset = RespellerDataset(\n",
    "    wordaligned_speechreps_dir='/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels', # path to directory that contains folders of word aligned speech reps\n",
    "    wordlist='/home/s1785140/data/ljspeech_fastpitch/respeller_dev_words.json',\n",
    ")\n",
    "num_cpus = 1 # TODO change to CLA?\n",
    "train_loader = DataLoader(train_dataset, num_workers=2*num_cpus, shuffle=True,\n",
    "                          sampler=None, batch_size=args.batch_size,\n",
    "                          pin_memory=False, drop_last=True,\n",
    "                          collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, num_workers=2*num_cpus, shuffle=False,\n",
    "                          sampler=None, batch_size=args.batch_size,\n",
    "                          pin_memory=False, collate_fn=collate_fn)\n",
    "\n",
    "# load pretrained hifigan\n",
    "\n",
    "# log spectrograms and generated audio for first few validation wordtypes\n",
    "\n",
    "# train loop\n",
    "respeller.train()\n",
    "quantiser.train()\n",
    "_ = tts.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from fastpitch.common.utils import mask_from_lens\n",
    "import fastpitch.common.tb_dllogger as logger\n",
    "from collections import OrderedDict\n",
    "\n",
    "def touch_file(path):\n",
    "    if not os.path.exists(path):\n",
    "        basedir = os.path.dirname(path)\n",
    "        os.makedirs(basedir, exist_ok=True)\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(\"\")\n",
    "\n",
    "# initialise logger\n",
    "tb_subsets = ['train', 'val']\n",
    "log_fpath = args.log_file or os.path.join(args.chkpt_save_dir, 'nvlog.json')\n",
    "touch_file(log_fpath)\n",
    "\n",
    "logger.init(log_fpath, args.chkpt_save_dir, enabled=(args.local_rank == 0),\n",
    "            tb_subsets=tb_subsets)\n",
    "logger.parameters(vars(args), tb_subset='train')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def forward_pass(respeller, tts, x):\n",
    "    \"\"\"x: inputs\n",
    "    x = {\n",
    "        'words': words,\n",
    "        'text_padded': text_padded,\n",
    "        'text_lengths': text_lengths,\n",
    "    }\"\"\"\n",
    "    logits = respeller(x['text_padded'])\n",
    "    \n",
    "    quantiser_outdict = quantiser(logits, produce_targets=True)\n",
    "    g_embedding_indices = quantiser_outdict[\"targets\"].squeeze(2)\n",
    "    g_embeddings = quantiser_outdict[\"x\"]\n",
    "\n",
    "    log_mel, dec_lens, _dur_pred, _pitch_pred = tts(\n",
    "        inputs=g_embeddings,\n",
    "        ids=g_embedding_indices,\n",
    "        skip_embeddings=True,\n",
    "    )\n",
    "    \n",
    "    # log_mel: [bsz, dim, seqlen]\n",
    "    log_mel = log_mel.transpose(1,2)\n",
    "    # log_mel: [bsz, seqlen, dim]\n",
    "    \n",
    "    # return mask for masking acoustic loss\n",
    "    # padding_idx = 0\n",
    "    # mask = (g_embedding_indices != padding_idx).unsqueeze(2)\n",
    "    # mask.size()\n",
    "    # dec_mask = mask_from_lens(dec_lens).unsqueeze(2)\n",
    "    \n",
    "    return log_mel, dec_lens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def log_stdout(logger, subset, epoch_iters, total_steps, loss, took):\n",
    "    logger_data = [\n",
    "        ('Loss/Total', loss),\n",
    "    ]\n",
    "    logger_data.append(('Time/Iter time', took))\n",
    "    logger.log(epoch_iters,\n",
    "               tb_total_steps=total_steps,\n",
    "               subset=subset,\n",
    "               data=OrderedDict(logger_data)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, args.epochs + 1):\n",
    "    # logging metrics\n",
    "    epoch_start_time = time.perf_counter()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_mel_loss = 0.0\n",
    "    epoch_num_frames = 0\n",
    "    epoch_frames_per_sec = 0.0\n",
    "    iter_loss = 0\n",
    "    iter_num_frames = 0\n",
    "    iter_meta = {}\n",
    "    epoch_iter = 0\n",
    "    num_iters = len(train_loader)\n",
    "    \n",
    "    print(\"debug, epoch\", epoch)\n",
    "\n",
    "    # iterate over all batches in epoch\n",
    "    for batch in train_loader:        \n",
    "        if epoch_iter == 1:\n",
    "            break # NB quit training loop, FOR DEVELOPMENT!!!\n",
    "        \n",
    "        if epoch_iter == num_iters: # useful for gradient accumulation\n",
    "            break\n",
    "            \n",
    "        print(\"debug, epoch_iter\", epoch_iter)\n",
    "                    \n",
    "        total_iter += 1\n",
    "        epoch_iter += 1\n",
    "        iter_start_time = time.perf_counter()\n",
    "\n",
    "        adjust_learning_rate(total_iter, optimizer, args.learning_rate,\n",
    "                             args.warmup_steps)\n",
    "\n",
    "        respeller.zero_grad()\n",
    "        quantiser.zero_grad()\n",
    "\n",
    "        x, y = batch_to_gpu(batch) # x: inputs, y: targets\n",
    "        gt_mel = y[\"mel_padded\"]\n",
    "        \n",
    "        # # y: targets\n",
    "        # y = {\n",
    "        #     'mel_padded': mel_padded, \n",
    "        #     'mel_lengths': mel_lengths,\n",
    "        # }\n",
    "        \n",
    "        # forward pass through models (respeller -> quantiser -> tts)\n",
    "        pred_mel, dec_lens = forward_pass(respeller, tts, x)\n",
    "        \n",
    "        print(f'{pred_mel.size()=}')\n",
    "        print(f'{gt_mel.size()=}')\n",
    "        \n",
    "        # TODO: DO WE NEED MASK IF WE USE SOFTDTW LOSS? \n",
    "        # I THINK IT AUTOMATICALLY WILL ALIGN PADDED FRAMES WITH EACH OTHER???\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(pred_mel, gt_mel)\n",
    "        print('raw loss from softdtw', loss)\n",
    "        \n",
    "        loss = loss / dec_lens\n",
    "        print('loss avg according to dec seqlens', loss)\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        print('loss avged across batch', loss)\n",
    "        \n",
    "        # backpropagation of loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # log metrics\n",
    "        iter_loss += loss.item()\n",
    "        iter_time = time.perf_counter() - iter_start_time\n",
    "        epoch_loss += iter_loss\n",
    "        \n",
    "        log_stdout(\n",
    "            logger,\n",
    "            'train',\n",
    "            (epoch, epoch_iter, num_iters),\n",
    "            total_iter,\n",
    "            iter_loss,\n",
    "            iter_time,\n",
    "        )\n",
    "        ### Finished Epoch!\n",
    "        \n",
    "    epoch_time = time.perf_counter() - epoch_start_time\n",
    "        \n",
    "    def validate():\n",
    "        pass\n",
    "\n",
    "    validate()\n",
    "\n",
    "    def maybe_save_checkpoint(args, model, optimizer, epoch,\n",
    "                              total_iter, config):\n",
    "        if args.local_rank != 0:\n",
    "            return\n",
    "\n",
    "        intermediate = (args.epochs_per_checkpoint > 0\n",
    "                        and epoch % args.epochs_per_checkpoint == 0)\n",
    "\n",
    "        if not intermediate and epoch < args.epochs:\n",
    "            return\n",
    "\n",
    "        fpath = os.path.join(args.chkpt_save_dir, f\"FastPitch_checkpoint_{epoch}.pt\")\n",
    "        print(f\"Saving model and optimizer state at epoch {epoch} to {fpath}\")\n",
    "        checkpoint = {'epoch': epoch,\n",
    "                      'iteration': total_iter,\n",
    "                      'config': config,\n",
    "                      'state_dict': model.state_dict(),\n",
    "                      'optimizer': optimizer.state_dict()}\n",
    "        torch.save(checkpoint, fpath)\n",
    "\n",
    "    maybe_save_checkpoint(args, respeller, optimizer, \n",
    "                          epoch, total_iter, model_config)\n",
    "\n",
    "    logger.flush()\n",
    "        \n",
    "print(\"\\n *** Finished training! ***\")\n",
    "\n",
    "print(\"Performing final validation\")\n",
    "log_stdout(\n",
    "    logger,\n",
    "    'train_avg',\n",
    "    (),\n",
    "    None,\n",
    "    epoch_loss / epoch_iter,\n",
    "    epoch_time,\n",
    ")\n",
    "\n",
    "validate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## validate() fn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def validate(respeller_model, tts_model, vocoder, valset, epoch, audio_interval=5):\n",
    "    \"\"\"Handles all the validation scoring and printing\n",
    "    GT (beginning of training):\n",
    "    - log GT mel spec and vocoded audio for several validation set words\n",
    "    \n",
    "    Model outputs:\n",
    "    - log predicted mel spec and vocoded audio from fastpitch\n",
    "    - log respelled word from respeller\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    # log spectrograms and generated audio for first few utterances\n",
    "    if (i == 0) and (epoch % audio_interval == 0 if epoch is not None else True):\n",
    "        # TODO: sort utterances by mel length rather than more variable text length\n",
    "        # for consistent sample across different experiments\n",
    "        batch_audiopaths_and_text = sorted(valset.audiopaths_and_text[:batch_size],\n",
    "                                           key=lambda x: len(valset.get_text(x[3])),\n",
    "                                           reverse=True)\n",
    "        tb_fnames = [i[0] for i in batch_audiopaths_and_text]\n",
    "        plot_spectrograms(\n",
    "            y_pred, tb_fnames, total_iter, n=4, label='Predicted spectrogram', mas=mas)\n",
    "        if vocoder is not None:\n",
    "            generate_audio(y_pred, tb_fnames, total_iter, vocoder, sampling_rate,\n",
    "                           hop_length, n=4, label='Predicted audio', mas=mas)\n",
    "    \n",
    "    if was_training:\n",
    "        model.train()\n",
    "        \n",
    "    return val_meta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "validate(\n",
    "    respeller_model=respeller, \n",
    "    tts_model=tts, \n",
    "    vocoder=vocoder,\n",
    "    valset=val_set, \n",
    "    epoch=0,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b1c4b39-189d-4af5-a5c0-7489fbc95833",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, args.epochs + 1):\n",
    "    # logging metrics\n",
    "    epoch_start_time = time.perf_counter()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_mel_loss = 0.0\n",
    "    epoch_num_frames = 0\n",
    "    epoch_frames_per_sec = 0.0\n",
    "    iter_loss = 0\n",
    "    iter_num_frames = 0\n",
    "    iter_meta = {}\n",
    "    epoch_iter = 0\n",
    "    num_iters = len(train_loader)\n",
    "    \n",
    "    print(\"debug, epoch\", epoch)\n",
    "\n",
    "    # iterate over all batches in epoch\n",
    "    for batch in train_loader:        \n",
    "        if epoch_iter == 1:\n",
    "            break # NB quit training loop, FOR DEVELOPMENT!!!\n",
    "        \n",
    "        if epoch_iter == num_iters: # useful for gradient accumulation\n",
    "            break\n",
    "            \n",
    "        print(\"debug, epoch_iter\", epoch_iter)\n",
    "                    \n",
    "        total_iter += 1\n",
    "        epoch_iter += 1\n",
    "        iter_start_time = time.perf_counter()\n",
    "\n",
    "        adjust_learning_rate(total_iter, optimizer, args.learning_rate,\n",
    "                             args.warmup_steps)\n",
    "\n",
    "        respeller.zero_grad()\n",
    "        quantiser.zero_grad()\n",
    "\n",
    "        x, y = batch_to_gpu(batch) # x: inputs, y: targets\n",
    "        gt_mel = y[\"mel_padded\"]\n",
    "        \n",
    "        # # y: targets\n",
    "        # y = {\n",
    "        #     'mel_padded': mel_padded, \n",
    "        #     'mel_lengths': mel_lengths,\n",
    "        # }\n",
    "        \n",
    "        # forward pass through models (respeller -> quantiser -> tts)\n",
    "        pred_mel, dec_lens = forward_pass(respeller, tts, x)\n",
    "        \n",
    "        print(f'{pred_mel.size()=}')\n",
    "        print(f'{gt_mel.size()=}')\n",
    "        \n",
    "        # TODO: DO WE NEED MASK IF WE USE SOFTDTW LOSS? \n",
    "        # I THINK IT AUTOMATICALLY WILL ALIGN PADDED FRAMES WITH EACH OTHER???\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(pred_mel, gt_mel)\n",
    "        print('raw loss from softdtw', loss)\n",
    "        \n",
    "        loss = loss / dec_lens\n",
    "        print('loss avg according to dec seqlens', loss)\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        print('loss avged across batch', loss)\n",
    "        \n",
    "        # backpropagation of loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # log metrics\n",
    "        iter_loss += loss.item()\n",
    "        iter_time = time.perf_counter() - iter_start_time\n",
    "        epoch_loss += iter_loss\n",
    "        \n",
    "        log_stdout(\n",
    "            logger,\n",
    "            'train',\n",
    "            (epoch, epoch_iter, num_iters),\n",
    "            total_iter,\n",
    "            iter_loss,\n",
    "            iter_time,\n",
    "        )\n",
    "        ### Finished Epoch!\n",
    "        \n",
    "    epoch_time = time.perf_counter() - epoch_start_time\n",
    "        \n",
    "    def validate():\n",
    "        pass\n",
    "\n",
    "    validate()\n",
    "\n",
    "    def maybe_save_checkpoint(args, model, optimizer, epoch,\n",
    "                              total_iter, config):\n",
    "        if args.local_rank != 0:\n",
    "            return\n",
    "\n",
    "        intermediate = (args.epochs_per_checkpoint > 0\n",
    "                        and epoch % args.epochs_per_checkpoint == 0)\n",
    "\n",
    "        if not intermediate and epoch < args.epochs:\n",
    "            return\n",
    "\n",
    "        fpath = os.path.join(args.chkpt_save_dir, f\"FastPitch_checkpoint_{epoch}.pt\")\n",
    "        print(f\"Saving model and optimizer state at epoch {epoch} to {fpath}\")\n",
    "        checkpoint = {'epoch': epoch,\n",
    "                      'iteration': total_iter,\n",
    "                      'config': config,\n",
    "                      'state_dict': model.state_dict(),\n",
    "                      'optimizer': optimizer.state_dict()}\n",
    "        torch.save(checkpoint, fpath)\n",
    "\n",
    "    maybe_save_checkpoint(args, respeller, optimizer, \n",
    "                          epoch, total_iter, model_config)\n",
    "\n",
    "    logger.flush()\n",
    "        \n",
    "print(\"\\n *** Finished training! ***\")\n",
    "\n",
    "print(\"Performing final validation\")\n",
    "log_stdout(\n",
    "    logger,\n",
    "    'train_avg',\n",
    "    (),\n",
    "    None,\n",
    "    epoch_loss / epoch_iter,\n",
    "    epoch_time,\n",
    ")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f34556-f71c-4907-a455-f8673375690e",
   "metadata": {},
   "source": [
    "## validate() fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ba4dcb39-2c7d-43ab-b739-c19d2c3ab54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(respeller_model, tts_model, vocoder, valset, epoch, audio_interval=5):\n",
    "    \"\"\"Handles all the validation scoring and printing\n",
    "    GT (beginning of training):\n",
    "    - log GT mel spec and vocoded audio for several validation set words\n",
    "    \n",
    "    Model outputs:\n",
    "    - log predicted mel spec and vocoded audio from fastpitch\n",
    "    - log respelled word from respeller\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    # log spectrograms and generated audio for first few utterances\n",
    "    if (i == 0) and (epoch % audio_interval == 0 if epoch is not None else True):\n",
    "        # TODO: sort utterances by mel length rather than more variable text length\n",
    "        # for consistent sample across different experiments\n",
    "        batch_audiopaths_and_text = sorted(valset.audiopaths_and_text[:batch_size],\n",
    "                                           key=lambda x: len(valset.get_text(x[3])),\n",
    "                                           reverse=True)\n",
    "        tb_fnames = [i[0] for i in batch_audiopaths_and_text]\n",
    "        plot_spectrograms(\n",
    "            y_pred, tb_fnames, total_iter, n=4, label='Predicted spectrogram', mas=mas)\n",
    "        if vocoder is not None:\n",
    "            generate_audio(y_pred, tb_fnames, total_iter, vocoder, sampling_rate,\n",
    "                           hop_length, n=4, label='Predicted audio', mas=mas)\n",
    "    \n",
    "    if was_training:\n",
    "        model.train()\n",
    "        \n",
    "    return val_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2b25b35f-db18-4837-ad0a-2695e21e6688",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [96], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m validate(\n\u001B[1;32m      2\u001B[0m     respeller_model\u001B[38;5;241m=\u001B[39mrespeller, \n\u001B[1;32m      3\u001B[0m     tts_model\u001B[38;5;241m=\u001B[39mtts, \n\u001B[1;32m      4\u001B[0m     vocoder\u001B[38;5;241m=\u001B[39mvocoder,\n\u001B[0;32m----> 5\u001B[0m     valset\u001B[38;5;241m=\u001B[39m\u001B[43mval_set\u001B[49m, \n\u001B[1;32m      6\u001B[0m     epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m      7\u001B[0m )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'val_set' is not defined"
     ]
    }
   ],
   "source": [
    "validate(\n",
    "    respeller_model=respeller, \n",
    "    tts_model=tts, \n",
    "    vocoder=vocoder,\n",
    "    valset=val_set, \n",
    "    epoch=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5680048d-c409-4e96-9838-6f3c778f94de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}